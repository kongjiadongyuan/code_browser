<def f='linux/arch/x86/include/asm/processor.h' l='806' ll='811' type='void prefetchw(const void * x)'/>
<use f='linux/arch/x86/include/asm/processor.h' l='815' u='c' c='spin_lock_prefetch'/>
<doc f='linux/arch/x86/include/asm/processor.h' l='801'>/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */</doc>
<use f='linux/arch/x86/mm/fault.c' l='1251' u='c' c='__do_page_fault'/>
<use f='linux/include/linux/netdevice.h' l='2953' u='c' c='netdev_txq_bql_enqueue_prefetchw'/>
<use f='linux/include/linux/netdevice.h' l='2967' u='c' c='netdev_txq_bql_complete_prefetchw'/>
<use f='linux/drivers/gpu/drm/i915/i915_gem_request.c' l='393' u='c' c='i915_gem_request_retire'/>
<use f='linux/fs/ext4/readpage.c' l='129' u='c' c='ext4_mpage_readpages'/>
<use f='linux/fs/mpage.c' l='378' u='c' c='mpage_readpages'/>
<use f='linux/kernel/locking/qspinlock.c' l='433' u='c' c='queued_spin_lock_slowpath'/>
<use f='linux/mm/page_alloc.c' l='1280' u='c' c='__free_pages_boot_core'/>
<use f='linux/mm/page_alloc.c' l='1282' u='c' c='__free_pages_boot_core'/>
<use f='linux/mm/vmscan.c' l='142' u='c' c='isolate_lru_pages'/>
<use f='linux/net/core/skbuff.c' l='196' u='c' c='__alloc_skb'/>
<use f='linux/net/core/skbuff.c' l='213' u='c' c='__alloc_skb'/>
<use f='linux/net/core/skbuff.c' l='743' u='c' c='_kfree_skb_defer'/>
