<def f='linux/include/asm-generic/qspinlock.h' l='50' ll='53' type='int queued_spin_value_unlocked(struct qspinlock lock)'/>
<doc f='linux/include/asm-generic/qspinlock.h' l='40'>/**
 * queued_spin_value_unlocked - is the spinlock structure unlocked?
 * @lock: queued spinlock structure
 * Return: 1 if it is unlocked, 0 otherwise
 *
 * N.B. Whenever there are tasks waiting for the lock, it is considered
 *      locked wrt the lockref code to avoid lock stealing by the lockref
 *      code and change things underneath the lock. This also allows some
 *      optimizations to be applied without conflict with lockref.
 */</doc>
<use f='linux/lib/lockref.c' l='43' macro='1' u='c' c='lockref_get'/>
<use f='linux/lib/lockref.c' l='64' macro='1' u='c' c='lockref_get_not_zero'/>
<use f='linux/lib/lockref.c' l='91' macro='1' u='c' c='lockref_get_or_lock'/>
<use f='linux/lib/lockref.c' l='117' macro='1' u='c' c='lockref_put_return'/>
<use f='linux/lib/lockref.c' l='135' macro='1' u='c' c='lockref_put_or_lock'/>
<use f='linux/lib/lockref.c' l='172' macro='1' u='c' c='lockref_get_not_dead'/>
