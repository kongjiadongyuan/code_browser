<def f='linux/arch/x86/include/asm/fixmap.h' l='66' ll='135'/>
<use f='linux/arch/x86/include/asm/fixmap.h' l='151' c='__native_set_fixmap'/>
<use f='linux/arch/x86/include/asm/fixmap.h' l='152' c='native_set_fixmap'/>
<use f='linux/arch/x86/include/asm/fixmap.h' l='156' c='__set_fixmap'/>
<use f='linux/arch/x86/include/asm/fixmap.h' l='188' c='__early_set_fixmap'/>
<doc f='linux/arch/x86/include/asm/fixmap.h' l='47'>/*
 * Here we define all the compile-time &apos;special&apos; virtual
 * addresses. The point is to have a constant address at
 * compile time, but to set the physical address only
 * in the boot process.
 * for x86_32: We allocate these special addresses
 * from the end of virtual memory (0xfffff000) backwards.
 * Also this lets us do fail-safe vmalloc(), we
 * can guarantee that these special addresses and
 * vmalloc()-ed addresses never overlap.
 *
 * These &apos;compile-time allocated&apos; memory buffers are
 * fixed-size 4k pages (or larger if used with an increment
 * higher than 1). Use set_fixmap(idx,phys) to associate
 * physical memory with fixmap indices.
 *
 * TLB entries of such buffers will not be flushed across
 * task switches.
 */</doc>
<use f='linux/arch/x86/mm/ioremap.c' l='807' c='__early_set_fixmap'/>
<use f='linux/arch/x86/mm/pgtable.c' l='571' c='__native_set_fixmap'/>
<use f='linux/arch/x86/mm/pgtable.c' l='583' c='native_set_fixmap'/>
<use f='linux/mm/early_ioremap.c' l='111' c='__early_ioremap'/>
<use f='linux/mm/early_ioremap.c' l='173' c='early_iounmap'/>
