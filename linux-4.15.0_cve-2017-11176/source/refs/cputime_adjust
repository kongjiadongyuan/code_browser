<dec f='linux/include/linux/sched/cputime.h' l='57' type='void cputime_adjust(struct task_cputime * curr, struct prev_cputime * prev, u64 * ut, u64 * st)'/>
<use f='linux/kernel/cgroup/stat.c' l='272' u='c' c='cgroup_stat_show_cputime'/>
<def f='linux/kernel/sched/cputime.c' l='594' ll='664' type='void cputime_adjust(struct task_cputime * curr, struct prev_cputime * prev, u64 * ut, u64 * st)'/>
<use f='linux/kernel/sched/cputime.c' l='673' u='c' c='task_cputime_adjusted'/>
<use f='linux/kernel/sched/cputime.c' l='682' u='c' c='thread_group_cputime_adjusted'/>
<doc f='linux/kernel/sched/cputime.c' l='574'>/*
 * Adjust tick based cputime random precision against scheduler runtime
 * accounting.
 *
 * Tick based cputime accounting depend on random scheduling timeslices of a
 * task to be interrupted or not by the timer.  Depending on these
 * circumstances, the number of these interrupts may be over or
 * under-optimistic, matching the real user and system cputime with a variable
 * precision.
 *
 * Fix this by scaling these tick based values against the total runtime
 * accounted by the CFS scheduler.
 *
 * This code provides the following guarantees:
 *
 *   stime + utime == rtime
 *   stime_i+1 &gt;= stime_i, utime_i+1 &gt;= utime_i
 *
 * Assuming that rtime_i+1 &gt;= rtime_i.
 */</doc>
