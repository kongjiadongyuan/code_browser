<dec f='linux/mm/internal.h' l='513' type='void flush_tlb_batched_pending(struct mm_struct * mm)'/>
<use f='linux/mm/madvise.c' l='333' u='c' c='madvise_free_pte_range'/>
<use f='linux/mm/memory.c' l='1297' u='c' c='zap_pte_range'/>
<use f='linux/mm/mprotect.c' l='68' u='c' c='change_pte_range'/>
<use f='linux/mm/mremap.c' l='156' u='c' c='move_ptes'/>
<def f='linux/mm/rmap.c' l='658' ll='670' type='void flush_tlb_batched_pending(struct mm_struct * mm)'/>
<doc f='linux/mm/rmap.c' l='643'>/*
 * Reclaim unmaps pages under the PTL but do not flush the TLB prior to
 * releasing the PTL if TLB flushes are batched. It&apos;s possible for a parallel
 * operation such as mprotect or munmap to race between reclaim unmapping
 * the page and flushing the page. If this race occurs, it potentially allows
 * access to data via a stale TLB entry. Tracking all mm&apos;s that have TLB
 * batching in flight would be expensive during reclaim so instead track
 * whether TLB batching occurred in the past and if so then do a flush here
 * if required. This will cost one additional flush per reclaim cycle paid
 * by the first operation at risk such as mprotect and mumap.
 *
 * This must be called under the PTL so that an access to tlb_flush_batched
 * that is potentially a &quot;reclaim vs mprotect/munmap/etc&quot; race will synchronise
 * via the PTL.
 */</doc>
