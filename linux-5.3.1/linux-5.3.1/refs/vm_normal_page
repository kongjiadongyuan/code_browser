<dec f='linux-5.3.1/include/linux/mm.h' l='1421' type='struct page * vm_normal_page(struct vm_area_struct * vma, unsigned long addr, pte_t pte)'/>
<use f='linux-5.3.1/fs/proc/task_mmu.c' l='527' u='c' c='smaps_pte_entry'/>
<use f='linux-5.3.1/fs/proc/task_mmu.c' l='713' u='c' c='smaps_hugetlb_range'/>
<use f='linux-5.3.1/fs/proc/task_mmu.c' l='1085' u='c' c='clear_refs_pte_range'/>
<use f='linux-5.3.1/fs/proc/task_mmu.c' l='1326' u='c' c='pte_to_pagemap_entry'/>
<use f='linux-5.3.1/fs/proc/task_mmu.c' l='1696' u='c' c='can_gather_numa_stats'/>
<use f='linux-5.3.1/mm/gup.c' l='228' u='c' c='follow_page_pte'/>
<use f='linux-5.3.1/mm/gup.c' l='607' u='c' c='get_gate_page'/>
<use f='linux-5.3.1/mm/madvise.c' l='357' u='c' c='madvise_free_pte_range'/>
<def f='linux-5.3.1/mm/memory.c' l='574' ll='627' type='struct page * vm_normal_page(struct vm_area_struct * vma, unsigned long addr, pte_t pte)'/>
<use f='linux-5.3.1/mm/memory.c' l='770' u='c' c='copy_one_pte'/>
<use f='linux-5.3.1/mm/memory.c' l='1032' u='c' c='zap_pte_range'/>
<use f='linux-5.3.1/mm/memory.c' l='2543' u='c' c='do_wp_page'/>
<use f='linux-5.3.1/mm/memory.c' l='3674' u='c' c='do_numa_page'/>
<doc f='linux-5.3.1/mm/memory.c' l='532'>/*
 * vm_normal_page -- This function gets the &quot;struct page&quot; associated with a pte.
 *
 * &quot;Special&quot; mappings do not wish to be associated with a &quot;struct page&quot; (either
 * it doesn&apos;t exist, or it exists but they don&apos;t want to touch it). In this
 * case, NULL is returned here. &quot;Normal&quot; mappings do have a struct page.
 *
 * There are 2 broad cases. Firstly, an architecture may define a pte_special()
 * pte bit, in which case this function is trivial. Secondly, an architecture
 * may not have a spare pte bit, which requires a more complicated scheme,
 * described below.
 *
 * A raw VM_PFNMAP mapping (ie. one that is not COWed) is always considered a
 * special mapping (even if there are underlying and valid &quot;struct pages&quot;).
 * COWed pages of a VM_PFNMAP are always normal.
 *
 * The way we recognize COWed pages within VM_PFNMAP mappings is through the
 * rules set up by &quot;remap_pfn_range()&quot;: the vma will have the VM_PFNMAP bit
 * set, and the vm_pgoff will point to the first PFN mapped: thus every special
 * mapping will always honor the rule
 *
 *	pfn_of_page == vma-&gt;vm_pgoff + ((addr - vma-&gt;vm_start) &gt;&gt; PAGE_SHIFT)
 *
 * And for normal mappings this is false.
 *
 * This restricts such mappings to be a linear translation from virtual address
 * to pfn. To get around this restriction, we allow arbitrary mappings so long
 * as the vma is not a COW mapping; in that case, we know that all ptes are
 * special (because none can have been COWed).
 *
 *
 * In order to support COW of arbitrary special mappings, we have VM_MIXEDMAP.
 *
 * VM_MIXEDMAP mappings can likewise contain memory with or without &quot;struct
 * page&quot; backing, however the difference is that _all_ pages with a struct
 * page (that is, those where pfn_valid is true) are refcounted and considered
 * normal pages by the VM. The disadvantage is that pages are refcounted
 * (which can be slower and simply not an option for some PFNMAP users). The
 * advantage is that we don&apos;t have to follow the strict linearity rule of
 * PFNMAP mappings in order to support COWable mappings.
 *
 */</doc>
<use f='linux-5.3.1/mm/mempolicy.c' l='517' u='c' c='queue_pages_pte_range'/>
<use f='linux-5.3.1/mm/mlock.c' l='399' u='c' c='__munlock_pagevec_fill'/>
<use f='linux-5.3.1/mm/mprotect.c' l='83' u='c' c='change_pte_range'/>
