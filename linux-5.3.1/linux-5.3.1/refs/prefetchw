<def f='linux-5.3.1/arch/x86/include/asm/processor.h' l='805' ll='810' type='void prefetchw(const void * x)'/>
<use f='linux-5.3.1/arch/x86/include/asm/processor.h' l='814' u='c' c='spin_lock_prefetch'/>
<doc f='linux-5.3.1/arch/x86/include/asm/processor.h' l='800'>/*
 * 3dnow prefetch to get an exclusive cache line.
 * Useful for spinlocks to avoid one state transition in the
 * cache coherency protocol:
 */</doc>
<use f='linux-5.3.1/arch/x86/mm/fault.c' l='1497' u='c' c='__do_page_fault'/>
<use f='linux-5.3.1/include/linux/netdevice.h' l='3173' u='c' c='netdev_txq_bql_enqueue_prefetchw'/>
<use f='linux-5.3.1/include/linux/netdevice.h' l='3187' u='c' c='netdev_txq_bql_complete_prefetchw'/>
<use f='linux-5.3.1/drivers/gpu/drm/i915/i915_request.c' l='225' u='c' c='i915_request_retire'/>
<use f='linux-5.3.1/fs/ext4/readpage.c' l='132' u='c' c='ext4_mpage_readpages'/>
<use f='linux-5.3.1/fs/mpage.c' l='397' u='c' c='mpage_readpages'/>
<use f='linux-5.3.1/kernel/locking/qspinlock.c' l='483' u='c' c='queued_spin_lock_slowpath'/>
<use f='linux-5.3.1/mm/page_alloc.c' l='1436' u='c' c='__free_pages_core'/>
<use f='linux-5.3.1/mm/page_alloc.c' l='1438' u='c' c='__free_pages_core'/>
<use f='linux-5.3.1/mm/vmscan.c' l='157' u='c' c='isolate_lru_pages'/>
<use f='linux-5.3.1/net/core/skbuff.c' l='200' u='c' c='__alloc_skb'/>
<use f='linux-5.3.1/net/core/skbuff.c' l='217' u='c' c='__alloc_skb'/>
<use f='linux-5.3.1/net/core/skbuff.c' l='880' u='c' c='_kfree_skb_defer'/>
