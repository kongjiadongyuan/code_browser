<!doctype html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1.0"><title>tree_plugin.h source code [linux-5.3.1/kernel/rcu/tree_plugin.h] - Woboq Code Browser</title>
<link rel="stylesheet" href="../../../../data/qtcreator.css" title="QtCreator"/>
<link rel="alternate stylesheet" href="../../../../data/kdevelop.css" title="KDevelop"/>
<script type="text/javascript" src="../../../../data/jquery/jquery.min.js"></script>
<script type="text/javascript" src="../../../../data/jquery/jquery-ui.min.js"></script>
<script>var file = 'linux-5.3.1/kernel/rcu/tree_plugin.h'; var root_path = '../../..'; var data_path = '../../../../data'; var ecma_script_api_version = 2;</script>
<script src='../../../../data/codebrowser.js'></script>
</head>
<body><div id='header'><h1 id='breadcrumb'><span>Browse the source code of </span><a href='../..'>linux-5.3.1</a>/<a href='..'>kernel</a>/<a href='./'>rcu</a>/<a href='tree_plugin.h.html'>tree_plugin.h</a></h1></div>
<hr/><div id='content'><table class="code">
<tr><th id="1">1</th><td><i>/* SPDX-License-Identifier: GPL-2.0+ */</i></td></tr>
<tr><th id="2">2</th><td><i>/*</i></td></tr>
<tr><th id="3">3</th><td><i> * Read-Copy Update mechanism for mutual exclusion (tree-based version)</i></td></tr>
<tr><th id="4">4</th><td><i> * Internal non-public definitions that provide either classic</i></td></tr>
<tr><th id="5">5</th><td><i> * or preemptible semantics.</i></td></tr>
<tr><th id="6">6</th><td><i> *</i></td></tr>
<tr><th id="7">7</th><td><i> * Copyright Red Hat, 2009</i></td></tr>
<tr><th id="8">8</th><td><i> * Copyright IBM Corporation, 2009</i></td></tr>
<tr><th id="9">9</th><td><i> *</i></td></tr>
<tr><th id="10">10</th><td><i> * Author: Ingo Molnar &lt;mingo@elte.hu&gt;</i></td></tr>
<tr><th id="11">11</th><td><i> *	   Paul E. McKenney &lt;paulmck@linux.ibm.com&gt;</i></td></tr>
<tr><th id="12">12</th><td><i> */</i></td></tr>
<tr><th id="13">13</th><td></td></tr>
<tr><th id="14">14</th><td><u>#include <a href="../locking/rtmutex_common.h.html">"../locking/rtmutex_common.h"</a></u></td></tr>
<tr><th id="15">15</th><td></td></tr>
<tr><th id="16">16</th><td><u>#<span data-ppcond="16">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_NOCB_CPU">CONFIG_RCU_NOCB_CPU</span></u></td></tr>
<tr><th id="17">17</th><td><em>static</em> cpumask_var_t rcu_nocb_mask; <i>/* CPUs to have callbacks offloaded. */</i></td></tr>
<tr><th id="18">18</th><td><em>static</em> bool __read_mostly rcu_nocb_poll;    <i>/* Offload kthread are to poll. */</i></td></tr>
<tr><th id="19">19</th><td><u>#<span data-ppcond="16">endif</span> /* #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="20">20</th><td></td></tr>
<tr><th id="21">21</th><td><i>/*</i></td></tr>
<tr><th id="22">22</th><td><i> * Check the RCU kernel configuration parameters and print informative</i></td></tr>
<tr><th id="23">23</th><td><i> * messages about anything out of the ordinary.</i></td></tr>
<tr><th id="24">24</th><td><i> */</i></td></tr>
<tr><th id="25">25</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__((__section__(&quot;.init.text&quot;))) __attribute__((__cold__))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_bootup_announce_oddness" title='rcu_bootup_announce_oddness' data-ref="rcu_bootup_announce_oddness" data-ref-filename="rcu_bootup_announce_oddness">rcu_bootup_announce_oddness</dfn>(<em>void</em>)</td></tr>
<tr><th id="26">26</th><td>{</td></tr>
<tr><th id="27">27</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="1" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(<a class="macro" href="../../include/generated/autoconf.h.html#217" title="1" data-ref="_M/CONFIG_RCU_TRACE">CONFIG_RCU_TRACE</a>))</td></tr>
<tr><th id="28">28</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU event tracing is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU event tracing is enabled.\n"</q>);</td></tr>
<tr><th id="29">29</th><td>	<b>if</b> ((<a class="macro" href="../../include/linux/kconfig.h.html#71" title="1" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(<a class="macro" href="../../include/generated/autoconf.h.html#1284" title="1" data-ref="_M/CONFIG_64BIT">CONFIG_64BIT</a>) &amp;&amp; <a class="macro" href="../../include/linux/rcu_node_tree.h.html#35" title="64" data-ref="_M/RCU_FANOUT">RCU_FANOUT</a> != <var>64</var>) ||</td></tr>
<tr><th id="30">30</th><td>	    (!<a class="macro" href="../../include/linux/kconfig.h.html#71" title="1" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(<a class="macro" href="../../include/generated/autoconf.h.html#1284" title="1" data-ref="_M/CONFIG_64BIT">CONFIG_64BIT</a>) &amp;&amp; <a class="macro" href="../../include/linux/rcu_node_tree.h.html#35" title="64" data-ref="_M/RCU_FANOUT">RCU_FANOUT</a> != <var>32</var>))</td></tr>
<tr><th id="31">31</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tCONFIG_RCU_FANOUT set to non-default value of %d.\n&quot;, 64)" data-ref="_M/pr_info">pr_info</a>(<q>"\tCONFIG_RCU_FANOUT set to non-default value of %d.\n"</q>,</td></tr>
<tr><th id="32">32</th><td>			RCU_FANOUT);</td></tr>
<tr><th id="33">33</th><td>	<b>if</b> (<a class="tu ref" href="rcu.h.html#rcu_fanout_exact" title='rcu_fanout_exact' data-use='r' data-ref="rcu_fanout_exact" data-ref-filename="rcu_fanout_exact">rcu_fanout_exact</a>)</td></tr>
<tr><th id="34">34</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tHierarchical RCU autobalancing is disabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tHierarchical RCU autobalancing is disabled.\n"</q>);</td></tr>
<tr><th id="35">35</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_RCU_FAST_NO_HZ))</td></tr>
<tr><th id="36">36</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU dyntick-idle grace-period acceleration is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU dyntick-idle grace-period acceleration is enabled.\n"</q>);</td></tr>
<tr><th id="37">37</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_PROVE_RCU))</td></tr>
<tr><th id="38">38</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU lockdep checking is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU lockdep checking is enabled.\n"</q>);</td></tr>
<tr><th id="39">39</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/rcu_node_tree.h.html#60" title="2" data-ref="_M/RCU_NUM_LVLS">RCU_NUM_LVLS</a> &gt;= <var>4</var>)</td></tr>
<tr><th id="40">40</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tFour(or more)-level hierarchy is enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tFour(or more)-level hierarchy is enabled.\n"</q>);</td></tr>
<tr><th id="41">41</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/rcu_node_tree.h.html#44" title="16" data-ref="_M/RCU_FANOUT_LEAF">RCU_FANOUT_LEAF</a> != <var>16</var>)</td></tr>
<tr><th id="42">42</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBuild-time adjustment of leaf fanout to %d.\n&quot;, 16)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBuild-time adjustment of leaf fanout to %d.\n"</q>,</td></tr>
<tr><th id="43">43</th><td>			RCU_FANOUT_LEAF);</td></tr>
<tr><th id="44">44</th><td>	<b>if</b> (<a class="tu ref" href="rcu.h.html#rcu_fanout_leaf" title='rcu_fanout_leaf' data-use='r' data-ref="rcu_fanout_leaf" data-ref-filename="rcu_fanout_leaf">rcu_fanout_leaf</a> != <a class="macro" href="../../include/linux/rcu_node_tree.h.html#44" title="16" data-ref="_M/RCU_FANOUT_LEAF">RCU_FANOUT_LEAF</a>)</td></tr>
<tr><th id="45">45</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of leaf fanout to %d.\n&quot;, rcu_fanout_leaf)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of leaf fanout to %d.\n"</q>,</td></tr>
<tr><th id="46">46</th><td>			<a class="tu ref" href="rcu.h.html#rcu_fanout_leaf" title='rcu_fanout_leaf' data-use='r' data-ref="rcu_fanout_leaf" data-ref-filename="rcu_fanout_leaf">rcu_fanout_leaf</a>);</td></tr>
<tr><th id="47">47</th><td>	<b>if</b> (<a class="ref" href="../../include/linux/cpumask.h.html#nr_cpu_ids" title='nr_cpu_ids' data-ref="nr_cpu_ids" data-ref-filename="nr_cpu_ids">nr_cpu_ids</a> != <a class="macro" href="../../include/linux/threads.h.html#21" title="64" data-ref="_M/NR_CPUS">NR_CPUS</a>)</td></tr>
<tr><th id="48">48</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU restricting CPUs from NR_CPUS=%d to nr_cpu_ids=%u.\n&quot;, 64, nr_cpu_ids)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU restricting CPUs from NR_CPUS=%d to nr_cpu_ids=%u.\n"</q>, NR_CPUS, <a class="ref" href="../../include/linux/cpumask.h.html#nr_cpu_ids" title='nr_cpu_ids' data-ref="nr_cpu_ids" data-ref-filename="nr_cpu_ids">nr_cpu_ids</a>);</td></tr>
<tr><th id="49">49</th><td><u>#<span data-ppcond="49">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="50">50</th><td>	pr_info(<q>"\tRCU priority boosting: priority %d delay %d ms.\n"</q>,</td></tr>
<tr><th id="51">51</th><td>		kthread_prio, CONFIG_RCU_BOOST_DELAY);</td></tr>
<tr><th id="52">52</th><td><u>#<span data-ppcond="49">endif</span></u></td></tr>
<tr><th id="53">53</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#blimit" title='blimit' data-use='r' data-ref="blimit" data-ref-filename="blimit">blimit</a> != <a class="macro" href="tree.c.html#404" title="10" data-ref="_M/DEFAULT_RCU_BLIMIT">DEFAULT_RCU_BLIMIT</a>)</td></tr>
<tr><th id="54">54</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of callback invocation limit to %ld.\n&quot;, blimit)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback invocation limit to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#blimit" title='blimit' data-use='r' data-ref="blimit" data-ref-filename="blimit">blimit</a>);</td></tr>
<tr><th id="55">55</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#qhimark" title='qhimark' data-use='r' data-ref="qhimark" data-ref-filename="qhimark">qhimark</a> != <a class="macro" href="tree.c.html#407" title="10000" data-ref="_M/DEFAULT_RCU_QHIMARK">DEFAULT_RCU_QHIMARK</a>)</td></tr>
<tr><th id="56">56</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of callback high-water mark to %ld.\n&quot;, qhimark)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback high-water mark to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#qhimark" title='qhimark' data-use='r' data-ref="qhimark" data-ref-filename="qhimark">qhimark</a>);</td></tr>
<tr><th id="57">57</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#qlowmark" title='qlowmark' data-use='r' data-ref="qlowmark" data-ref-filename="qlowmark">qlowmark</a> != <a class="macro" href="tree.c.html#409" title="100" data-ref="_M/DEFAULT_RCU_QLOMARK">DEFAULT_RCU_QLOMARK</a>)</td></tr>
<tr><th id="58">58</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of callback low-water mark to %ld.\n&quot;, qlowmark)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of callback low-water mark to %ld.\n"</q>, <a class="tu ref" href="tree.c.html#qlowmark" title='qlowmark' data-use='r' data-ref="qlowmark" data-ref-filename="qlowmark">qlowmark</a>);</td></tr>
<tr><th id="59">59</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#jiffies_till_first_fqs" title='jiffies_till_first_fqs' data-use='r' data-ref="jiffies_till_first_fqs" data-ref-filename="jiffies_till_first_fqs">jiffies_till_first_fqs</a> != <a class="macro" href="../../include/linux/limits.h.html#16" title="(~0UL)" data-ref="_M/ULONG_MAX">ULONG_MAX</a>)</td></tr>
<tr><th id="60">60</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of first FQS scan delay to %ld jiffies.\n&quot;, jiffies_till_first_fqs)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of first FQS scan delay to %ld jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#jiffies_till_first_fqs" title='jiffies_till_first_fqs' data-use='r' data-ref="jiffies_till_first_fqs" data-ref-filename="jiffies_till_first_fqs">jiffies_till_first_fqs</a>);</td></tr>
<tr><th id="61">61</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#jiffies_till_next_fqs" title='jiffies_till_next_fqs' data-use='r' data-ref="jiffies_till_next_fqs" data-ref-filename="jiffies_till_next_fqs">jiffies_till_next_fqs</a> != <a class="macro" href="../../include/linux/limits.h.html#16" title="(~0UL)" data-ref="_M/ULONG_MAX">ULONG_MAX</a>)</td></tr>
<tr><th id="62">62</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of subsequent FQS scan delay to %ld jiffies.\n&quot;, jiffies_till_next_fqs)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of subsequent FQS scan delay to %ld jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#jiffies_till_next_fqs" title='jiffies_till_next_fqs' data-use='r' data-ref="jiffies_till_next_fqs" data-ref-filename="jiffies_till_next_fqs">jiffies_till_next_fqs</a>);</td></tr>
<tr><th id="63">63</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#jiffies_till_sched_qs" title='jiffies_till_sched_qs' data-use='r' data-ref="jiffies_till_sched_qs" data-ref-filename="jiffies_till_sched_qs">jiffies_till_sched_qs</a> != <a class="macro" href="../../include/linux/limits.h.html#16" title="(~0UL)" data-ref="_M/ULONG_MAX">ULONG_MAX</a>)</td></tr>
<tr><th id="64">64</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tBoot-time adjustment of scheduler-enlistment delay to %ld jiffies.\n&quot;, jiffies_till_sched_qs)" data-ref="_M/pr_info">pr_info</a>(<q>"\tBoot-time adjustment of scheduler-enlistment delay to %ld jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#jiffies_till_sched_qs" title='jiffies_till_sched_qs' data-use='r' data-ref="jiffies_till_sched_qs" data-ref-filename="jiffies_till_sched_qs">jiffies_till_sched_qs</a>);</td></tr>
<tr><th id="65">65</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#rcu_kick_kthreads" title='rcu_kick_kthreads' data-use='r' data-ref="rcu_kick_kthreads" data-ref-filename="rcu_kick_kthreads">rcu_kick_kthreads</a>)</td></tr>
<tr><th id="66">66</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tKick kthreads if too-long grace period.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tKick kthreads if too-long grace period.\n"</q>);</td></tr>
<tr><th id="67">67</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_DEBUG_OBJECTS_RCU_HEAD))</td></tr>
<tr><th id="68">68</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU callback double-/use-after-free debug enabled.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU callback double-/use-after-free debug enabled.\n"</q>);</td></tr>
<tr><th id="69">69</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_preinit_delay" title='gp_preinit_delay' data-use='r' data-ref="gp_preinit_delay" data-ref-filename="gp_preinit_delay">gp_preinit_delay</a>)</td></tr>
<tr><th id="70">70</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU debug GP pre-init slowdown %d jiffies.\n&quot;, gp_preinit_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP pre-init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_preinit_delay" title='gp_preinit_delay' data-use='r' data-ref="gp_preinit_delay" data-ref-filename="gp_preinit_delay">gp_preinit_delay</a>);</td></tr>
<tr><th id="71">71</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_init_delay" title='gp_init_delay' data-use='r' data-ref="gp_init_delay" data-ref-filename="gp_init_delay">gp_init_delay</a>)</td></tr>
<tr><th id="72">72</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU debug GP init slowdown %d jiffies.\n&quot;, gp_init_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_init_delay" title='gp_init_delay' data-use='r' data-ref="gp_init_delay" data-ref-filename="gp_init_delay">gp_init_delay</a>);</td></tr>
<tr><th id="73">73</th><td>	<b>if</b> (<a class="tu ref" href="tree.c.html#gp_cleanup_delay" title='gp_cleanup_delay' data-use='r' data-ref="gp_cleanup_delay" data-ref-filename="gp_cleanup_delay">gp_cleanup_delay</a>)</td></tr>
<tr><th id="74">74</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU debug GP init slowdown %d jiffies.\n&quot;, gp_cleanup_delay)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug GP init slowdown %d jiffies.\n"</q>, <a class="tu ref" href="tree.c.html#gp_cleanup_delay" title='gp_cleanup_delay' data-use='r' data-ref="gp_cleanup_delay" data-ref-filename="gp_cleanup_delay">gp_cleanup_delay</a>);</td></tr>
<tr><th id="75">75</th><td>	<b>if</b> (!<a class="tu ref" href="tree.c.html#use_softirq" title='use_softirq' data-use='r' data-ref="use_softirq" data-ref-filename="use_softirq">use_softirq</a>)</td></tr>
<tr><th id="76">76</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU_SOFTIRQ processing moved to rcuc kthreads.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU_SOFTIRQ processing moved to rcuc kthreads.\n"</q>);</td></tr>
<tr><th id="77">77</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/kconfig.h.html#71" title="0" data-ref="_M/IS_ENABLED">IS_ENABLED</a>(CONFIG_RCU_EQS_DEBUG))</td></tr>
<tr><th id="78">78</th><td>		<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;\tRCU debug extended QS entry/exit.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"\tRCU debug extended QS entry/exit.\n"</q>);</td></tr>
<tr><th id="79">79</th><td>	<a class="ref fn" href="rcu.h.html#rcupdate_announce_bootup_oddness" title='rcupdate_announce_bootup_oddness' data-ref="rcupdate_announce_bootup_oddness" data-ref-filename="rcupdate_announce_bootup_oddness">rcupdate_announce_bootup_oddness</a>();</td></tr>
<tr><th id="80">80</th><td>}</td></tr>
<tr><th id="81">81</th><td></td></tr>
<tr><th id="82">82</th><td><u>#<span data-ppcond="82">ifdef</span> <span class="macro" data-ref="_M/CONFIG_PREEMPT_RCU">CONFIG_PREEMPT_RCU</span></u></td></tr>
<tr><th id="83">83</th><td></td></tr>
<tr><th id="84">84</th><td><em>static</em> <em>void</em> rcu_report_exp_rnp(<b>struct</b> rcu_node *rnp, bool wake);</td></tr>
<tr><th id="85">85</th><td><em>static</em> <em>void</em> rcu_read_unlock_special(<b>struct</b> task_struct *t);</td></tr>
<tr><th id="86">86</th><td></td></tr>
<tr><th id="87">87</th><td><i>/*</i></td></tr>
<tr><th id="88">88</th><td><i> * Tell them what RCU they are running.</i></td></tr>
<tr><th id="89">89</th><td><i> */</i></td></tr>
<tr><th id="90">90</th><td><em>static</em> <em>void</em> __init rcu_bootup_announce(<em>void</em>)</td></tr>
<tr><th id="91">91</th><td>{</td></tr>
<tr><th id="92">92</th><td>	pr_info(<q>"Preemptible hierarchical RCU implementation.\n"</q>);</td></tr>
<tr><th id="93">93</th><td>	rcu_bootup_announce_oddness();</td></tr>
<tr><th id="94">94</th><td>}</td></tr>
<tr><th id="95">95</th><td></td></tr>
<tr><th id="96">96</th><td><i>/* Flags for rcu_preempt_ctxt_queue() decision table. */</i></td></tr>
<tr><th id="97">97</th><td><u>#define RCU_GP_TASKS	0x8</u></td></tr>
<tr><th id="98">98</th><td><u>#define RCU_EXP_TASKS	0x4</u></td></tr>
<tr><th id="99">99</th><td><u>#define RCU_GP_BLKD	0x2</u></td></tr>
<tr><th id="100">100</th><td><u>#define RCU_EXP_BLKD	0x1</u></td></tr>
<tr><th id="101">101</th><td></td></tr>
<tr><th id="102">102</th><td><i>/*</i></td></tr>
<tr><th id="103">103</th><td><i> * Queues a task preempted within an RCU-preempt read-side critical</i></td></tr>
<tr><th id="104">104</th><td><i> * section into the appropriate location within the -&gt;blkd_tasks list,</i></td></tr>
<tr><th id="105">105</th><td><i> * depending on the states of any ongoing normal and expedited grace</i></td></tr>
<tr><th id="106">106</th><td><i> * periods.  The -&gt;gp_tasks pointer indicates which element the normal</i></td></tr>
<tr><th id="107">107</th><td><i> * grace period is waiting on (NULL if none), and the -&gt;exp_tasks pointer</i></td></tr>
<tr><th id="108">108</th><td><i> * indicates which element the expedited grace period is waiting on (again,</i></td></tr>
<tr><th id="109">109</th><td><i> * NULL if none).  If a grace period is waiting on a given element in the</i></td></tr>
<tr><th id="110">110</th><td><i> * -&gt;blkd_tasks list, it also waits on all subsequent elements.  Thus,</i></td></tr>
<tr><th id="111">111</th><td><i> * adding a task to the tail of the list blocks any grace period that is</i></td></tr>
<tr><th id="112">112</th><td><i> * already waiting on one of the elements.  In contrast, adding a task</i></td></tr>
<tr><th id="113">113</th><td><i> * to the head of the list won't block any grace period that is already</i></td></tr>
<tr><th id="114">114</th><td><i> * waiting on one of the elements.</i></td></tr>
<tr><th id="115">115</th><td><i> *</i></td></tr>
<tr><th id="116">116</th><td><i> * This queuing is imprecise, and can sometimes make an ongoing grace</i></td></tr>
<tr><th id="117">117</th><td><i> * period wait for a task that is not strictly speaking blocking it.</i></td></tr>
<tr><th id="118">118</th><td><i> * Given the choice, we needlessly block a normal grace period rather than</i></td></tr>
<tr><th id="119">119</th><td><i> * blocking an expedited grace period.</i></td></tr>
<tr><th id="120">120</th><td><i> *</i></td></tr>
<tr><th id="121">121</th><td><i> * Note that an endless sequence of expedited grace periods still cannot</i></td></tr>
<tr><th id="122">122</th><td><i> * indefinitely postpone a normal grace period.  Eventually, all of the</i></td></tr>
<tr><th id="123">123</th><td><i> * fixed number of preempted tasks blocking the normal grace period that are</i></td></tr>
<tr><th id="124">124</th><td><i> * not also blocking the expedited grace period will resume and complete</i></td></tr>
<tr><th id="125">125</th><td><i> * their RCU read-side critical sections.  At that point, the -&gt;gp_tasks</i></td></tr>
<tr><th id="126">126</th><td><i> * pointer will equal the -&gt;exp_tasks pointer, at which point the end of</i></td></tr>
<tr><th id="127">127</th><td><i> * the corresponding expedited grace period will also be the end of the</i></td></tr>
<tr><th id="128">128</th><td><i> * normal grace period.</i></td></tr>
<tr><th id="129">129</th><td><i> */</i></td></tr>
<tr><th id="130">130</th><td><em>static</em> <em>void</em> rcu_preempt_ctxt_queue(<b>struct</b> rcu_node *rnp, <b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="131">131</th><td>	__releases(rnp-&gt;lock) <i>/* But leaves rrupts disabled. */</i></td></tr>
<tr><th id="132">132</th><td>{</td></tr>
<tr><th id="133">133</th><td>	<em>int</em> blkd_state = (rnp-&gt;gp_tasks ? RCU_GP_TASKS : <var>0</var>) +</td></tr>
<tr><th id="134">134</th><td>			 (rnp-&gt;exp_tasks ? RCU_EXP_TASKS : <var>0</var>) +</td></tr>
<tr><th id="135">135</th><td>			 (rnp-&gt;qsmask &amp; rdp-&gt;grpmask ? RCU_GP_BLKD : <var>0</var>) +</td></tr>
<tr><th id="136">136</th><td>			 (rnp-&gt;expmask &amp; rdp-&gt;grpmask ? RCU_EXP_BLKD : <var>0</var>);</td></tr>
<tr><th id="137">137</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="138">138</th><td></td></tr>
<tr><th id="139">139</th><td>	raw_lockdep_assert_held_rcu_node(rnp);</td></tr>
<tr><th id="140">140</th><td>	WARN_ON_ONCE(rdp-&gt;mynode != rnp);</td></tr>
<tr><th id="141">141</th><td>	WARN_ON_ONCE(!rcu_is_leaf_node(rnp));</td></tr>
<tr><th id="142">142</th><td>	<i>/* RCU better not be waiting on newly onlined CPUs! */</i></td></tr>
<tr><th id="143">143</th><td>	WARN_ON_ONCE(rnp-&gt;qsmaskinitnext &amp; ~rnp-&gt;qsmaskinit &amp; rnp-&gt;qsmask &amp;</td></tr>
<tr><th id="144">144</th><td>		     rdp-&gt;grpmask);</td></tr>
<tr><th id="145">145</th><td></td></tr>
<tr><th id="146">146</th><td>	<i>/*</i></td></tr>
<tr><th id="147">147</th><td><i>	 * Decide where to queue the newly blocked task.  In theory,</i></td></tr>
<tr><th id="148">148</th><td><i>	 * this could be an if-statement.  In practice, when I tried</i></td></tr>
<tr><th id="149">149</th><td><i>	 * that, it was quite messy.</i></td></tr>
<tr><th id="150">150</th><td><i>	 */</i></td></tr>
<tr><th id="151">151</th><td>	<b>switch</b> (blkd_state) {</td></tr>
<tr><th id="152">152</th><td>	<b>case</b> <var>0</var>:</td></tr>
<tr><th id="153">153</th><td>	<b>case</b>                RCU_EXP_TASKS:</td></tr>
<tr><th id="154">154</th><td>	<b>case</b>                RCU_EXP_TASKS + RCU_GP_BLKD:</td></tr>
<tr><th id="155">155</th><td>	<b>case</b> RCU_GP_TASKS:</td></tr>
<tr><th id="156">156</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS:</td></tr>
<tr><th id="157">157</th><td></td></tr>
<tr><th id="158">158</th><td>		<i>/*</i></td></tr>
<tr><th id="159">159</th><td><i>		 * Blocking neither GP, or first task blocking the normal</i></td></tr>
<tr><th id="160">160</th><td><i>		 * GP but not blocking the already-waiting expedited GP.</i></td></tr>
<tr><th id="161">161</th><td><i>		 * Queue at the head of the list to avoid unnecessarily</i></td></tr>
<tr><th id="162">162</th><td><i>		 * blocking the already-waiting GPs.</i></td></tr>
<tr><th id="163">163</th><td><i>		 */</i></td></tr>
<tr><th id="164">164</th><td>		list_add(&amp;t-&gt;rcu_node_entry, &amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="165">165</th><td>		<b>break</b>;</td></tr>
<tr><th id="166">166</th><td></td></tr>
<tr><th id="167">167</th><td>	<b>case</b>                                              RCU_EXP_BLKD:</td></tr>
<tr><th id="168">168</th><td>	<b>case</b>                                RCU_GP_BLKD:</td></tr>
<tr><th id="169">169</th><td>	<b>case</b>                                RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="170">170</th><td>	<b>case</b> RCU_GP_TASKS +                               RCU_EXP_BLKD:</td></tr>
<tr><th id="171">171</th><td>	<b>case</b> RCU_GP_TASKS +                 RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="172">172</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="173">173</th><td></td></tr>
<tr><th id="174">174</th><td>		<i>/*</i></td></tr>
<tr><th id="175">175</th><td><i>		 * First task arriving that blocks either GP, or first task</i></td></tr>
<tr><th id="176">176</th><td><i>		 * arriving that blocks the expedited GP (with the normal</i></td></tr>
<tr><th id="177">177</th><td><i>		 * GP already waiting), or a task arriving that blocks</i></td></tr>
<tr><th id="178">178</th><td><i>		 * both GPs with both GPs already waiting.  Queue at the</i></td></tr>
<tr><th id="179">179</th><td><i>		 * tail of the list to avoid any GP waiting on any of the</i></td></tr>
<tr><th id="180">180</th><td><i>		 * already queued tasks that are not blocking it.</i></td></tr>
<tr><th id="181">181</th><td><i>		 */</i></td></tr>
<tr><th id="182">182</th><td>		list_add_tail(&amp;t-&gt;rcu_node_entry, &amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="183">183</th><td>		<b>break</b>;</td></tr>
<tr><th id="184">184</th><td></td></tr>
<tr><th id="185">185</th><td>	<b>case</b>                RCU_EXP_TASKS +               RCU_EXP_BLKD:</td></tr>
<tr><th id="186">186</th><td>	<b>case</b>                RCU_EXP_TASKS + RCU_GP_BLKD + RCU_EXP_BLKD:</td></tr>
<tr><th id="187">187</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS +               RCU_EXP_BLKD:</td></tr>
<tr><th id="188">188</th><td></td></tr>
<tr><th id="189">189</th><td>		<i>/*</i></td></tr>
<tr><th id="190">190</th><td><i>		 * Second or subsequent task blocking the expedited GP.</i></td></tr>
<tr><th id="191">191</th><td><i>		 * The task either does not block the normal GP, or is the</i></td></tr>
<tr><th id="192">192</th><td><i>		 * first task blocking the normal GP.  Queue just after</i></td></tr>
<tr><th id="193">193</th><td><i>		 * the first task blocking the expedited GP.</i></td></tr>
<tr><th id="194">194</th><td><i>		 */</i></td></tr>
<tr><th id="195">195</th><td>		list_add(&amp;t-&gt;rcu_node_entry, rnp-&gt;exp_tasks);</td></tr>
<tr><th id="196">196</th><td>		<b>break</b>;</td></tr>
<tr><th id="197">197</th><td></td></tr>
<tr><th id="198">198</th><td>	<b>case</b> RCU_GP_TASKS +                 RCU_GP_BLKD:</td></tr>
<tr><th id="199">199</th><td>	<b>case</b> RCU_GP_TASKS + RCU_EXP_TASKS + RCU_GP_BLKD:</td></tr>
<tr><th id="200">200</th><td></td></tr>
<tr><th id="201">201</th><td>		<i>/*</i></td></tr>
<tr><th id="202">202</th><td><i>		 * Second or subsequent task blocking the normal GP.</i></td></tr>
<tr><th id="203">203</th><td><i>		 * The task does not block the expedited GP. Queue just</i></td></tr>
<tr><th id="204">204</th><td><i>		 * after the first task blocking the normal GP.</i></td></tr>
<tr><th id="205">205</th><td><i>		 */</i></td></tr>
<tr><th id="206">206</th><td>		list_add(&amp;t-&gt;rcu_node_entry, rnp-&gt;gp_tasks);</td></tr>
<tr><th id="207">207</th><td>		<b>break</b>;</td></tr>
<tr><th id="208">208</th><td></td></tr>
<tr><th id="209">209</th><td>	<b>default</b>:</td></tr>
<tr><th id="210">210</th><td></td></tr>
<tr><th id="211">211</th><td>		<i>/* Yet another exercise in excessive paranoia. */</i></td></tr>
<tr><th id="212">212</th><td>		WARN_ON_ONCE(<var>1</var>);</td></tr>
<tr><th id="213">213</th><td>		<b>break</b>;</td></tr>
<tr><th id="214">214</th><td>	}</td></tr>
<tr><th id="215">215</th><td></td></tr>
<tr><th id="216">216</th><td>	<i>/*</i></td></tr>
<tr><th id="217">217</th><td><i>	 * We have now queued the task.  If it was the first one to</i></td></tr>
<tr><th id="218">218</th><td><i>	 * block either grace period, update the -&gt;gp_tasks and/or</i></td></tr>
<tr><th id="219">219</th><td><i>	 * -&gt;exp_tasks pointers, respectively, to reference the newly</i></td></tr>
<tr><th id="220">220</th><td><i>	 * blocked tasks.</i></td></tr>
<tr><th id="221">221</th><td><i>	 */</i></td></tr>
<tr><th id="222">222</th><td>	<b>if</b> (!rnp-&gt;gp_tasks &amp;&amp; (blkd_state &amp; RCU_GP_BLKD)) {</td></tr>
<tr><th id="223">223</th><td>		rnp-&gt;gp_tasks = &amp;t-&gt;rcu_node_entry;</td></tr>
<tr><th id="224">224</th><td>		WARN_ON_ONCE(rnp-&gt;completedqs == rnp-&gt;gp_seq);</td></tr>
<tr><th id="225">225</th><td>	}</td></tr>
<tr><th id="226">226</th><td>	<b>if</b> (!rnp-&gt;exp_tasks &amp;&amp; (blkd_state &amp; RCU_EXP_BLKD))</td></tr>
<tr><th id="227">227</th><td>		rnp-&gt;exp_tasks = &amp;t-&gt;rcu_node_entry;</td></tr>
<tr><th id="228">228</th><td>	WARN_ON_ONCE(!(blkd_state &amp; RCU_GP_BLKD) !=</td></tr>
<tr><th id="229">229</th><td>		     !(rnp-&gt;qsmask &amp; rdp-&gt;grpmask));</td></tr>
<tr><th id="230">230</th><td>	WARN_ON_ONCE(!(blkd_state &amp; RCU_EXP_BLKD) !=</td></tr>
<tr><th id="231">231</th><td>		     !(rnp-&gt;expmask &amp; rdp-&gt;grpmask));</td></tr>
<tr><th id="232">232</th><td>	raw_spin_unlock_rcu_node(rnp); <i>/* interrupts remain disabled. */</i></td></tr>
<tr><th id="233">233</th><td></td></tr>
<tr><th id="234">234</th><td>	<i>/*</i></td></tr>
<tr><th id="235">235</th><td><i>	 * Report the quiescent state for the expedited GP.  This expedited</i></td></tr>
<tr><th id="236">236</th><td><i>	 * GP should not be able to end until we report, so there should be</i></td></tr>
<tr><th id="237">237</th><td><i>	 * no need to check for a subsequent expedited GP.  (Though we are</i></td></tr>
<tr><th id="238">238</th><td><i>	 * still in a quiescent state in any case.)</i></td></tr>
<tr><th id="239">239</th><td><i>	 */</i></td></tr>
<tr><th id="240">240</th><td>	<b>if</b> (blkd_state &amp; RCU_EXP_BLKD &amp;&amp; rdp-&gt;exp_deferred_qs)</td></tr>
<tr><th id="241">241</th><td>		rcu_report_exp_rdp(rdp);</td></tr>
<tr><th id="242">242</th><td>	<b>else</b></td></tr>
<tr><th id="243">243</th><td>		WARN_ON_ONCE(rdp-&gt;exp_deferred_qs);</td></tr>
<tr><th id="244">244</th><td>}</td></tr>
<tr><th id="245">245</th><td></td></tr>
<tr><th id="246">246</th><td><i>/*</i></td></tr>
<tr><th id="247">247</th><td><i> * Record a preemptible-RCU quiescent state for the specified CPU.</i></td></tr>
<tr><th id="248">248</th><td><i> * Note that this does not necessarily mean that the task currently running</i></td></tr>
<tr><th id="249">249</th><td><i> * on the CPU is in a quiescent state:  Instead, it means that the current</i></td></tr>
<tr><th id="250">250</th><td><i> * grace period need not wait on any RCU read-side critical section that</i></td></tr>
<tr><th id="251">251</th><td><i> * starts later on this CPU.  It also means that if the current task is</i></td></tr>
<tr><th id="252">252</th><td><i> * in an RCU read-side critical section, it has already added itself to</i></td></tr>
<tr><th id="253">253</th><td><i> * some leaf rcu_node structure's -&gt;blkd_tasks list.  In addition to the</i></td></tr>
<tr><th id="254">254</th><td><i> * current task, there might be any number of other tasks blocked while</i></td></tr>
<tr><th id="255">255</th><td><i> * in an RCU read-side critical section.</i></td></tr>
<tr><th id="256">256</th><td><i> *</i></td></tr>
<tr><th id="257">257</th><td><i> * Callers to this function must disable preemption.</i></td></tr>
<tr><th id="258">258</th><td><i> */</i></td></tr>
<tr><th id="259">259</th><td><em>static</em> <em>void</em> rcu_qs(<em>void</em>)</td></tr>
<tr><th id="260">260</th><td>{</td></tr>
<tr><th id="261">261</th><td>	RCU_LOCKDEP_WARN(preemptible(), <q>"rcu_qs() invoked with preemption enabled!!!\n"</q>);</td></tr>
<tr><th id="262">262</th><td>	<b>if</b> (__this_cpu_read(rcu_data.cpu_no_qs.s)) {</td></tr>
<tr><th id="263">263</th><td>		trace_rcu_grace_period(TPS(<q>"rcu_preempt"</q>),</td></tr>
<tr><th id="264">264</th><td>				       __this_cpu_read(rcu_data.gp_seq),</td></tr>
<tr><th id="265">265</th><td>				       TPS(<q>"cpuqs"</q>));</td></tr>
<tr><th id="266">266</th><td>		__this_cpu_write(rcu_data.cpu_no_qs.b.norm, false);</td></tr>
<tr><th id="267">267</th><td>		barrier(); <i>/* Coordinate with rcu_flavor_sched_clock_irq(). */</i></td></tr>
<tr><th id="268">268</th><td>		WRITE_ONCE(current-&gt;rcu_read_unlock_special.b.need_qs, false);</td></tr>
<tr><th id="269">269</th><td>	}</td></tr>
<tr><th id="270">270</th><td>}</td></tr>
<tr><th id="271">271</th><td></td></tr>
<tr><th id="272">272</th><td><i>/*</i></td></tr>
<tr><th id="273">273</th><td><i> * We have entered the scheduler, and the current task might soon be</i></td></tr>
<tr><th id="274">274</th><td><i> * context-switched away from.  If this task is in an RCU read-side</i></td></tr>
<tr><th id="275">275</th><td><i> * critical section, we will no longer be able to rely on the CPU to</i></td></tr>
<tr><th id="276">276</th><td><i> * record that fact, so we enqueue the task on the blkd_tasks list.</i></td></tr>
<tr><th id="277">277</th><td><i> * The task will dequeue itself when it exits the outermost enclosing</i></td></tr>
<tr><th id="278">278</th><td><i> * RCU read-side critical section.  Therefore, the current grace period</i></td></tr>
<tr><th id="279">279</th><td><i> * cannot be permitted to complete until the blkd_tasks list entries</i></td></tr>
<tr><th id="280">280</th><td><i> * predating the current grace period drain, in other words, until</i></td></tr>
<tr><th id="281">281</th><td><i> * rnp-&gt;gp_tasks becomes NULL.</i></td></tr>
<tr><th id="282">282</th><td><i> *</i></td></tr>
<tr><th id="283">283</th><td><i> * Caller must disable interrupts.</i></td></tr>
<tr><th id="284">284</th><td><i> */</i></td></tr>
<tr><th id="285">285</th><td><em>void</em> rcu_note_context_switch(bool preempt)</td></tr>
<tr><th id="286">286</th><td>{</td></tr>
<tr><th id="287">287</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="288">288</th><td>	<b>struct</b> rcu_data *rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="289">289</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="290">290</th><td></td></tr>
<tr><th id="291">291</th><td>	barrier(); <i>/* Avoid RCU read-side critical sections leaking down. */</i></td></tr>
<tr><th id="292">292</th><td>	trace_rcu_utilization(TPS(<q>"Start context switch"</q>));</td></tr>
<tr><th id="293">293</th><td>	lockdep_assert_irqs_disabled();</td></tr>
<tr><th id="294">294</th><td>	WARN_ON_ONCE(!preempt &amp;&amp; t-&gt;rcu_read_lock_nesting &gt; <var>0</var>);</td></tr>
<tr><th id="295">295</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting &gt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="296">296</th><td>	    !t-&gt;rcu_read_unlock_special.b.blocked) {</td></tr>
<tr><th id="297">297</th><td></td></tr>
<tr><th id="298">298</th><td>		<i>/* Possibly blocking in an RCU read-side critical section. */</i></td></tr>
<tr><th id="299">299</th><td>		rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="300">300</th><td>		raw_spin_lock_rcu_node(rnp);</td></tr>
<tr><th id="301">301</th><td>		t-&gt;rcu_read_unlock_special.b.blocked = true;</td></tr>
<tr><th id="302">302</th><td>		t-&gt;rcu_blocked_node = rnp;</td></tr>
<tr><th id="303">303</th><td></td></tr>
<tr><th id="304">304</th><td>		<i>/*</i></td></tr>
<tr><th id="305">305</th><td><i>		 * Verify the CPU's sanity, trace the preemption, and</i></td></tr>
<tr><th id="306">306</th><td><i>		 * then queue the task as required based on the states</i></td></tr>
<tr><th id="307">307</th><td><i>		 * of any ongoing and expedited grace periods.</i></td></tr>
<tr><th id="308">308</th><td><i>		 */</i></td></tr>
<tr><th id="309">309</th><td>		WARN_ON_ONCE((rdp-&gt;grpmask &amp; rcu_rnp_online_cpus(rnp)) == <var>0</var>);</td></tr>
<tr><th id="310">310</th><td>		WARN_ON_ONCE(!list_empty(&amp;t-&gt;rcu_node_entry));</td></tr>
<tr><th id="311">311</th><td>		trace_rcu_preempt_task(rcu_state.name,</td></tr>
<tr><th id="312">312</th><td>				       t-&gt;pid,</td></tr>
<tr><th id="313">313</th><td>				       (rnp-&gt;qsmask &amp; rdp-&gt;grpmask)</td></tr>
<tr><th id="314">314</th><td>				       ? rnp-&gt;gp_seq</td></tr>
<tr><th id="315">315</th><td>				       : rcu_seq_snap(&amp;rnp-&gt;gp_seq));</td></tr>
<tr><th id="316">316</th><td>		rcu_preempt_ctxt_queue(rnp, rdp);</td></tr>
<tr><th id="317">317</th><td>	} <b>else</b> <b>if</b> (t-&gt;rcu_read_lock_nesting &lt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="318">318</th><td>		   t-&gt;rcu_read_unlock_special.s) {</td></tr>
<tr><th id="319">319</th><td></td></tr>
<tr><th id="320">320</th><td>		<i>/*</i></td></tr>
<tr><th id="321">321</th><td><i>		 * Complete exit from RCU read-side critical section on</i></td></tr>
<tr><th id="322">322</th><td><i>		 * behalf of preempted instance of __rcu_read_unlock().</i></td></tr>
<tr><th id="323">323</th><td><i>		 */</i></td></tr>
<tr><th id="324">324</th><td>		rcu_read_unlock_special(t);</td></tr>
<tr><th id="325">325</th><td>		rcu_preempt_deferred_qs(t);</td></tr>
<tr><th id="326">326</th><td>	} <b>else</b> {</td></tr>
<tr><th id="327">327</th><td>		rcu_preempt_deferred_qs(t);</td></tr>
<tr><th id="328">328</th><td>	}</td></tr>
<tr><th id="329">329</th><td></td></tr>
<tr><th id="330">330</th><td>	<i>/*</i></td></tr>
<tr><th id="331">331</th><td><i>	 * Either we were not in an RCU read-side critical section to</i></td></tr>
<tr><th id="332">332</th><td><i>	 * begin with, or we have now recorded that critical section</i></td></tr>
<tr><th id="333">333</th><td><i>	 * globally.  Either way, we can now note a quiescent state</i></td></tr>
<tr><th id="334">334</th><td><i>	 * for this CPU.  Again, if we were in an RCU read-side critical</i></td></tr>
<tr><th id="335">335</th><td><i>	 * section, and if that critical section was blocking the current</i></td></tr>
<tr><th id="336">336</th><td><i>	 * grace period, then the fact that the task has been enqueued</i></td></tr>
<tr><th id="337">337</th><td><i>	 * means that we continue to block the current grace period.</i></td></tr>
<tr><th id="338">338</th><td><i>	 */</i></td></tr>
<tr><th id="339">339</th><td>	rcu_qs();</td></tr>
<tr><th id="340">340</th><td>	<b>if</b> (rdp-&gt;exp_deferred_qs)</td></tr>
<tr><th id="341">341</th><td>		rcu_report_exp_rdp(rdp);</td></tr>
<tr><th id="342">342</th><td>	trace_rcu_utilization(TPS(<q>"End context switch"</q>));</td></tr>
<tr><th id="343">343</th><td>	barrier(); <i>/* Avoid RCU read-side critical sections leaking up. */</i></td></tr>
<tr><th id="344">344</th><td>}</td></tr>
<tr><th id="345">345</th><td>EXPORT_SYMBOL_GPL(rcu_note_context_switch);</td></tr>
<tr><th id="346">346</th><td></td></tr>
<tr><th id="347">347</th><td><i>/*</i></td></tr>
<tr><th id="348">348</th><td><i> * Check for preempted RCU readers blocking the current grace period</i></td></tr>
<tr><th id="349">349</th><td><i> * for the specified rcu_node structure.  If the caller needs a reliable</i></td></tr>
<tr><th id="350">350</th><td><i> * answer, it must hold the rcu_node's -&gt;lock.</i></td></tr>
<tr><th id="351">351</th><td><i> */</i></td></tr>
<tr><th id="352">352</th><td><em>static</em> <em>int</em> rcu_preempt_blocked_readers_cgp(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="353">353</th><td>{</td></tr>
<tr><th id="354">354</th><td>	<b>return</b> rnp-&gt;gp_tasks != NULL;</td></tr>
<tr><th id="355">355</th><td>}</td></tr>
<tr><th id="356">356</th><td></td></tr>
<tr><th id="357">357</th><td><i>/* Bias and limit values for -&gt;rcu_read_lock_nesting. */</i></td></tr>
<tr><th id="358">358</th><td><u>#define RCU_NEST_BIAS INT_MAX</u></td></tr>
<tr><th id="359">359</th><td><u>#define RCU_NEST_NMAX (-INT_MAX / 2)</u></td></tr>
<tr><th id="360">360</th><td><u>#define RCU_NEST_PMAX (INT_MAX / 2)</u></td></tr>
<tr><th id="361">361</th><td></td></tr>
<tr><th id="362">362</th><td><i>/*</i></td></tr>
<tr><th id="363">363</th><td><i> * Preemptible RCU implementation for rcu_read_lock().</i></td></tr>
<tr><th id="364">364</th><td><i> * Just increment -&gt;rcu_read_lock_nesting, shared state will be updated</i></td></tr>
<tr><th id="365">365</th><td><i> * if we block.</i></td></tr>
<tr><th id="366">366</th><td><i> */</i></td></tr>
<tr><th id="367">367</th><td><em>void</em> __rcu_read_lock(<em>void</em>)</td></tr>
<tr><th id="368">368</th><td>{</td></tr>
<tr><th id="369">369</th><td>	current-&gt;rcu_read_lock_nesting++;</td></tr>
<tr><th id="370">370</th><td>	<b>if</b> (IS_ENABLED(CONFIG_PROVE_LOCKING))</td></tr>
<tr><th id="371">371</th><td>		WARN_ON_ONCE(current-&gt;rcu_read_lock_nesting &gt; RCU_NEST_PMAX);</td></tr>
<tr><th id="372">372</th><td>	barrier();  <i>/* critical section after entry code. */</i></td></tr>
<tr><th id="373">373</th><td>}</td></tr>
<tr><th id="374">374</th><td>EXPORT_SYMBOL_GPL(__rcu_read_lock);</td></tr>
<tr><th id="375">375</th><td></td></tr>
<tr><th id="376">376</th><td><i>/*</i></td></tr>
<tr><th id="377">377</th><td><i> * Preemptible RCU implementation for rcu_read_unlock().</i></td></tr>
<tr><th id="378">378</th><td><i> * Decrement -&gt;rcu_read_lock_nesting.  If the result is zero (outermost</i></td></tr>
<tr><th id="379">379</th><td><i> * rcu_read_unlock()) and -&gt;rcu_read_unlock_special is non-zero, then</i></td></tr>
<tr><th id="380">380</th><td><i> * invoke rcu_read_unlock_special() to clean up after a context switch</i></td></tr>
<tr><th id="381">381</th><td><i> * in an RCU read-side critical section and other special cases.</i></td></tr>
<tr><th id="382">382</th><td><i> */</i></td></tr>
<tr><th id="383">383</th><td><em>void</em> __rcu_read_unlock(<em>void</em>)</td></tr>
<tr><th id="384">384</th><td>{</td></tr>
<tr><th id="385">385</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="386">386</th><td></td></tr>
<tr><th id="387">387</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting != <var>1</var>) {</td></tr>
<tr><th id="388">388</th><td>		--t-&gt;rcu_read_lock_nesting;</td></tr>
<tr><th id="389">389</th><td>	} <b>else</b> {</td></tr>
<tr><th id="390">390</th><td>		barrier();  <i>/* critical section before exit code. */</i></td></tr>
<tr><th id="391">391</th><td>		t-&gt;rcu_read_lock_nesting = -RCU_NEST_BIAS;</td></tr>
<tr><th id="392">392</th><td>		barrier();  <i>/* assign before -&gt;rcu_read_unlock_special load */</i></td></tr>
<tr><th id="393">393</th><td>		<b>if</b> (unlikely(READ_ONCE(t-&gt;rcu_read_unlock_special.s)))</td></tr>
<tr><th id="394">394</th><td>			rcu_read_unlock_special(t);</td></tr>
<tr><th id="395">395</th><td>		barrier();  <i>/* -&gt;rcu_read_unlock_special load before assign */</i></td></tr>
<tr><th id="396">396</th><td>		t-&gt;rcu_read_lock_nesting = <var>0</var>;</td></tr>
<tr><th id="397">397</th><td>	}</td></tr>
<tr><th id="398">398</th><td>	<b>if</b> (IS_ENABLED(CONFIG_PROVE_LOCKING)) {</td></tr>
<tr><th id="399">399</th><td>		<em>int</em> rrln = t-&gt;rcu_read_lock_nesting;</td></tr>
<tr><th id="400">400</th><td></td></tr>
<tr><th id="401">401</th><td>		WARN_ON_ONCE(rrln &lt; <var>0</var> &amp;&amp; rrln &gt; RCU_NEST_NMAX);</td></tr>
<tr><th id="402">402</th><td>	}</td></tr>
<tr><th id="403">403</th><td>}</td></tr>
<tr><th id="404">404</th><td>EXPORT_SYMBOL_GPL(__rcu_read_unlock);</td></tr>
<tr><th id="405">405</th><td></td></tr>
<tr><th id="406">406</th><td><i>/*</i></td></tr>
<tr><th id="407">407</th><td><i> * Advance a -&gt;blkd_tasks-list pointer to the next entry, instead</i></td></tr>
<tr><th id="408">408</th><td><i> * returning NULL if at the end of the list.</i></td></tr>
<tr><th id="409">409</th><td><i> */</i></td></tr>
<tr><th id="410">410</th><td><em>static</em> <b>struct</b> list_head *rcu_next_node_entry(<b>struct</b> task_struct *t,</td></tr>
<tr><th id="411">411</th><td>					     <b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="412">412</th><td>{</td></tr>
<tr><th id="413">413</th><td>	<b>struct</b> list_head *np;</td></tr>
<tr><th id="414">414</th><td></td></tr>
<tr><th id="415">415</th><td>	np = t-&gt;rcu_node_entry.next;</td></tr>
<tr><th id="416">416</th><td>	<b>if</b> (np == &amp;rnp-&gt;blkd_tasks)</td></tr>
<tr><th id="417">417</th><td>		np = NULL;</td></tr>
<tr><th id="418">418</th><td>	<b>return</b> np;</td></tr>
<tr><th id="419">419</th><td>}</td></tr>
<tr><th id="420">420</th><td></td></tr>
<tr><th id="421">421</th><td><i>/*</i></td></tr>
<tr><th id="422">422</th><td><i> * Return true if the specified rcu_node structure has tasks that were</i></td></tr>
<tr><th id="423">423</th><td><i> * preempted within an RCU read-side critical section.</i></td></tr>
<tr><th id="424">424</th><td><i> */</i></td></tr>
<tr><th id="425">425</th><td><em>static</em> bool rcu_preempt_has_tasks(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="426">426</th><td>{</td></tr>
<tr><th id="427">427</th><td>	<b>return</b> !list_empty(&amp;rnp-&gt;blkd_tasks);</td></tr>
<tr><th id="428">428</th><td>}</td></tr>
<tr><th id="429">429</th><td></td></tr>
<tr><th id="430">430</th><td><i>/*</i></td></tr>
<tr><th id="431">431</th><td><i> * Report deferred quiescent states.  The deferral time can</i></td></tr>
<tr><th id="432">432</th><td><i> * be quite short, for example, in the case of the call from</i></td></tr>
<tr><th id="433">433</th><td><i> * rcu_read_unlock_special().</i></td></tr>
<tr><th id="434">434</th><td><i> */</i></td></tr>
<tr><th id="435">435</th><td><em>static</em> <em>void</em></td></tr>
<tr><th id="436">436</th><td>rcu_preempt_deferred_qs_irqrestore(<b>struct</b> task_struct *t, <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="437">437</th><td>{</td></tr>
<tr><th id="438">438</th><td>	bool empty_exp;</td></tr>
<tr><th id="439">439</th><td>	bool empty_norm;</td></tr>
<tr><th id="440">440</th><td>	bool empty_exp_now;</td></tr>
<tr><th id="441">441</th><td>	<b>struct</b> list_head *np;</td></tr>
<tr><th id="442">442</th><td>	bool drop_boost_mutex = false;</td></tr>
<tr><th id="443">443</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="444">444</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="445">445</th><td>	<b>union</b> rcu_special special;</td></tr>
<tr><th id="446">446</th><td></td></tr>
<tr><th id="447">447</th><td>	<i>/*</i></td></tr>
<tr><th id="448">448</th><td><i>	 * If RCU core is waiting for this CPU to exit its critical section,</i></td></tr>
<tr><th id="449">449</th><td><i>	 * report the fact that it has exited.  Because irqs are disabled,</i></td></tr>
<tr><th id="450">450</th><td><i>	 * t-&gt;rcu_read_unlock_special cannot change.</i></td></tr>
<tr><th id="451">451</th><td><i>	 */</i></td></tr>
<tr><th id="452">452</th><td>	special = t-&gt;rcu_read_unlock_special;</td></tr>
<tr><th id="453">453</th><td>	rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="454">454</th><td>	<b>if</b> (!special.s &amp;&amp; !rdp-&gt;exp_deferred_qs) {</td></tr>
<tr><th id="455">455</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="456">456</th><td>		<b>return</b>;</td></tr>
<tr><th id="457">457</th><td>	}</td></tr>
<tr><th id="458">458</th><td>	t-&gt;rcu_read_unlock_special.b.deferred_qs = false;</td></tr>
<tr><th id="459">459</th><td>	<b>if</b> (special.b.need_qs) {</td></tr>
<tr><th id="460">460</th><td>		rcu_qs();</td></tr>
<tr><th id="461">461</th><td>		t-&gt;rcu_read_unlock_special.b.need_qs = false;</td></tr>
<tr><th id="462">462</th><td>		<b>if</b> (!t-&gt;rcu_read_unlock_special.s &amp;&amp; !rdp-&gt;exp_deferred_qs) {</td></tr>
<tr><th id="463">463</th><td>			local_irq_restore(flags);</td></tr>
<tr><th id="464">464</th><td>			<b>return</b>;</td></tr>
<tr><th id="465">465</th><td>		}</td></tr>
<tr><th id="466">466</th><td>	}</td></tr>
<tr><th id="467">467</th><td></td></tr>
<tr><th id="468">468</th><td>	<i>/*</i></td></tr>
<tr><th id="469">469</th><td><i>	 * Respond to a request by an expedited grace period for a</i></td></tr>
<tr><th id="470">470</th><td><i>	 * quiescent state from this CPU.  Note that requests from</i></td></tr>
<tr><th id="471">471</th><td><i>	 * tasks are handled when removing the task from the</i></td></tr>
<tr><th id="472">472</th><td><i>	 * blocked-tasks list below.</i></td></tr>
<tr><th id="473">473</th><td><i>	 */</i></td></tr>
<tr><th id="474">474</th><td>	<b>if</b> (rdp-&gt;exp_deferred_qs) {</td></tr>
<tr><th id="475">475</th><td>		rcu_report_exp_rdp(rdp);</td></tr>
<tr><th id="476">476</th><td>		<b>if</b> (!t-&gt;rcu_read_unlock_special.s) {</td></tr>
<tr><th id="477">477</th><td>			local_irq_restore(flags);</td></tr>
<tr><th id="478">478</th><td>			<b>return</b>;</td></tr>
<tr><th id="479">479</th><td>		}</td></tr>
<tr><th id="480">480</th><td>	}</td></tr>
<tr><th id="481">481</th><td></td></tr>
<tr><th id="482">482</th><td>	<i>/* Clean up if blocked during RCU read-side critical section. */</i></td></tr>
<tr><th id="483">483</th><td>	<b>if</b> (special.b.blocked) {</td></tr>
<tr><th id="484">484</th><td>		t-&gt;rcu_read_unlock_special.b.blocked = false;</td></tr>
<tr><th id="485">485</th><td></td></tr>
<tr><th id="486">486</th><td>		<i>/*</i></td></tr>
<tr><th id="487">487</th><td><i>		 * Remove this task from the list it blocked on.  The task</i></td></tr>
<tr><th id="488">488</th><td><i>		 * now remains queued on the rcu_node corresponding to the</i></td></tr>
<tr><th id="489">489</th><td><i>		 * CPU it first blocked on, so there is no longer any need</i></td></tr>
<tr><th id="490">490</th><td><i>		 * to loop.  Retain a WARN_ON_ONCE() out of sheer paranoia.</i></td></tr>
<tr><th id="491">491</th><td><i>		 */</i></td></tr>
<tr><th id="492">492</th><td>		rnp = t-&gt;rcu_blocked_node;</td></tr>
<tr><th id="493">493</th><td>		raw_spin_lock_rcu_node(rnp); <i>/* irqs already disabled. */</i></td></tr>
<tr><th id="494">494</th><td>		WARN_ON_ONCE(rnp != t-&gt;rcu_blocked_node);</td></tr>
<tr><th id="495">495</th><td>		WARN_ON_ONCE(!rcu_is_leaf_node(rnp));</td></tr>
<tr><th id="496">496</th><td>		empty_norm = !rcu_preempt_blocked_readers_cgp(rnp);</td></tr>
<tr><th id="497">497</th><td>		WARN_ON_ONCE(rnp-&gt;completedqs == rnp-&gt;gp_seq &amp;&amp;</td></tr>
<tr><th id="498">498</th><td>			     (!empty_norm || rnp-&gt;qsmask));</td></tr>
<tr><th id="499">499</th><td>		empty_exp = sync_rcu_preempt_exp_done(rnp);</td></tr>
<tr><th id="500">500</th><td>		smp_mb(); <i>/* ensure expedited fastpath sees end of RCU c-s. */</i></td></tr>
<tr><th id="501">501</th><td>		np = rcu_next_node_entry(t, rnp);</td></tr>
<tr><th id="502">502</th><td>		list_del_init(&amp;t-&gt;rcu_node_entry);</td></tr>
<tr><th id="503">503</th><td>		t-&gt;rcu_blocked_node = NULL;</td></tr>
<tr><th id="504">504</th><td>		trace_rcu_unlock_preempted_task(TPS(<q>"rcu_preempt"</q>),</td></tr>
<tr><th id="505">505</th><td>						rnp-&gt;gp_seq, t-&gt;pid);</td></tr>
<tr><th id="506">506</th><td>		<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;gp_tasks)</td></tr>
<tr><th id="507">507</th><td>			rnp-&gt;gp_tasks = np;</td></tr>
<tr><th id="508">508</th><td>		<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;exp_tasks)</td></tr>
<tr><th id="509">509</th><td>			rnp-&gt;exp_tasks = np;</td></tr>
<tr><th id="510">510</th><td>		<b>if</b> (IS_ENABLED(CONFIG_RCU_BOOST)) {</td></tr>
<tr><th id="511">511</th><td>			<i>/* Snapshot -&gt;boost_mtx ownership w/rnp-&gt;lock held. */</i></td></tr>
<tr><th id="512">512</th><td>			drop_boost_mutex = rt_mutex_owner(&amp;rnp-&gt;boost_mtx) == t;</td></tr>
<tr><th id="513">513</th><td>			<b>if</b> (&amp;t-&gt;rcu_node_entry == rnp-&gt;boost_tasks)</td></tr>
<tr><th id="514">514</th><td>				rnp-&gt;boost_tasks = np;</td></tr>
<tr><th id="515">515</th><td>		}</td></tr>
<tr><th id="516">516</th><td></td></tr>
<tr><th id="517">517</th><td>		<i>/*</i></td></tr>
<tr><th id="518">518</th><td><i>		 * If this was the last task on the current list, and if</i></td></tr>
<tr><th id="519">519</th><td><i>		 * we aren't waiting on any CPUs, report the quiescent state.</i></td></tr>
<tr><th id="520">520</th><td><i>		 * Note that rcu_report_unblock_qs_rnp() releases rnp-&gt;lock,</i></td></tr>
<tr><th id="521">521</th><td><i>		 * so we must take a snapshot of the expedited state.</i></td></tr>
<tr><th id="522">522</th><td><i>		 */</i></td></tr>
<tr><th id="523">523</th><td>		empty_exp_now = sync_rcu_preempt_exp_done(rnp);</td></tr>
<tr><th id="524">524</th><td>		<b>if</b> (!empty_norm &amp;&amp; !rcu_preempt_blocked_readers_cgp(rnp)) {</td></tr>
<tr><th id="525">525</th><td>			trace_rcu_quiescent_state_report(TPS(<q>"preempt_rcu"</q>),</td></tr>
<tr><th id="526">526</th><td>							 rnp-&gt;gp_seq,</td></tr>
<tr><th id="527">527</th><td>							 <var>0</var>, rnp-&gt;qsmask,</td></tr>
<tr><th id="528">528</th><td>							 rnp-&gt;level,</td></tr>
<tr><th id="529">529</th><td>							 rnp-&gt;grplo,</td></tr>
<tr><th id="530">530</th><td>							 rnp-&gt;grphi,</td></tr>
<tr><th id="531">531</th><td>							 !!rnp-&gt;gp_tasks);</td></tr>
<tr><th id="532">532</th><td>			rcu_report_unblock_qs_rnp(rnp, flags);</td></tr>
<tr><th id="533">533</th><td>		} <b>else</b> {</td></tr>
<tr><th id="534">534</th><td>			raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="535">535</th><td>		}</td></tr>
<tr><th id="536">536</th><td></td></tr>
<tr><th id="537">537</th><td>		<i>/* Unboost if we were boosted. */</i></td></tr>
<tr><th id="538">538</th><td>		<b>if</b> (IS_ENABLED(CONFIG_RCU_BOOST) &amp;&amp; drop_boost_mutex)</td></tr>
<tr><th id="539">539</th><td>			rt_mutex_futex_unlock(&amp;rnp-&gt;boost_mtx);</td></tr>
<tr><th id="540">540</th><td></td></tr>
<tr><th id="541">541</th><td>		<i>/*</i></td></tr>
<tr><th id="542">542</th><td><i>		 * If this was the last task on the expedited lists,</i></td></tr>
<tr><th id="543">543</th><td><i>		 * then we need to report up the rcu_node hierarchy.</i></td></tr>
<tr><th id="544">544</th><td><i>		 */</i></td></tr>
<tr><th id="545">545</th><td>		<b>if</b> (!empty_exp &amp;&amp; empty_exp_now)</td></tr>
<tr><th id="546">546</th><td>			rcu_report_exp_rnp(rnp, true);</td></tr>
<tr><th id="547">547</th><td>	} <b>else</b> {</td></tr>
<tr><th id="548">548</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="549">549</th><td>	}</td></tr>
<tr><th id="550">550</th><td>}</td></tr>
<tr><th id="551">551</th><td></td></tr>
<tr><th id="552">552</th><td><i>/*</i></td></tr>
<tr><th id="553">553</th><td><i> * Is a deferred quiescent-state pending, and are we also not in</i></td></tr>
<tr><th id="554">554</th><td><i> * an RCU read-side critical section?  It is the caller's responsibility</i></td></tr>
<tr><th id="555">555</th><td><i> * to ensure it is otherwise safe to report any deferred quiescent</i></td></tr>
<tr><th id="556">556</th><td><i> * states.  The reason for this is that it is safe to report a</i></td></tr>
<tr><th id="557">557</th><td><i> * quiescent state during context switch even though preemption</i></td></tr>
<tr><th id="558">558</th><td><i> * is disabled.  This function cannot be expected to understand these</i></td></tr>
<tr><th id="559">559</th><td><i> * nuances, so the caller must handle them.</i></td></tr>
<tr><th id="560">560</th><td><i> */</i></td></tr>
<tr><th id="561">561</th><td><em>static</em> bool rcu_preempt_need_deferred_qs(<b>struct</b> task_struct *t)</td></tr>
<tr><th id="562">562</th><td>{</td></tr>
<tr><th id="563">563</th><td>	<b>return</b> (__this_cpu_read(rcu_data.exp_deferred_qs) ||</td></tr>
<tr><th id="564">564</th><td>		READ_ONCE(t-&gt;rcu_read_unlock_special.s)) &amp;&amp;</td></tr>
<tr><th id="565">565</th><td>	       t-&gt;rcu_read_lock_nesting &lt;= <var>0</var>;</td></tr>
<tr><th id="566">566</th><td>}</td></tr>
<tr><th id="567">567</th><td></td></tr>
<tr><th id="568">568</th><td><i>/*</i></td></tr>
<tr><th id="569">569</th><td><i> * Report a deferred quiescent state if needed and safe to do so.</i></td></tr>
<tr><th id="570">570</th><td><i> * As with rcu_preempt_need_deferred_qs(), "safe" involves only</i></td></tr>
<tr><th id="571">571</th><td><i> * not being in an RCU read-side critical section.  The caller must</i></td></tr>
<tr><th id="572">572</th><td><i> * evaluate safety in terms of interrupt, softirq, and preemption</i></td></tr>
<tr><th id="573">573</th><td><i> * disabling.</i></td></tr>
<tr><th id="574">574</th><td><i> */</i></td></tr>
<tr><th id="575">575</th><td><em>static</em> <em>void</em> rcu_preempt_deferred_qs(<b>struct</b> task_struct *t)</td></tr>
<tr><th id="576">576</th><td>{</td></tr>
<tr><th id="577">577</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="578">578</th><td>	bool couldrecurse = t-&gt;rcu_read_lock_nesting &gt;= <var>0</var>;</td></tr>
<tr><th id="579">579</th><td></td></tr>
<tr><th id="580">580</th><td>	<b>if</b> (!rcu_preempt_need_deferred_qs(t))</td></tr>
<tr><th id="581">581</th><td>		<b>return</b>;</td></tr>
<tr><th id="582">582</th><td>	<b>if</b> (couldrecurse)</td></tr>
<tr><th id="583">583</th><td>		t-&gt;rcu_read_lock_nesting -= RCU_NEST_BIAS;</td></tr>
<tr><th id="584">584</th><td>	local_irq_save(flags);</td></tr>
<tr><th id="585">585</th><td>	rcu_preempt_deferred_qs_irqrestore(t, flags);</td></tr>
<tr><th id="586">586</th><td>	<b>if</b> (couldrecurse)</td></tr>
<tr><th id="587">587</th><td>		t-&gt;rcu_read_lock_nesting += RCU_NEST_BIAS;</td></tr>
<tr><th id="588">588</th><td>}</td></tr>
<tr><th id="589">589</th><td></td></tr>
<tr><th id="590">590</th><td><i>/*</i></td></tr>
<tr><th id="591">591</th><td><i> * Minimal handler to give the scheduler a chance to re-evaluate.</i></td></tr>
<tr><th id="592">592</th><td><i> */</i></td></tr>
<tr><th id="593">593</th><td><em>static</em> <em>void</em> rcu_preempt_deferred_qs_handler(<b>struct</b> irq_work *iwp)</td></tr>
<tr><th id="594">594</th><td>{</td></tr>
<tr><th id="595">595</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="596">596</th><td></td></tr>
<tr><th id="597">597</th><td>	rdp = container_of(iwp, <b>struct</b> rcu_data, defer_qs_iw);</td></tr>
<tr><th id="598">598</th><td>	rdp-&gt;defer_qs_iw_pending = false;</td></tr>
<tr><th id="599">599</th><td>}</td></tr>
<tr><th id="600">600</th><td></td></tr>
<tr><th id="601">601</th><td><i>/*</i></td></tr>
<tr><th id="602">602</th><td><i> * Handle special cases during rcu_read_unlock(), such as needing to</i></td></tr>
<tr><th id="603">603</th><td><i> * notify RCU core processing or task having blocked during the RCU</i></td></tr>
<tr><th id="604">604</th><td><i> * read-side critical section.</i></td></tr>
<tr><th id="605">605</th><td><i> */</i></td></tr>
<tr><th id="606">606</th><td><em>static</em> <em>void</em> rcu_read_unlock_special(<b>struct</b> task_struct *t)</td></tr>
<tr><th id="607">607</th><td>{</td></tr>
<tr><th id="608">608</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="609">609</th><td>	bool preempt_bh_were_disabled =</td></tr>
<tr><th id="610">610</th><td>			!!(preempt_count() &amp; (PREEMPT_MASK | SOFTIRQ_MASK));</td></tr>
<tr><th id="611">611</th><td>	bool irqs_were_disabled;</td></tr>
<tr><th id="612">612</th><td></td></tr>
<tr><th id="613">613</th><td>	<i>/* NMI handlers cannot block and cannot safely manipulate state. */</i></td></tr>
<tr><th id="614">614</th><td>	<b>if</b> (in_nmi())</td></tr>
<tr><th id="615">615</th><td>		<b>return</b>;</td></tr>
<tr><th id="616">616</th><td></td></tr>
<tr><th id="617">617</th><td>	local_irq_save(flags);</td></tr>
<tr><th id="618">618</th><td>	irqs_were_disabled = irqs_disabled_flags(flags);</td></tr>
<tr><th id="619">619</th><td>	<b>if</b> (preempt_bh_were_disabled || irqs_were_disabled) {</td></tr>
<tr><th id="620">620</th><td>		bool exp;</td></tr>
<tr><th id="621">621</th><td>		<b>struct</b> rcu_data *rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="622">622</th><td>		<b>struct</b> rcu_node *rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="623">623</th><td></td></tr>
<tr><th id="624">624</th><td>		t-&gt;rcu_read_unlock_special.b.exp_hint = false;</td></tr>
<tr><th id="625">625</th><td>		exp = (t-&gt;rcu_blocked_node &amp;&amp; t-&gt;rcu_blocked_node-&gt;exp_tasks) ||</td></tr>
<tr><th id="626">626</th><td>		      (rdp-&gt;grpmask &amp; rnp-&gt;expmask) ||</td></tr>
<tr><th id="627">627</th><td>		      tick_nohz_full_cpu(rdp-&gt;cpu);</td></tr>
<tr><th id="628">628</th><td>		<i>// Need to defer quiescent state until everything is enabled.</i></td></tr>
<tr><th id="629">629</th><td>		<b>if</b> ((exp || in_irq()) &amp;&amp; irqs_were_disabled &amp;&amp; use_softirq &amp;&amp;</td></tr>
<tr><th id="630">630</th><td>		    (in_irq() || !t-&gt;rcu_read_unlock_special.b.deferred_qs)) {</td></tr>
<tr><th id="631">631</th><td>			<i>// Using softirq, safe to awaken, and we get</i></td></tr>
<tr><th id="632">632</th><td><i>			// no help from enabling irqs, unlike bh/preempt.</i></td></tr>
<tr><th id="633">633</th><td>			raise_softirq_irqoff(RCU_SOFTIRQ);</td></tr>
<tr><th id="634">634</th><td>		} <b>else</b> <b>if</b> (exp &amp;&amp; irqs_were_disabled &amp;&amp; !use_softirq &amp;&amp;</td></tr>
<tr><th id="635">635</th><td>			   !t-&gt;rcu_read_unlock_special.b.deferred_qs) {</td></tr>
<tr><th id="636">636</th><td>			<i>// Safe to awaken and we get no help from enabling</i></td></tr>
<tr><th id="637">637</th><td><i>			// irqs, unlike bh/preempt.</i></td></tr>
<tr><th id="638">638</th><td>			invoke_rcu_core();</td></tr>
<tr><th id="639">639</th><td>		} <b>else</b> {</td></tr>
<tr><th id="640">640</th><td>			<i>// Enabling BH or preempt does reschedule, so...</i></td></tr>
<tr><th id="641">641</th><td><i>			// Also if no expediting or NO_HZ_FULL, slow is OK.</i></td></tr>
<tr><th id="642">642</th><td>			set_tsk_need_resched(current);</td></tr>
<tr><th id="643">643</th><td>			set_preempt_need_resched();</td></tr>
<tr><th id="644">644</th><td>			<b>if</b> (IS_ENABLED(CONFIG_IRQ_WORK) &amp;&amp;</td></tr>
<tr><th id="645">645</th><td>			    !rdp-&gt;defer_qs_iw_pending &amp;&amp; exp) {</td></tr>
<tr><th id="646">646</th><td>				<i>// Get scheduler to re-evaluate and call hooks.</i></td></tr>
<tr><th id="647">647</th><td><i>				// If !IRQ_WORK, FQS scan will eventually IPI.</i></td></tr>
<tr><th id="648">648</th><td>				init_irq_work(&amp;rdp-&gt;defer_qs_iw,</td></tr>
<tr><th id="649">649</th><td>					      rcu_preempt_deferred_qs_handler);</td></tr>
<tr><th id="650">650</th><td>				rdp-&gt;defer_qs_iw_pending = true;</td></tr>
<tr><th id="651">651</th><td>				irq_work_queue_on(&amp;rdp-&gt;defer_qs_iw, rdp-&gt;cpu);</td></tr>
<tr><th id="652">652</th><td>			}</td></tr>
<tr><th id="653">653</th><td>		}</td></tr>
<tr><th id="654">654</th><td>		t-&gt;rcu_read_unlock_special.b.deferred_qs = true;</td></tr>
<tr><th id="655">655</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="656">656</th><td>		<b>return</b>;</td></tr>
<tr><th id="657">657</th><td>	}</td></tr>
<tr><th id="658">658</th><td>	WRITE_ONCE(t-&gt;rcu_read_unlock_special.b.exp_hint, false);</td></tr>
<tr><th id="659">659</th><td>	rcu_preempt_deferred_qs_irqrestore(t, flags);</td></tr>
<tr><th id="660">660</th><td>}</td></tr>
<tr><th id="661">661</th><td></td></tr>
<tr><th id="662">662</th><td><i>/*</i></td></tr>
<tr><th id="663">663</th><td><i> * Check that the list of blocked tasks for the newly completed grace</i></td></tr>
<tr><th id="664">664</th><td><i> * period is in fact empty.  It is a serious bug to complete a grace</i></td></tr>
<tr><th id="665">665</th><td><i> * period that still has RCU readers blocked!  This function must be</i></td></tr>
<tr><th id="666">666</th><td><i> * invoked -before- updating this rnp's -&gt;gp_seq, and the rnp's -&gt;lock</i></td></tr>
<tr><th id="667">667</th><td><i> * must be held by the caller.</i></td></tr>
<tr><th id="668">668</th><td><i> *</i></td></tr>
<tr><th id="669">669</th><td><i> * Also, if there are blocked tasks on the list, they automatically</i></td></tr>
<tr><th id="670">670</th><td><i> * block the newly created grace period, so set up -&gt;gp_tasks accordingly.</i></td></tr>
<tr><th id="671">671</th><td><i> */</i></td></tr>
<tr><th id="672">672</th><td><em>static</em> <em>void</em> rcu_preempt_check_blocked_tasks(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="673">673</th><td>{</td></tr>
<tr><th id="674">674</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="675">675</th><td></td></tr>
<tr><th id="676">676</th><td>	RCU_LOCKDEP_WARN(preemptible(), <q>"rcu_preempt_check_blocked_tasks() invoked with preemption enabled!!!\n"</q>);</td></tr>
<tr><th id="677">677</th><td>	<b>if</b> (WARN_ON_ONCE(rcu_preempt_blocked_readers_cgp(rnp)))</td></tr>
<tr><th id="678">678</th><td>		dump_blkd_tasks(rnp, <var>10</var>);</td></tr>
<tr><th id="679">679</th><td>	<b>if</b> (rcu_preempt_has_tasks(rnp) &amp;&amp;</td></tr>
<tr><th id="680">680</th><td>	    (rnp-&gt;qsmaskinit || rnp-&gt;wait_blkd_tasks)) {</td></tr>
<tr><th id="681">681</th><td>		rnp-&gt;gp_tasks = rnp-&gt;blkd_tasks.next;</td></tr>
<tr><th id="682">682</th><td>		t = container_of(rnp-&gt;gp_tasks, <b>struct</b> task_struct,</td></tr>
<tr><th id="683">683</th><td>				 rcu_node_entry);</td></tr>
<tr><th id="684">684</th><td>		trace_rcu_unlock_preempted_task(TPS(<q>"rcu_preempt-GPS"</q>),</td></tr>
<tr><th id="685">685</th><td>						rnp-&gt;gp_seq, t-&gt;pid);</td></tr>
<tr><th id="686">686</th><td>	}</td></tr>
<tr><th id="687">687</th><td>	WARN_ON_ONCE(rnp-&gt;qsmask);</td></tr>
<tr><th id="688">688</th><td>}</td></tr>
<tr><th id="689">689</th><td></td></tr>
<tr><th id="690">690</th><td><i>/*</i></td></tr>
<tr><th id="691">691</th><td><i> * Check for a quiescent state from the current CPU, including voluntary</i></td></tr>
<tr><th id="692">692</th><td><i> * context switches for Tasks RCU.  When a task blocks, the task is</i></td></tr>
<tr><th id="693">693</th><td><i> * recorded in the corresponding CPU's rcu_node structure, which is checked</i></td></tr>
<tr><th id="694">694</th><td><i> * elsewhere, hence this function need only check for quiescent states</i></td></tr>
<tr><th id="695">695</th><td><i> * related to the current CPU, not to those related to tasks.</i></td></tr>
<tr><th id="696">696</th><td><i> */</i></td></tr>
<tr><th id="697">697</th><td><em>static</em> <em>void</em> rcu_flavor_sched_clock_irq(<em>int</em> user)</td></tr>
<tr><th id="698">698</th><td>{</td></tr>
<tr><th id="699">699</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="700">700</th><td></td></tr>
<tr><th id="701">701</th><td>	<b>if</b> (user || rcu_is_cpu_rrupt_from_idle()) {</td></tr>
<tr><th id="702">702</th><td>		rcu_note_voluntary_context_switch(current);</td></tr>
<tr><th id="703">703</th><td>	}</td></tr>
<tr><th id="704">704</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting &gt; <var>0</var> ||</td></tr>
<tr><th id="705">705</th><td>	    (preempt_count() &amp; (PREEMPT_MASK | SOFTIRQ_MASK))) {</td></tr>
<tr><th id="706">706</th><td>		<i>/* No QS, force context switch if deferred. */</i></td></tr>
<tr><th id="707">707</th><td>		<b>if</b> (rcu_preempt_need_deferred_qs(t)) {</td></tr>
<tr><th id="708">708</th><td>			set_tsk_need_resched(t);</td></tr>
<tr><th id="709">709</th><td>			set_preempt_need_resched();</td></tr>
<tr><th id="710">710</th><td>		}</td></tr>
<tr><th id="711">711</th><td>	} <b>else</b> <b>if</b> (rcu_preempt_need_deferred_qs(t)) {</td></tr>
<tr><th id="712">712</th><td>		rcu_preempt_deferred_qs(t); <i>/* Report deferred QS. */</i></td></tr>
<tr><th id="713">713</th><td>		<b>return</b>;</td></tr>
<tr><th id="714">714</th><td>	} <b>else</b> <b>if</b> (!t-&gt;rcu_read_lock_nesting) {</td></tr>
<tr><th id="715">715</th><td>		rcu_qs(); <i>/* Report immediate QS. */</i></td></tr>
<tr><th id="716">716</th><td>		<b>return</b>;</td></tr>
<tr><th id="717">717</th><td>	}</td></tr>
<tr><th id="718">718</th><td></td></tr>
<tr><th id="719">719</th><td>	<i>/* If GP is oldish, ask for help from rcu_read_unlock_special(). */</i></td></tr>
<tr><th id="720">720</th><td>	<b>if</b> (t-&gt;rcu_read_lock_nesting &gt; <var>0</var> &amp;&amp;</td></tr>
<tr><th id="721">721</th><td>	    __this_cpu_read(rcu_data.core_needs_qs) &amp;&amp;</td></tr>
<tr><th id="722">722</th><td>	    __this_cpu_read(rcu_data.cpu_no_qs.b.norm) &amp;&amp;</td></tr>
<tr><th id="723">723</th><td>	    !t-&gt;rcu_read_unlock_special.b.need_qs &amp;&amp;</td></tr>
<tr><th id="724">724</th><td>	    time_after(jiffies, rcu_state.gp_start + HZ))</td></tr>
<tr><th id="725">725</th><td>		t-&gt;rcu_read_unlock_special.b.need_qs = true;</td></tr>
<tr><th id="726">726</th><td>}</td></tr>
<tr><th id="727">727</th><td></td></tr>
<tr><th id="728">728</th><td><i>/*</i></td></tr>
<tr><th id="729">729</th><td><i> * Check for a task exiting while in a preemptible-RCU read-side</i></td></tr>
<tr><th id="730">730</th><td><i> * critical section, clean up if so.  No need to issue warnings, as</i></td></tr>
<tr><th id="731">731</th><td><i> * debug_check_no_locks_held() already does this if lockdep is enabled.</i></td></tr>
<tr><th id="732">732</th><td><i> * Besides, if this function does anything other than just immediately</i></td></tr>
<tr><th id="733">733</th><td><i> * return, there was a bug of some sort.  Spewing warnings from this</i></td></tr>
<tr><th id="734">734</th><td><i> * function is like as not to simply obscure important prior warnings.</i></td></tr>
<tr><th id="735">735</th><td><i> */</i></td></tr>
<tr><th id="736">736</th><td><em>void</em> exit_rcu(<em>void</em>)</td></tr>
<tr><th id="737">737</th><td>{</td></tr>
<tr><th id="738">738</th><td>	<b>struct</b> task_struct *t = current;</td></tr>
<tr><th id="739">739</th><td></td></tr>
<tr><th id="740">740</th><td>	<b>if</b> (unlikely(!list_empty(&amp;current-&gt;rcu_node_entry))) {</td></tr>
<tr><th id="741">741</th><td>		t-&gt;rcu_read_lock_nesting = <var>1</var>;</td></tr>
<tr><th id="742">742</th><td>		barrier();</td></tr>
<tr><th id="743">743</th><td>		WRITE_ONCE(t-&gt;rcu_read_unlock_special.b.blocked, true);</td></tr>
<tr><th id="744">744</th><td>	} <b>else</b> <b>if</b> (unlikely(t-&gt;rcu_read_lock_nesting)) {</td></tr>
<tr><th id="745">745</th><td>		t-&gt;rcu_read_lock_nesting = <var>1</var>;</td></tr>
<tr><th id="746">746</th><td>	} <b>else</b> {</td></tr>
<tr><th id="747">747</th><td>		<b>return</b>;</td></tr>
<tr><th id="748">748</th><td>	}</td></tr>
<tr><th id="749">749</th><td>	__rcu_read_unlock();</td></tr>
<tr><th id="750">750</th><td>	rcu_preempt_deferred_qs(current);</td></tr>
<tr><th id="751">751</th><td>}</td></tr>
<tr><th id="752">752</th><td></td></tr>
<tr><th id="753">753</th><td><i>/*</i></td></tr>
<tr><th id="754">754</th><td><i> * Dump the blocked-tasks state, but limit the list dump to the</i></td></tr>
<tr><th id="755">755</th><td><i> * specified number of elements.</i></td></tr>
<tr><th id="756">756</th><td><i> */</i></td></tr>
<tr><th id="757">757</th><td><em>static</em> <em>void</em></td></tr>
<tr><th id="758">758</th><td>dump_blkd_tasks(<b>struct</b> rcu_node *rnp, <em>int</em> ncheck)</td></tr>
<tr><th id="759">759</th><td>{</td></tr>
<tr><th id="760">760</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="761">761</th><td>	<em>int</em> i;</td></tr>
<tr><th id="762">762</th><td>	<b>struct</b> list_head *lhp;</td></tr>
<tr><th id="763">763</th><td>	bool onl;</td></tr>
<tr><th id="764">764</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="765">765</th><td>	<b>struct</b> rcu_node *rnp1;</td></tr>
<tr><th id="766">766</th><td></td></tr>
<tr><th id="767">767</th><td>	raw_lockdep_assert_held_rcu_node(rnp);</td></tr>
<tr><th id="768">768</th><td>	pr_info(<q>"%s: grp: %d-%d level: %d -&gt;gp_seq %ld -&gt;completedqs %ld\n"</q>,</td></tr>
<tr><th id="769">769</th><td>		<b>__func__</b>, rnp-&gt;grplo, rnp-&gt;grphi, rnp-&gt;level,</td></tr>
<tr><th id="770">770</th><td>		(<em>long</em>)rnp-&gt;gp_seq, (<em>long</em>)rnp-&gt;completedqs);</td></tr>
<tr><th id="771">771</th><td>	<b>for</b> (rnp1 = rnp; rnp1; rnp1 = rnp1-&gt;parent)</td></tr>
<tr><th id="772">772</th><td>		pr_info(<q>"%s: %d:%d -&gt;qsmask %#lx -&gt;qsmaskinit %#lx -&gt;qsmaskinitnext %#lx\n"</q>,</td></tr>
<tr><th id="773">773</th><td>			<b>__func__</b>, rnp1-&gt;grplo, rnp1-&gt;grphi, rnp1-&gt;qsmask, rnp1-&gt;qsmaskinit, rnp1-&gt;qsmaskinitnext);</td></tr>
<tr><th id="774">774</th><td>	pr_info(<q>"%s: -&gt;gp_tasks %p -&gt;boost_tasks %p -&gt;exp_tasks %p\n"</q>,</td></tr>
<tr><th id="775">775</th><td>		<b>__func__</b>, rnp-&gt;gp_tasks, rnp-&gt;boost_tasks, rnp-&gt;exp_tasks);</td></tr>
<tr><th id="776">776</th><td>	pr_info(<q>"%s: -&gt;blkd_tasks"</q>, <b>__func__</b>);</td></tr>
<tr><th id="777">777</th><td>	i = <var>0</var>;</td></tr>
<tr><th id="778">778</th><td>	list_for_each(lhp, &amp;rnp-&gt;blkd_tasks) {</td></tr>
<tr><th id="779">779</th><td>		pr_cont(<q>" %p"</q>, lhp);</td></tr>
<tr><th id="780">780</th><td>		<b>if</b> (++i &gt;= ncheck)</td></tr>
<tr><th id="781">781</th><td>			<b>break</b>;</td></tr>
<tr><th id="782">782</th><td>	}</td></tr>
<tr><th id="783">783</th><td>	pr_cont(<q>"\n"</q>);</td></tr>
<tr><th id="784">784</th><td>	<b>for</b> (cpu = rnp-&gt;grplo; cpu &lt;= rnp-&gt;grphi; cpu++) {</td></tr>
<tr><th id="785">785</th><td>		rdp = per_cpu_ptr(&amp;rcu_data, cpu);</td></tr>
<tr><th id="786">786</th><td>		onl = !!(rdp-&gt;grpmask &amp; rcu_rnp_online_cpus(rnp));</td></tr>
<tr><th id="787">787</th><td>		pr_info(<q>"\t%d: %c online: %ld(%d) offline: %ld(%d)\n"</q>,</td></tr>
<tr><th id="788">788</th><td>			cpu, <q>".o"</q>[onl],</td></tr>
<tr><th id="789">789</th><td>			(<em>long</em>)rdp-&gt;rcu_onl_gp_seq, rdp-&gt;rcu_onl_gp_flags,</td></tr>
<tr><th id="790">790</th><td>			(<em>long</em>)rdp-&gt;rcu_ofl_gp_seq, rdp-&gt;rcu_ofl_gp_flags);</td></tr>
<tr><th id="791">791</th><td>	}</td></tr>
<tr><th id="792">792</th><td>}</td></tr>
<tr><th id="793">793</th><td></td></tr>
<tr><th id="794">794</th><td><u>#<span data-ppcond="82">else</span> /* #ifdef CONFIG_PREEMPT_RCU */</u></td></tr>
<tr><th id="795">795</th><td></td></tr>
<tr><th id="796">796</th><td><i>/*</i></td></tr>
<tr><th id="797">797</th><td><i> * Tell them what RCU they are running.</i></td></tr>
<tr><th id="798">798</th><td><i> */</i></td></tr>
<tr><th id="799">799</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__((__section__(&quot;.init.text&quot;))) __attribute__((__cold__))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_bootup_announce" title='rcu_bootup_announce' data-ref="rcu_bootup_announce" data-ref-filename="rcu_bootup_announce">rcu_bootup_announce</dfn>(<em>void</em>)</td></tr>
<tr><th id="800">800</th><td>{</td></tr>
<tr><th id="801">801</th><td>	<a class="macro" href="../../include/linux/printk.h.html#310" title="printk(&quot;\001&quot; &quot;6&quot; &quot;rcu: &quot; &quot;Hierarchical RCU implementation.\n&quot;)" data-ref="_M/pr_info">pr_info</a>(<q>"Hierarchical RCU implementation.\n"</q>);</td></tr>
<tr><th id="802">802</th><td>	<a class="ref fn" href="#rcu_bootup_announce_oddness" title='rcu_bootup_announce_oddness' data-ref="rcu_bootup_announce_oddness" data-ref-filename="rcu_bootup_announce_oddness">rcu_bootup_announce_oddness</a>();</td></tr>
<tr><th id="803">803</th><td>}</td></tr>
<tr><th id="804">804</th><td></td></tr>
<tr><th id="805">805</th><td><i>/*</i></td></tr>
<tr><th id="806">806</th><td><i> * Note a quiescent state for PREEMPT=n.  Because we do not need to know</i></td></tr>
<tr><th id="807">807</th><td><i> * how many quiescent states passed, just if there was at least one since</i></td></tr>
<tr><th id="808">808</th><td><i> * the start of the grace period, this just sets a flag.  The caller must</i></td></tr>
<tr><th id="809">809</th><td><i> * have disabled preemption.</i></td></tr>
<tr><th id="810">810</th><td><i> */</i></td></tr>
<tr><th id="811">811</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_qs" title='rcu_qs' data-ref="rcu_qs" data-ref-filename="rcu_qs">rcu_qs</dfn>(<em>void</em>)</td></tr>
<tr><th id="812">812</th><td>{</td></tr>
<tr><th id="813">813</th><td>	<a class="macro" href="../../include/linux/rcupdate.h.html#283" title="do { } while (0)" data-ref="_M/RCU_LOCKDEP_WARN">RCU_LOCKDEP_WARN</a>(preemptible(), <q>"rcu_qs() invoked with preemption enabled!!!"</q>);</td></tr>
<tr><th id="814">814</th><td>	<b>if</b> (!<a class="macro" href="../../include/linux/percpu-defs.h.html#444" title="({ __this_cpu_preempt_check(&quot;read&quot;); ({ typeof(rcu_data.cpu_no_qs.s) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.cpu_no_qs.s)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.cpu_no_qs.s)) { case 1: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.s) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.s)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.s) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.s)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.s) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.s)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.s) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.s)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.s)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); })" data-ref="_M/__this_cpu_read">__this_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::cpu_no_qs" title='rcu_data::cpu_no_qs' data-ref="rcu_data::cpu_no_qs" data-ref-filename="rcu_data..cpu_no_qs">cpu_no_qs</a>.<a class="ref field" href="tree.h.html#rcu_noqs::s" title='rcu_noqs::s' data-ref="rcu_noqs::s" data-ref-filename="rcu_noqs..s">s</a>))</td></tr>
<tr><th id="815">815</th><td>		<b>return</b>;</td></tr>
<tr><th id="816">816</th><td>	<a class="ref fn" href="../../include/trace/events/rcu.h.html#69" title='trace_rcu_grace_period' data-ref="trace_rcu_grace_period" data-ref-filename="trace_rcu_grace_period">trace_rcu_grace_period</a>(<a class="macro" href="rcu.h.html#256" title="({ static const char *___tp_str __attribute__((section(&quot;__tracepoint_str&quot;))) = &quot;rcu_sched&quot;; ___tp_str; })" data-ref="_M/TPS">TPS</a>(<q>"rcu_sched"</q>),</td></tr>
<tr><th id="817">817</th><td>			       <a class="macro" href="../../include/linux/percpu-defs.h.html#444" title="({ __this_cpu_preempt_check(&quot;read&quot;); ({ typeof(rcu_data.gp_seq) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.gp_seq)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.gp_seq)) { case 1: pscr_ret__ = ({ typeof(rcu_data.gp_seq) pfo_ret__; switch (sizeof(rcu_data.gp_seq)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.gp_seq) pfo_ret__; switch (sizeof(rcu_data.gp_seq)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.gp_seq) pfo_ret__; switch (sizeof(rcu_data.gp_seq)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.gp_seq) pfo_ret__; switch (sizeof(rcu_data.gp_seq)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.gp_seq)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); })" data-ref="_M/__this_cpu_read">__this_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::gp_seq" title='rcu_data::gp_seq' data-ref="rcu_data::gp_seq" data-ref-filename="rcu_data..gp_seq">gp_seq</a>), <a class="macro" href="rcu.h.html#256" title="({ static const char *___tp_str __attribute__((section(&quot;__tracepoint_str&quot;))) = &quot;cpuqs&quot;; ___tp_str; })" data-ref="_M/TPS">TPS</a>(<q>"cpuqs"</q>));</td></tr>
<tr><th id="818">818</th><td>	<a class="macro" href="../../include/linux/percpu-defs.h.html#450" title="({ __this_cpu_preempt_check(&quot;write&quot;); do { do { const void *__vpp_verify = (typeof((&amp;(rcu_data.cpu_no_qs.b.norm)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.cpu_no_qs.b.norm)) { case 1: do { typedef typeof((rcu_data.cpu_no_qs.b.norm)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.norm))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((rcu_data.cpu_no_qs.b.norm)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.norm))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((rcu_data.cpu_no_qs.b.norm)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.norm))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((rcu_data.cpu_no_qs.b.norm)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.norm))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.norm)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; default: __bad_size_call_parameter();break; } } while (0); })" data-ref="_M/__this_cpu_write">__this_cpu_write</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::cpu_no_qs" title='rcu_data::cpu_no_qs' data-ref="rcu_data::cpu_no_qs" data-ref-filename="rcu_data..cpu_no_qs">cpu_no_qs</a>.<a class="ref field" href="tree.h.html#rcu_noqs::b" title='rcu_noqs::b' data-ref="rcu_noqs::b" data-ref-filename="rcu_noqs..b">b</a>.<a class="ref field" href="tree.h.html#rcu_noqs::(anonymous)::norm" title='rcu_noqs::(anonymous struct)::norm' data-ref="rcu_noqs::(anonymous)::norm" data-ref-filename="rcu_noqs..(anonymous)..norm">norm</a>, <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>);</td></tr>
<tr><th id="819">819</th><td>	<b>if</b> (!<a class="macro" href="../../include/linux/percpu-defs.h.html#444" title="({ __this_cpu_preempt_check(&quot;read&quot;); ({ typeof(rcu_data.cpu_no_qs.b.exp) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.cpu_no_qs.b.exp)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.b.exp) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.b.exp) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.b.exp) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.cpu_no_qs.b.exp) pfo_ret__; switch (sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.cpu_no_qs.b.exp)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; }); })" data-ref="_M/__this_cpu_read">__this_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::cpu_no_qs" title='rcu_data::cpu_no_qs' data-ref="rcu_data::cpu_no_qs" data-ref-filename="rcu_data..cpu_no_qs">cpu_no_qs</a>.<a class="ref field" href="tree.h.html#rcu_noqs::b" title='rcu_noqs::b' data-ref="rcu_noqs::b" data-ref-filename="rcu_noqs..b">b</a>.<a class="ref field" href="tree.h.html#rcu_noqs::(anonymous)::exp" title='rcu_noqs::(anonymous struct)::exp' data-ref="rcu_noqs::(anonymous)::exp" data-ref-filename="rcu_noqs..(anonymous)..exp">exp</a>))</td></tr>
<tr><th id="820">820</th><td>		<b>return</b>;</td></tr>
<tr><th id="821">821</th><td>	<a class="macro" href="../../include/linux/percpu-defs.h.html#450" title="({ __this_cpu_preempt_check(&quot;write&quot;); do { do { const void *__vpp_verify = (typeof((&amp;(rcu_data.cpu_no_qs.b.exp)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.cpu_no_qs.b.exp)) { case 1: do { typedef typeof((rcu_data.cpu_no_qs.b.exp)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.exp))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((rcu_data.cpu_no_qs.b.exp)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.exp))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((rcu_data.cpu_no_qs.b.exp)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.exp))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((rcu_data.cpu_no_qs.b.exp)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.cpu_no_qs.b.exp))) { case 1: asm (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.cpu_no_qs.b.exp)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; default: __bad_size_call_parameter();break; } } while (0); })" data-ref="_M/__this_cpu_write">__this_cpu_write</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::cpu_no_qs" title='rcu_data::cpu_no_qs' data-ref="rcu_data::cpu_no_qs" data-ref-filename="rcu_data..cpu_no_qs">cpu_no_qs</a>.<a class="ref field" href="tree.h.html#rcu_noqs::b" title='rcu_noqs::b' data-ref="rcu_noqs::b" data-ref-filename="rcu_noqs..b">b</a>.<a class="ref field" href="tree.h.html#rcu_noqs::(anonymous)::exp" title='rcu_noqs::(anonymous struct)::exp' data-ref="rcu_noqs::(anonymous)::exp" data-ref-filename="rcu_noqs..(anonymous)..exp">exp</a>, <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>);</td></tr>
<tr><th id="822">822</th><td>	<a class="ref fn" href="tree_exp.h.html#rcu_report_exp_rdp" title='rcu_report_exp_rdp' data-ref="rcu_report_exp_rdp" data-ref-filename="rcu_report_exp_rdp">rcu_report_exp_rdp</a>(<a class="macro" href="../../include/linux/percpu-defs.h.html#253" title="({ do { const void *__vpp_verify = (typeof((&amp;rcu_data) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data)); (typeof(*(&amp;rcu_data)) *)tcp_ptr__; }); })" data-ref="_M/this_cpu_ptr">this_cpu_ptr</a>(&amp;<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='a' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>));</td></tr>
<tr><th id="823">823</th><td>}</td></tr>
<tr><th id="824">824</th><td></td></tr>
<tr><th id="825">825</th><td><i>/*</i></td></tr>
<tr><th id="826">826</th><td><i> * Register an urgently needed quiescent state.  If there is an</i></td></tr>
<tr><th id="827">827</th><td><i> * emergency, invoke rcu_momentary_dyntick_idle() to do a heavy-weight</i></td></tr>
<tr><th id="828">828</th><td><i> * dyntick-idle quiescent state visible to other CPUs, which will in</i></td></tr>
<tr><th id="829">829</th><td><i> * some cases serve for expedited as well as normal grace periods.</i></td></tr>
<tr><th id="830">830</th><td><i> * Either way, register a lightweight quiescent state.</i></td></tr>
<tr><th id="831">831</th><td><i> *</i></td></tr>
<tr><th id="832">832</th><td><i> * The barrier() calls are redundant in the common case when this is</i></td></tr>
<tr><th id="833">833</th><td><i> * called externally, but just in case this is called from within this</i></td></tr>
<tr><th id="834">834</th><td><i> * file.</i></td></tr>
<tr><th id="835">835</th><td><i> *</i></td></tr>
<tr><th id="836">836</th><td><i> */</i></td></tr>
<tr><th id="837">837</th><td><em>void</em> <dfn class="decl def fn" id="rcu_all_qs" title='rcu_all_qs' data-ref="rcu_all_qs" data-ref-filename="rcu_all_qs">rcu_all_qs</dfn>(<em>void</em>)</td></tr>
<tr><th id="838">838</th><td>{</td></tr>
<tr><th id="839">839</th><td>	<em>unsigned</em> <em>long</em> <dfn class="local col9 decl" id="519flags" title='flags' data-type='unsigned long' data-ref="519flags" data-ref-filename="519flags">flags</dfn>;</td></tr>
<tr><th id="840">840</th><td></td></tr>
<tr><th id="841">841</th><td>	<b>if</b> (!<a class="macro" href="../../include/linux/percpu-defs.h.html#421" title="({ typeof(rcu_data.rcu_urgent_qs) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_urgent_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_urgent_qs)) { case 1: pscr_ret__ = ({ typeof(rcu_data.rcu_urgent_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_urgent_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.rcu_urgent_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_urgent_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.rcu_urgent_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_urgent_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.rcu_urgent_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_urgent_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_urgent_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })" data-ref="_M/raw_cpu_read">raw_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_urgent_qs" title='rcu_data::rcu_urgent_qs' data-ref="rcu_data::rcu_urgent_qs" data-ref-filename="rcu_data..rcu_urgent_qs">rcu_urgent_qs</a>))</td></tr>
<tr><th id="842">842</th><td>		<b>return</b>;</td></tr>
<tr><th id="843">843</th><td>	<a class="macro" href="../../include/linux/preempt.h.html#242" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/preempt_disable">preempt_disable</a>();</td></tr>
<tr><th id="844">844</th><td>	<i>/* Load rcu_urgent_qs before other flags. */</i></td></tr>
<tr><th id="845">845</th><td>	<b>if</b> (!<a class="macro" href="../../include/asm-generic/barrier.h.html#157" title="({ typeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) ___p1 = ({ union { typeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) __val; char __c[1]; } __u; if (1) __read_once_size(&amp;(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })), __u.__c, sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); }))); else __read_once_size_nocheck(&amp;(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })), __u.__c, sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); }))); do { } while (0); __u.__val; }); do { extern void __compiletime_assert_845(void) ; if (!((sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(char) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(short) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(int) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(long)))) __compiletime_assert_845(); } while (0); __asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;); ___p1; })" data-ref="_M/smp_load_acquire">smp_load_acquire</a>(<a class="macro" href="../../include/linux/percpu-defs.h.html#253" title="({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })" data-ref="_M/this_cpu_ptr">this_cpu_ptr</a>(&amp;<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_urgent_qs" title='rcu_data::rcu_urgent_qs' data-ref="rcu_data::rcu_urgent_qs" data-ref-filename="rcu_data..rcu_urgent_qs">rcu_urgent_qs</a>))) {</td></tr>
<tr><th id="846">846</th><td>		<a class="macro" href="../../include/linux/preempt.h.html#245" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/preempt_enable">preempt_enable</a>();</td></tr>
<tr><th id="847">847</th><td>		<b>return</b>;</td></tr>
<tr><th id="848">848</th><td>	}</td></tr>
<tr><th id="849">849</th><td>	<a class="macro" href="../../include/linux/percpu-defs.h.html#509" title="do { do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_urgent_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_urgent_qs)) { case 1: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; default: __bad_size_call_parameter();break; } } while (0)" data-ref="_M/this_cpu_write">this_cpu_write</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_urgent_qs" title='rcu_data::rcu_urgent_qs' data-ref="rcu_data::rcu_urgent_qs" data-ref-filename="rcu_data..rcu_urgent_qs">rcu_urgent_qs</a>, <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>);</td></tr>
<tr><th id="850">850</th><td>	<a class="macro" href="../../include/linux/compiler-clang.h.html#44" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/barrier">barrier</a>(); <i>/* Avoid RCU read-side critical sections leaking down. */</i></td></tr>
<tr><th id="851">851</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/compiler.h.html#78" title="__builtin_expect(!!(({ typeof(rcu_data.rcu_need_heavy_qs) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_need_heavy_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })), 0)" data-ref="_M/unlikely">unlikely</a>(<a class="macro" href="../../include/linux/percpu-defs.h.html#421" title="({ typeof(rcu_data.rcu_need_heavy_qs) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_need_heavy_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })" data-ref="_M/raw_cpu_read">raw_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_need_heavy_qs" title='rcu_data::rcu_need_heavy_qs' data-ref="rcu_data::rcu_need_heavy_qs" data-ref-filename="rcu_data..rcu_need_heavy_qs">rcu_need_heavy_qs</a>))) {</td></tr>
<tr><th id="852">852</th><td>		<a class="macro" href="../../include/linux/irqflags.h.html#142" title="do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&amp;__dummy == &amp;__dummy2); 1; }); flags = arch_local_irq_save(); } while (0); } while (0)" data-ref="_M/local_irq_save">local_irq_save</a>(<a class="local col9 ref" href="#519flags" title='flags' data-ref="519flags" data-ref-filename="519flags">flags</a>);</td></tr>
<tr><th id="853">853</th><td>		<a class="tu ref fn" href="tree.c.html#rcu_momentary_dyntick_idle" title='rcu_momentary_dyntick_idle' data-use='c' data-ref="rcu_momentary_dyntick_idle" data-ref-filename="rcu_momentary_dyntick_idle">rcu_momentary_dyntick_idle</a>();</td></tr>
<tr><th id="854">854</th><td>		<a class="macro" href="../../include/linux/irqflags.h.html#146" title="do { do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&amp;__dummy == &amp;__dummy2); 1; }); arch_local_irq_restore(flags); } while (0); } while (0)" data-ref="_M/local_irq_restore">local_irq_restore</a>(<a class="local col9 ref" href="#519flags" title='flags' data-ref="519flags" data-ref-filename="519flags">flags</a>);</td></tr>
<tr><th id="855">855</th><td>	}</td></tr>
<tr><th id="856">856</th><td>	<a class="ref fn" href="#rcu_qs" title='rcu_qs' data-ref="rcu_qs" data-ref-filename="rcu_qs">rcu_qs</a>();</td></tr>
<tr><th id="857">857</th><td>	<a class="macro" href="../../include/linux/compiler-clang.h.html#44" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/barrier">barrier</a>(); <i>/* Avoid RCU read-side critical sections leaking up. */</i></td></tr>
<tr><th id="858">858</th><td>	<a class="macro" href="../../include/linux/preempt.h.html#245" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/preempt_enable">preempt_enable</a>();</td></tr>
<tr><th id="859">859</th><td>}</td></tr>
<tr><th id="860">860</th><td><a class="macro" href="../../include/linux/export.h.html#126" title="extern typeof(rcu_all_qs) rcu_all_qs; static const char __kstrtab_rcu_all_qs[] __attribute__((section(&quot;__ksymtab_strings&quot;), used, aligned(1))) = &quot;rcu_all_qs&quot;; static void * __attribute__((__section__(&quot;.discard.addressable&quot;))) __attribute__((__used__)) __addressable_rcu_all_qs860 = (void *)&amp;rcu_all_qs; asm(&quot;	.section \&quot;___ksymtab&quot; &quot;_gpl&quot; &quot;+&quot; &quot;rcu_all_qs&quot; &quot;\&quot;, \&quot;a\&quot;	\n&quot; &quot;	.balign	8					\n&quot; &quot;__ksymtab_&quot; &quot;rcu_all_qs&quot; &quot;:				\n&quot; &quot;	.long	&quot; &quot;rcu_all_qs&quot; &quot;- .				\n&quot; &quot;	.long	__kstrtab_&quot; &quot;rcu_all_qs&quot; &quot;- .			\n&quot; &quot;	.previous					\n&quot;)" data-ref="_M/EXPORT_SYMBOL_GPL">EXPORT_SYMBOL_GPL</a>(<a class="decl fn" href="#rcu_all_qs" title='rcu_all_qs' data-ref="rcu_all_qs" data-ref-filename="rcu_all_qs"><a class="ref fn" href="#rcu_all_qs" title='rcu_all_qs' data-ref="rcu_all_qs" data-ref-filename="rcu_all_qs"><a class="ref fn" href="#rcu_all_qs" title='rcu_all_qs' data-ref="rcu_all_qs" data-ref-filename="rcu_all_qs">rcu_all_qs</a></a></a>);</td></tr>
<tr><th id="861">861</th><td></td></tr>
<tr><th id="862">862</th><td><i>/*</i></td></tr>
<tr><th id="863">863</th><td><i> * Note a PREEMPT=n context switch.  The caller must have disabled interrupts.</i></td></tr>
<tr><th id="864">864</th><td><i> */</i></td></tr>
<tr><th id="865">865</th><td><em>void</em> <dfn class="decl def fn" id="rcu_note_context_switch" title='rcu_note_context_switch' data-ref="rcu_note_context_switch" data-ref-filename="rcu_note_context_switch">rcu_note_context_switch</dfn>(<a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="local col0 decl" id="520preempt" title='preempt' data-type='bool' data-ref="520preempt" data-ref-filename="520preempt">preempt</dfn>)</td></tr>
<tr><th id="866">866</th><td>{</td></tr>
<tr><th id="867">867</th><td>	<a class="macro" href="../../include/linux/compiler-clang.h.html#44" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/barrier">barrier</a>(); <i>/* Avoid RCU read-side critical sections leaking down. */</i></td></tr>
<tr><th id="868">868</th><td>	<a class="ref fn" href="../../include/trace/events/rcu.h.html#27" title='trace_rcu_utilization' data-ref="trace_rcu_utilization" data-ref-filename="trace_rcu_utilization">trace_rcu_utilization</a>(<a class="macro" href="rcu.h.html#256" title="({ static const char *___tp_str __attribute__((section(&quot;__tracepoint_str&quot;))) = &quot;Start context switch&quot;; ___tp_str; })" data-ref="_M/TPS">TPS</a>(<q>"Start context switch"</q>));</td></tr>
<tr><th id="869">869</th><td>	<a class="ref fn" href="#rcu_qs" title='rcu_qs' data-ref="rcu_qs" data-ref-filename="rcu_qs">rcu_qs</a>();</td></tr>
<tr><th id="870">870</th><td>	<i>/* Load rcu_urgent_qs before other flags. */</i></td></tr>
<tr><th id="871">871</th><td>	<b>if</b> (!<a class="macro" href="../../include/asm-generic/barrier.h.html#157" title="({ typeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) ___p1 = ({ union { typeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) __val; char __c[1]; } __u; if (1) __read_once_size(&amp;(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })), __u.__c, sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); }))); else __read_once_size_nocheck(&amp;(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })), __u.__c, sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); }))); do { } while (0); __u.__val; }); do { extern void __compiletime_assert_871(void) ; if (!((sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(char) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(short) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(int) || sizeof(*({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })) == sizeof(long)))) __compiletime_assert_871(); } while (0); __asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;); ___p1; })" data-ref="_M/smp_load_acquire">smp_load_acquire</a>(<a class="macro" href="../../include/linux/percpu-defs.h.html#253" title="({ do { const void *__vpp_verify = (typeof((&amp;rcu_data.rcu_urgent_qs) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data.rcu_urgent_qs)); (typeof(*(&amp;rcu_data.rcu_urgent_qs)) *)tcp_ptr__; }); })" data-ref="_M/this_cpu_ptr">this_cpu_ptr</a>(&amp;<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_urgent_qs" title='rcu_data::rcu_urgent_qs' data-ref="rcu_data::rcu_urgent_qs" data-ref-filename="rcu_data..rcu_urgent_qs">rcu_urgent_qs</a>)))</td></tr>
<tr><th id="872">872</th><td>		<b>goto</b> <a class="lbl" href="#521out" data-ref="521out" data-ref-filename="521out">out</a>;</td></tr>
<tr><th id="873">873</th><td>	<a class="macro" href="../../include/linux/percpu-defs.h.html#509" title="do { do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_urgent_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_urgent_qs)) { case 1: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 2: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 4: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; case 8: do { typedef typeof((rcu_data.rcu_urgent_qs)) pto_T__; if (0) { pto_T__ pto_tmp__; pto_tmp__ = (false); (void)pto_tmp__; } switch (sizeof((rcu_data.rcu_urgent_qs))) { case 1: asm volatile (&quot;mov&quot; &quot;b %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;qi&quot; ((pto_T__)(false))); break; case 2: asm volatile (&quot;mov&quot; &quot;w %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 4: asm volatile (&quot;mov&quot; &quot;l %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;ri&quot; ((pto_T__)(false))); break; case 8: asm volatile (&quot;mov&quot; &quot;q %1,&quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;0&quot; : &quot;+m&quot; ((rcu_data.rcu_urgent_qs)) : &quot;re&quot; ((pto_T__)(false))); break; default: __bad_percpu_size(); } } while (0);break; default: __bad_size_call_parameter();break; } } while (0)" data-ref="_M/this_cpu_write">this_cpu_write</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_urgent_qs" title='rcu_data::rcu_urgent_qs' data-ref="rcu_data::rcu_urgent_qs" data-ref-filename="rcu_data..rcu_urgent_qs">rcu_urgent_qs</a>, <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>);</td></tr>
<tr><th id="874">874</th><td>	<b>if</b> (<a class="macro" href="../../include/linux/compiler.h.html#78" title="__builtin_expect(!!(({ typeof(rcu_data.rcu_need_heavy_qs) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_need_heavy_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })), 0)" data-ref="_M/unlikely">unlikely</a>(<a class="macro" href="../../include/linux/percpu-defs.h.html#421" title="({ typeof(rcu_data.rcu_need_heavy_qs) pscr_ret__; do { const void *__vpp_verify = (typeof((&amp;(rcu_data.rcu_need_heavy_qs)) + 0))((void *)0); (void)__vpp_verify; } while (0); switch(sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 2: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 4: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; case 8: pscr_ret__ = ({ typeof(rcu_data.rcu_need_heavy_qs) pfo_ret__; switch (sizeof(rcu_data.rcu_need_heavy_qs)) { case 1: asm (&quot;mov&quot; &quot;b &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=q&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 2: asm (&quot;mov&quot; &quot;w &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 4: asm (&quot;mov&quot; &quot;l &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; case 8: asm (&quot;mov&quot; &quot;q &quot;&quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot;&quot;,%0&quot; : &quot;=r&quot; (pfo_ret__) : &quot;m&quot; (rcu_data.rcu_need_heavy_qs)); break; default: __bad_percpu_size(); } pfo_ret__; }); break; default: __bad_size_call_parameter(); break; } pscr_ret__; })" data-ref="_M/raw_cpu_read">raw_cpu_read</a>(<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='m' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>.<a class="ref field" href="tree.h.html#rcu_data::rcu_need_heavy_qs" title='rcu_data::rcu_need_heavy_qs' data-ref="rcu_data::rcu_need_heavy_qs" data-ref-filename="rcu_data..rcu_need_heavy_qs">rcu_need_heavy_qs</a>)))</td></tr>
<tr><th id="875">875</th><td>		<a class="tu ref fn" href="tree.c.html#rcu_momentary_dyntick_idle" title='rcu_momentary_dyntick_idle' data-use='c' data-ref="rcu_momentary_dyntick_idle" data-ref-filename="rcu_momentary_dyntick_idle">rcu_momentary_dyntick_idle</a>();</td></tr>
<tr><th id="876">876</th><td>	<b>if</b> (!<a class="local col0 ref" href="#520preempt" title='preempt' data-ref="520preempt" data-ref-filename="520preempt">preempt</a>)</td></tr>
<tr><th id="877">877</th><td>		<a class="macro" href="../../include/linux/rcupdate.h.html#144" title="do { } while (0)" data-ref="_M/rcu_tasks_qs">rcu_tasks_qs</a>(current);</td></tr>
<tr><th id="878">878</th><td><dfn class="lbl" id="521out" data-ref="521out" data-ref-filename="521out">out</dfn>:</td></tr>
<tr><th id="879">879</th><td>	<a class="ref fn" href="../../include/trace/events/rcu.h.html#27" title='trace_rcu_utilization' data-ref="trace_rcu_utilization" data-ref-filename="trace_rcu_utilization">trace_rcu_utilization</a>(<a class="macro" href="rcu.h.html#256" title="({ static const char *___tp_str __attribute__((section(&quot;__tracepoint_str&quot;))) = &quot;End context switch&quot;; ___tp_str; })" data-ref="_M/TPS">TPS</a>(<q>"End context switch"</q>));</td></tr>
<tr><th id="880">880</th><td>	<a class="macro" href="../../include/linux/compiler-clang.h.html#44" title="__asm__ __volatile__(&quot;&quot; : : : &quot;memory&quot;)" data-ref="_M/barrier">barrier</a>(); <i>/* Avoid RCU read-side critical sections leaking up. */</i></td></tr>
<tr><th id="881">881</th><td>}</td></tr>
<tr><th id="882">882</th><td><a class="macro" href="../../include/linux/export.h.html#126" title="extern typeof(rcu_note_context_switch) rcu_note_context_switch; static const char __kstrtab_rcu_note_context_switch[] __attribute__((section(&quot;__ksymtab_strings&quot;), used, aligned(1))) = &quot;rcu_note_context_switch&quot;; static void * __attribute__((__section__(&quot;.discard.addressable&quot;))) __attribute__((__used__)) __addressable_rcu_note_context_switch882 = (void *)&amp;rcu_note_context_switch; asm(&quot;	.section \&quot;___ksymtab&quot; &quot;_gpl&quot; &quot;+&quot; &quot;rcu_note_context_switch&quot; &quot;\&quot;, \&quot;a\&quot;	\n&quot; &quot;	.balign	8					\n&quot; &quot;__ksymtab_&quot; &quot;rcu_note_context_switch&quot; &quot;:				\n&quot; &quot;	.long	&quot; &quot;rcu_note_context_switch&quot; &quot;- .				\n&quot; &quot;	.long	__kstrtab_&quot; &quot;rcu_note_context_switch&quot; &quot;- .			\n&quot; &quot;	.previous					\n&quot;)" data-ref="_M/EXPORT_SYMBOL_GPL">EXPORT_SYMBOL_GPL</a>(<a class="decl fn" href="#rcu_note_context_switch" title='rcu_note_context_switch' data-ref="rcu_note_context_switch" data-ref-filename="rcu_note_context_switch"><a class="ref fn" href="#rcu_note_context_switch" title='rcu_note_context_switch' data-ref="rcu_note_context_switch" data-ref-filename="rcu_note_context_switch"><a class="ref fn" href="#rcu_note_context_switch" title='rcu_note_context_switch' data-ref="rcu_note_context_switch" data-ref-filename="rcu_note_context_switch">rcu_note_context_switch</a></a></a>);</td></tr>
<tr><th id="883">883</th><td></td></tr>
<tr><th id="884">884</th><td><i>/*</i></td></tr>
<tr><th id="885">885</th><td><i> * Because preemptible RCU does not exist, there are never any preempted</i></td></tr>
<tr><th id="886">886</th><td><i> * RCU readers.</i></td></tr>
<tr><th id="887">887</th><td><i> */</i></td></tr>
<tr><th id="888">888</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_preempt_blocked_readers_cgp" title='rcu_preempt_blocked_readers_cgp' data-ref="rcu_preempt_blocked_readers_cgp" data-ref-filename="rcu_preempt_blocked_readers_cgp">rcu_preempt_blocked_readers_cgp</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col2 decl" id="522rnp" title='rnp' data-type='struct rcu_node *' data-ref="522rnp" data-ref-filename="522rnp">rnp</dfn>)</td></tr>
<tr><th id="889">889</th><td>{</td></tr>
<tr><th id="890">890</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="891">891</th><td>}</td></tr>
<tr><th id="892">892</th><td></td></tr>
<tr><th id="893">893</th><td><i>/*</i></td></tr>
<tr><th id="894">894</th><td><i> * Because there is no preemptible RCU, there can be no readers blocked.</i></td></tr>
<tr><th id="895">895</th><td><i> */</i></td></tr>
<tr><th id="896">896</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="rcu_preempt_has_tasks" title='rcu_preempt_has_tasks' data-ref="rcu_preempt_has_tasks" data-ref-filename="rcu_preempt_has_tasks">rcu_preempt_has_tasks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col3 decl" id="523rnp" title='rnp' data-type='struct rcu_node *' data-ref="523rnp" data-ref-filename="523rnp">rnp</dfn>)</td></tr>
<tr><th id="897">897</th><td>{</td></tr>
<tr><th id="898">898</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="899">899</th><td>}</td></tr>
<tr><th id="900">900</th><td></td></tr>
<tr><th id="901">901</th><td><i>/*</i></td></tr>
<tr><th id="902">902</th><td><i> * Because there is no preemptible RCU, there can be no deferred quiescent</i></td></tr>
<tr><th id="903">903</th><td><i> * states.</i></td></tr>
<tr><th id="904">904</th><td><i> */</i></td></tr>
<tr><th id="905">905</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="rcu_preempt_need_deferred_qs" title='rcu_preempt_need_deferred_qs' data-ref="rcu_preempt_need_deferred_qs" data-ref-filename="rcu_preempt_need_deferred_qs">rcu_preempt_need_deferred_qs</dfn>(<b>struct</b> <a class="type" href="../../include/linux/sched.h.html#task_struct" title='task_struct' data-ref="task_struct" data-ref-filename="task_struct">task_struct</a> *<dfn class="local col4 decl" id="524t" title='t' data-type='struct task_struct *' data-ref="524t" data-ref-filename="524t">t</dfn>)</td></tr>
<tr><th id="906">906</th><td>{</td></tr>
<tr><th id="907">907</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="908">908</th><td>}</td></tr>
<tr><th id="909">909</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_deferred_qs" title='rcu_preempt_deferred_qs' data-ref="rcu_preempt_deferred_qs" data-ref-filename="rcu_preempt_deferred_qs">rcu_preempt_deferred_qs</dfn>(<b>struct</b> <a class="type" href="../../include/linux/sched.h.html#task_struct" title='task_struct' data-ref="task_struct" data-ref-filename="task_struct">task_struct</a> *<dfn class="local col5 decl" id="525t" title='t' data-type='struct task_struct *' data-ref="525t" data-ref-filename="525t">t</dfn>) { }</td></tr>
<tr><th id="910">910</th><td></td></tr>
<tr><th id="911">911</th><td><i>/*</i></td></tr>
<tr><th id="912">912</th><td><i> * Because there is no preemptible RCU, there can be no readers blocked,</i></td></tr>
<tr><th id="913">913</th><td><i> * so there is no need to check for blocked tasks.  So check only for</i></td></tr>
<tr><th id="914">914</th><td><i> * bogus qsmask values.</i></td></tr>
<tr><th id="915">915</th><td><i> */</i></td></tr>
<tr><th id="916">916</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_check_blocked_tasks" title='rcu_preempt_check_blocked_tasks' data-ref="rcu_preempt_check_blocked_tasks" data-ref-filename="rcu_preempt_check_blocked_tasks">rcu_preempt_check_blocked_tasks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col6 decl" id="526rnp" title='rnp' data-type='struct rcu_node *' data-ref="526rnp" data-ref-filename="526rnp">rnp</dfn>)</td></tr>
<tr><th id="917">917</th><td>{</td></tr>
<tr><th id="918">918</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#68" title="({ int __ret_warn_on = !!(rnp-&gt;qsmask); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/woboq/linux-5.3.1/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (918), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm volatile(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (278)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(<a class="local col6 ref" href="#526rnp" title='rnp' data-ref="526rnp" data-ref-filename="526rnp">rnp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_node::qsmask" title='rcu_node::qsmask' data-ref="rcu_node::qsmask" data-ref-filename="rcu_node..qsmask">qsmask</a>);</td></tr>
<tr><th id="919">919</th><td>}</td></tr>
<tr><th id="920">920</th><td></td></tr>
<tr><th id="921">921</th><td><i>/*</i></td></tr>
<tr><th id="922">922</th><td><i> * Check to see if this CPU is in a non-context-switch quiescent state,</i></td></tr>
<tr><th id="923">923</th><td><i> * namely user mode and idle loop.</i></td></tr>
<tr><th id="924">924</th><td><i> */</i></td></tr>
<tr><th id="925">925</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_flavor_sched_clock_irq" title='rcu_flavor_sched_clock_irq' data-ref="rcu_flavor_sched_clock_irq" data-ref-filename="rcu_flavor_sched_clock_irq">rcu_flavor_sched_clock_irq</dfn>(<em>int</em> <dfn class="local col7 decl" id="527user" title='user' data-type='int' data-ref="527user" data-ref-filename="527user">user</dfn>)</td></tr>
<tr><th id="926">926</th><td>{</td></tr>
<tr><th id="927">927</th><td>	<b>if</b> (<a class="local col7 ref" href="#527user" title='user' data-ref="527user" data-ref-filename="527user">user</a> || <a class="tu ref fn" href="tree.c.html#rcu_is_cpu_rrupt_from_idle" title='rcu_is_cpu_rrupt_from_idle' data-use='c' data-ref="rcu_is_cpu_rrupt_from_idle" data-ref-filename="rcu_is_cpu_rrupt_from_idle">rcu_is_cpu_rrupt_from_idle</a>()) {</td></tr>
<tr><th id="928">928</th><td></td></tr>
<tr><th id="929">929</th><td>		<i>/*</i></td></tr>
<tr><th id="930">930</th><td><i>		 * Get here if this CPU took its interrupt from user</i></td></tr>
<tr><th id="931">931</th><td><i>		 * mode or from the idle loop, and if this is not a</i></td></tr>
<tr><th id="932">932</th><td><i>		 * nested interrupt.  In this case, the CPU is in</i></td></tr>
<tr><th id="933">933</th><td><i>		 * a quiescent state, so note it.</i></td></tr>
<tr><th id="934">934</th><td><i>		 *</i></td></tr>
<tr><th id="935">935</th><td><i>		 * No memory barrier is required here because rcu_qs()</i></td></tr>
<tr><th id="936">936</th><td><i>		 * references only CPU-local variables that other CPUs</i></td></tr>
<tr><th id="937">937</th><td><i>		 * neither access nor modify, at least not while the</i></td></tr>
<tr><th id="938">938</th><td><i>		 * corresponding CPU is online.</i></td></tr>
<tr><th id="939">939</th><td><i>		 */</i></td></tr>
<tr><th id="940">940</th><td></td></tr>
<tr><th id="941">941</th><td>		<a class="ref fn" href="#rcu_qs" title='rcu_qs' data-ref="rcu_qs" data-ref-filename="rcu_qs">rcu_qs</a>();</td></tr>
<tr><th id="942">942</th><td>	}</td></tr>
<tr><th id="943">943</th><td>}</td></tr>
<tr><th id="944">944</th><td></td></tr>
<tr><th id="945">945</th><td><i>/*</i></td></tr>
<tr><th id="946">946</th><td><i> * Because preemptible RCU does not exist, tasks cannot possibly exit</i></td></tr>
<tr><th id="947">947</th><td><i> * while in preemptible RCU read-side critical sections.</i></td></tr>
<tr><th id="948">948</th><td><i> */</i></td></tr>
<tr><th id="949">949</th><td><em>void</em> <dfn class="decl def fn" id="exit_rcu" title='exit_rcu' data-ref="exit_rcu" data-ref-filename="exit_rcu">exit_rcu</dfn>(<em>void</em>)</td></tr>
<tr><th id="950">950</th><td>{</td></tr>
<tr><th id="951">951</th><td>}</td></tr>
<tr><th id="952">952</th><td></td></tr>
<tr><th id="953">953</th><td><i>/*</i></td></tr>
<tr><th id="954">954</th><td><i> * Dump the guaranteed-empty blocked-tasks state.  Trust but verify.</i></td></tr>
<tr><th id="955">955</th><td><i> */</i></td></tr>
<tr><th id="956">956</th><td><em>static</em> <em>void</em></td></tr>
<tr><th id="957">957</th><td><dfn class="decl def fn" id="dump_blkd_tasks" title='dump_blkd_tasks' data-ref="dump_blkd_tasks" data-ref-filename="dump_blkd_tasks">dump_blkd_tasks</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col8 decl" id="528rnp" title='rnp' data-type='struct rcu_node *' data-ref="528rnp" data-ref-filename="528rnp">rnp</dfn>, <em>int</em> <dfn class="local col9 decl" id="529ncheck" title='ncheck' data-type='int' data-ref="529ncheck" data-ref-filename="529ncheck">ncheck</dfn>)</td></tr>
<tr><th id="958">958</th><td>{</td></tr>
<tr><th id="959">959</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#68" title="({ int __ret_warn_on = !!(!list_empty(&amp;rnp-&gt;blkd_tasks)); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/woboq/linux-5.3.1/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (959), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm volatile(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (280)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(!<a class="ref fn" href="../../include/linux/list.h.html#list_empty" title='list_empty' data-ref="list_empty" data-ref-filename="list_empty">list_empty</a>(&amp;<a class="local col8 ref" href="#528rnp" title='rnp' data-ref="528rnp" data-ref-filename="528rnp">rnp</a>-&gt;<a class="ref field" href="tree.h.html#rcu_node::blkd_tasks" title='rcu_node::blkd_tasks' data-ref="rcu_node::blkd_tasks" data-ref-filename="rcu_node..blkd_tasks">blkd_tasks</a>));</td></tr>
<tr><th id="960">960</th><td>}</td></tr>
<tr><th id="961">961</th><td></td></tr>
<tr><th id="962">962</th><td><u>#<span data-ppcond="82">endif</span> /* #else #ifdef CONFIG_PREEMPT_RCU */</u></td></tr>
<tr><th id="963">963</th><td></td></tr>
<tr><th id="964">964</th><td><i>/*</i></td></tr>
<tr><th id="965">965</th><td><i> * If boosting, set rcuc kthreads to realtime priority.</i></td></tr>
<tr><th id="966">966</th><td><i> */</i></td></tr>
<tr><th id="967">967</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_cpu_kthread_setup" title='rcu_cpu_kthread_setup' data-ref="rcu_cpu_kthread_setup" data-ref-filename="rcu_cpu_kthread_setup">rcu_cpu_kthread_setup</dfn>(<em>unsigned</em> <em>int</em> <dfn class="local col0 decl" id="530cpu" title='cpu' data-type='unsigned int' data-ref="530cpu" data-ref-filename="530cpu">cpu</dfn>)</td></tr>
<tr><th id="968">968</th><td>{</td></tr>
<tr><th id="969">969</th><td><u>#<span data-ppcond="969">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="970">970</th><td>	<b>struct</b> sched_param sp;</td></tr>
<tr><th id="971">971</th><td></td></tr>
<tr><th id="972">972</th><td>	sp.sched_priority = kthread_prio;</td></tr>
<tr><th id="973">973</th><td>	sched_setscheduler_nocheck(current, SCHED_FIFO, &amp;sp);</td></tr>
<tr><th id="974">974</th><td><u>#<span data-ppcond="969">endif</span> /* #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="975">975</th><td>}</td></tr>
<tr><th id="976">976</th><td></td></tr>
<tr><th id="977">977</th><td><u>#<span data-ppcond="977">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_BOOST">CONFIG_RCU_BOOST</span></u></td></tr>
<tr><th id="978">978</th><td></td></tr>
<tr><th id="979">979</th><td><i>/*</i></td></tr>
<tr><th id="980">980</th><td><i> * Carry out RCU priority boosting on the task indicated by -&gt;exp_tasks</i></td></tr>
<tr><th id="981">981</th><td><i> * or -&gt;boost_tasks, advancing the pointer to the next task in the</i></td></tr>
<tr><th id="982">982</th><td><i> * -&gt;blkd_tasks list.</i></td></tr>
<tr><th id="983">983</th><td><i> *</i></td></tr>
<tr><th id="984">984</th><td><i> * Note that irqs must be enabled: boosting the task can block.</i></td></tr>
<tr><th id="985">985</th><td><i> * Returns 1 if there are more tasks needing to be boosted.</i></td></tr>
<tr><th id="986">986</th><td><i> */</i></td></tr>
<tr><th id="987">987</th><td><em>static</em> <em>int</em> rcu_boost(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="988">988</th><td>{</td></tr>
<tr><th id="989">989</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="990">990</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="991">991</th><td>	<b>struct</b> list_head *tb;</td></tr>
<tr><th id="992">992</th><td></td></tr>
<tr><th id="993">993</th><td>	<b>if</b> (READ_ONCE(rnp-&gt;exp_tasks) == NULL &amp;&amp;</td></tr>
<tr><th id="994">994</th><td>	    READ_ONCE(rnp-&gt;boost_tasks) == NULL)</td></tr>
<tr><th id="995">995</th><td>		<b>return</b> <var>0</var>;  <i>/* Nothing left to boost. */</i></td></tr>
<tr><th id="996">996</th><td></td></tr>
<tr><th id="997">997</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="998">998</th><td></td></tr>
<tr><th id="999">999</th><td>	<i>/*</i></td></tr>
<tr><th id="1000">1000</th><td><i>	 * Recheck under the lock: all tasks in need of boosting</i></td></tr>
<tr><th id="1001">1001</th><td><i>	 * might exit their RCU read-side critical sections on their own.</i></td></tr>
<tr><th id="1002">1002</th><td><i>	 */</i></td></tr>
<tr><th id="1003">1003</th><td>	<b>if</b> (rnp-&gt;exp_tasks == NULL &amp;&amp; rnp-&gt;boost_tasks == NULL) {</td></tr>
<tr><th id="1004">1004</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1005">1005</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1006">1006</th><td>	}</td></tr>
<tr><th id="1007">1007</th><td></td></tr>
<tr><th id="1008">1008</th><td>	<i>/*</i></td></tr>
<tr><th id="1009">1009</th><td><i>	 * Preferentially boost tasks blocking expedited grace periods.</i></td></tr>
<tr><th id="1010">1010</th><td><i>	 * This cannot starve the normal grace periods because a second</i></td></tr>
<tr><th id="1011">1011</th><td><i>	 * expedited grace period must boost all blocked tasks, including</i></td></tr>
<tr><th id="1012">1012</th><td><i>	 * those blocking the pre-existing normal grace period.</i></td></tr>
<tr><th id="1013">1013</th><td><i>	 */</i></td></tr>
<tr><th id="1014">1014</th><td>	<b>if</b> (rnp-&gt;exp_tasks != NULL)</td></tr>
<tr><th id="1015">1015</th><td>		tb = rnp-&gt;exp_tasks;</td></tr>
<tr><th id="1016">1016</th><td>	<b>else</b></td></tr>
<tr><th id="1017">1017</th><td>		tb = rnp-&gt;boost_tasks;</td></tr>
<tr><th id="1018">1018</th><td></td></tr>
<tr><th id="1019">1019</th><td>	<i>/*</i></td></tr>
<tr><th id="1020">1020</th><td><i>	 * We boost task t by manufacturing an rt_mutex that appears to</i></td></tr>
<tr><th id="1021">1021</th><td><i>	 * be held by task t.  We leave a pointer to that rt_mutex where</i></td></tr>
<tr><th id="1022">1022</th><td><i>	 * task t can find it, and task t will release the mutex when it</i></td></tr>
<tr><th id="1023">1023</th><td><i>	 * exits its outermost RCU read-side critical section.  Then</i></td></tr>
<tr><th id="1024">1024</th><td><i>	 * simply acquiring this artificial rt_mutex will boost task</i></td></tr>
<tr><th id="1025">1025</th><td><i>	 * t's priority.  (Thanks to tglx for suggesting this approach!)</i></td></tr>
<tr><th id="1026">1026</th><td><i>	 *</i></td></tr>
<tr><th id="1027">1027</th><td><i>	 * Note that task t must acquire rnp-&gt;lock to remove itself from</i></td></tr>
<tr><th id="1028">1028</th><td><i>	 * the -&gt;blkd_tasks list, which it will do from exit() if from</i></td></tr>
<tr><th id="1029">1029</th><td><i>	 * nowhere else.  We therefore are guaranteed that task t will</i></td></tr>
<tr><th id="1030">1030</th><td><i>	 * stay around at least until we drop rnp-&gt;lock.  Note that</i></td></tr>
<tr><th id="1031">1031</th><td><i>	 * rnp-&gt;lock also resolves races between our priority boosting</i></td></tr>
<tr><th id="1032">1032</th><td><i>	 * and task t's exiting its outermost RCU read-side critical</i></td></tr>
<tr><th id="1033">1033</th><td><i>	 * section.</i></td></tr>
<tr><th id="1034">1034</th><td><i>	 */</i></td></tr>
<tr><th id="1035">1035</th><td>	t = container_of(tb, <b>struct</b> task_struct, rcu_node_entry);</td></tr>
<tr><th id="1036">1036</th><td>	rt_mutex_init_proxy_locked(&amp;rnp-&gt;boost_mtx, t);</td></tr>
<tr><th id="1037">1037</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1038">1038</th><td>	<i>/* Lock only for side effect: boosts task t's priority. */</i></td></tr>
<tr><th id="1039">1039</th><td>	rt_mutex_lock(&amp;rnp-&gt;boost_mtx);</td></tr>
<tr><th id="1040">1040</th><td>	rt_mutex_unlock(&amp;rnp-&gt;boost_mtx);  <i>/* Then keep lockdep happy. */</i></td></tr>
<tr><th id="1041">1041</th><td></td></tr>
<tr><th id="1042">1042</th><td>	<b>return</b> READ_ONCE(rnp-&gt;exp_tasks) != NULL ||</td></tr>
<tr><th id="1043">1043</th><td>	       READ_ONCE(rnp-&gt;boost_tasks) != NULL;</td></tr>
<tr><th id="1044">1044</th><td>}</td></tr>
<tr><th id="1045">1045</th><td></td></tr>
<tr><th id="1046">1046</th><td><i>/*</i></td></tr>
<tr><th id="1047">1047</th><td><i> * Priority-boosting kthread, one per leaf rcu_node.</i></td></tr>
<tr><th id="1048">1048</th><td><i> */</i></td></tr>
<tr><th id="1049">1049</th><td><em>static</em> <em>int</em> rcu_boost_kthread(<em>void</em> *arg)</td></tr>
<tr><th id="1050">1050</th><td>{</td></tr>
<tr><th id="1051">1051</th><td>	<b>struct</b> rcu_node *rnp = (<b>struct</b> rcu_node *)arg;</td></tr>
<tr><th id="1052">1052</th><td>	<em>int</em> spincnt = <var>0</var>;</td></tr>
<tr><th id="1053">1053</th><td>	<em>int</em> more2boost;</td></tr>
<tr><th id="1054">1054</th><td></td></tr>
<tr><th id="1055">1055</th><td>	trace_rcu_utilization(TPS(<q>"Start boost kthread@init"</q>));</td></tr>
<tr><th id="1056">1056</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="1057">1057</th><td>		rnp-&gt;boost_kthread_status = RCU_KTHREAD_WAITING;</td></tr>
<tr><th id="1058">1058</th><td>		trace_rcu_utilization(TPS(<q>"End boost kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1059">1059</th><td>		rcu_wait(rnp-&gt;boost_tasks || rnp-&gt;exp_tasks);</td></tr>
<tr><th id="1060">1060</th><td>		trace_rcu_utilization(TPS(<q>"Start boost kthread@rcu_wait"</q>));</td></tr>
<tr><th id="1061">1061</th><td>		rnp-&gt;boost_kthread_status = RCU_KTHREAD_RUNNING;</td></tr>
<tr><th id="1062">1062</th><td>		more2boost = rcu_boost(rnp);</td></tr>
<tr><th id="1063">1063</th><td>		<b>if</b> (more2boost)</td></tr>
<tr><th id="1064">1064</th><td>			spincnt++;</td></tr>
<tr><th id="1065">1065</th><td>		<b>else</b></td></tr>
<tr><th id="1066">1066</th><td>			spincnt = <var>0</var>;</td></tr>
<tr><th id="1067">1067</th><td>		<b>if</b> (spincnt &gt; <var>10</var>) {</td></tr>
<tr><th id="1068">1068</th><td>			rnp-&gt;boost_kthread_status = RCU_KTHREAD_YIELDING;</td></tr>
<tr><th id="1069">1069</th><td>			trace_rcu_utilization(TPS(<q>"End boost kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1070">1070</th><td>			schedule_timeout_interruptible(<var>2</var>);</td></tr>
<tr><th id="1071">1071</th><td>			trace_rcu_utilization(TPS(<q>"Start boost kthread@rcu_yield"</q>));</td></tr>
<tr><th id="1072">1072</th><td>			spincnt = <var>0</var>;</td></tr>
<tr><th id="1073">1073</th><td>		}</td></tr>
<tr><th id="1074">1074</th><td>	}</td></tr>
<tr><th id="1075">1075</th><td>	<i>/* NOTREACHED */</i></td></tr>
<tr><th id="1076">1076</th><td>	trace_rcu_utilization(TPS(<q>"End boost kthread@notreached"</q>));</td></tr>
<tr><th id="1077">1077</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1078">1078</th><td>}</td></tr>
<tr><th id="1079">1079</th><td></td></tr>
<tr><th id="1080">1080</th><td><i>/*</i></td></tr>
<tr><th id="1081">1081</th><td><i> * Check to see if it is time to start boosting RCU readers that are</i></td></tr>
<tr><th id="1082">1082</th><td><i> * blocking the current grace period, and, if so, tell the per-rcu_node</i></td></tr>
<tr><th id="1083">1083</th><td><i> * kthread to start boosting them.  If there is an expedited grace</i></td></tr>
<tr><th id="1084">1084</th><td><i> * period in progress, it is always time to boost.</i></td></tr>
<tr><th id="1085">1085</th><td><i> *</i></td></tr>
<tr><th id="1086">1086</th><td><i> * The caller must hold rnp-&gt;lock, which this function releases.</i></td></tr>
<tr><th id="1087">1087</th><td><i> * The -&gt;boost_kthread_task is immortal, so we don't need to worry</i></td></tr>
<tr><th id="1088">1088</th><td><i> * about it going away.</i></td></tr>
<tr><th id="1089">1089</th><td><i> */</i></td></tr>
<tr><th id="1090">1090</th><td><em>static</em> <em>void</em> rcu_initiate_boost(<b>struct</b> rcu_node *rnp, <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1091">1091</th><td>	__releases(rnp-&gt;lock)</td></tr>
<tr><th id="1092">1092</th><td>{</td></tr>
<tr><th id="1093">1093</th><td>	raw_lockdep_assert_held_rcu_node(rnp);</td></tr>
<tr><th id="1094">1094</th><td>	<b>if</b> (!rcu_preempt_blocked_readers_cgp(rnp) &amp;&amp; rnp-&gt;exp_tasks == NULL) {</td></tr>
<tr><th id="1095">1095</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1096">1096</th><td>		<b>return</b>;</td></tr>
<tr><th id="1097">1097</th><td>	}</td></tr>
<tr><th id="1098">1098</th><td>	<b>if</b> (rnp-&gt;exp_tasks != NULL ||</td></tr>
<tr><th id="1099">1099</th><td>	    (rnp-&gt;gp_tasks != NULL &amp;&amp;</td></tr>
<tr><th id="1100">1100</th><td>	     rnp-&gt;boost_tasks == NULL &amp;&amp;</td></tr>
<tr><th id="1101">1101</th><td>	     rnp-&gt;qsmask == <var>0</var> &amp;&amp;</td></tr>
<tr><th id="1102">1102</th><td>	     ULONG_CMP_GE(jiffies, rnp-&gt;boost_time))) {</td></tr>
<tr><th id="1103">1103</th><td>		<b>if</b> (rnp-&gt;exp_tasks == NULL)</td></tr>
<tr><th id="1104">1104</th><td>			rnp-&gt;boost_tasks = rnp-&gt;gp_tasks;</td></tr>
<tr><th id="1105">1105</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1106">1106</th><td>		rcu_wake_cond(rnp-&gt;boost_kthread_task,</td></tr>
<tr><th id="1107">1107</th><td>			      rnp-&gt;boost_kthread_status);</td></tr>
<tr><th id="1108">1108</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1109">1109</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1110">1110</th><td>	}</td></tr>
<tr><th id="1111">1111</th><td>}</td></tr>
<tr><th id="1112">1112</th><td></td></tr>
<tr><th id="1113">1113</th><td><i>/*</i></td></tr>
<tr><th id="1114">1114</th><td><i> * Is the current CPU running the RCU-callbacks kthread?</i></td></tr>
<tr><th id="1115">1115</th><td><i> * Caller must have preemption disabled.</i></td></tr>
<tr><th id="1116">1116</th><td><i> */</i></td></tr>
<tr><th id="1117">1117</th><td><em>static</em> bool rcu_is_callbacks_kthread(<em>void</em>)</td></tr>
<tr><th id="1118">1118</th><td>{</td></tr>
<tr><th id="1119">1119</th><td>	<b>return</b> __this_cpu_read(rcu_data.rcu_cpu_kthread_task) == current;</td></tr>
<tr><th id="1120">1120</th><td>}</td></tr>
<tr><th id="1121">1121</th><td></td></tr>
<tr><th id="1122">1122</th><td><u>#define RCU_BOOST_DELAY_JIFFIES DIV_ROUND_UP(CONFIG_RCU_BOOST_DELAY * HZ, 1000)</u></td></tr>
<tr><th id="1123">1123</th><td></td></tr>
<tr><th id="1124">1124</th><td><i>/*</i></td></tr>
<tr><th id="1125">1125</th><td><i> * Do priority-boost accounting for the start of a new grace period.</i></td></tr>
<tr><th id="1126">1126</th><td><i> */</i></td></tr>
<tr><th id="1127">1127</th><td><em>static</em> <em>void</em> rcu_preempt_boost_start_gp(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1128">1128</th><td>{</td></tr>
<tr><th id="1129">1129</th><td>	rnp-&gt;boost_time = jiffies + RCU_BOOST_DELAY_JIFFIES;</td></tr>
<tr><th id="1130">1130</th><td>}</td></tr>
<tr><th id="1131">1131</th><td></td></tr>
<tr><th id="1132">1132</th><td><i>/*</i></td></tr>
<tr><th id="1133">1133</th><td><i> * Create an RCU-boost kthread for the specified node if one does not</i></td></tr>
<tr><th id="1134">1134</th><td><i> * already exist.  We only create this kthread for preemptible RCU.</i></td></tr>
<tr><th id="1135">1135</th><td><i> * Returns zero if all is well, a negated errno otherwise.</i></td></tr>
<tr><th id="1136">1136</th><td><i> */</i></td></tr>
<tr><th id="1137">1137</th><td><em>static</em> <em>int</em> rcu_spawn_one_boost_kthread(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1138">1138</th><td>{</td></tr>
<tr><th id="1139">1139</th><td>	<em>int</em> rnp_index = rnp - rcu_get_root();</td></tr>
<tr><th id="1140">1140</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1141">1141</th><td>	<b>struct</b> sched_param sp;</td></tr>
<tr><th id="1142">1142</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="1143">1143</th><td></td></tr>
<tr><th id="1144">1144</th><td>	<b>if</b> (!IS_ENABLED(CONFIG_PREEMPT_RCU))</td></tr>
<tr><th id="1145">1145</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1146">1146</th><td></td></tr>
<tr><th id="1147">1147</th><td>	<b>if</b> (!rcu_scheduler_fully_active || rcu_rnp_online_cpus(rnp) == <var>0</var>)</td></tr>
<tr><th id="1148">1148</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1149">1149</th><td></td></tr>
<tr><th id="1150">1150</th><td>	rcu_state.boost = <var>1</var>;</td></tr>
<tr><th id="1151">1151</th><td>	<b>if</b> (rnp-&gt;boost_kthread_task != NULL)</td></tr>
<tr><th id="1152">1152</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1153">1153</th><td>	t = kthread_create(rcu_boost_kthread, (<em>void</em> *)rnp,</td></tr>
<tr><th id="1154">1154</th><td>			   <q>"rcub/%d"</q>, rnp_index);</td></tr>
<tr><th id="1155">1155</th><td>	<b>if</b> (IS_ERR(t))</td></tr>
<tr><th id="1156">1156</th><td>		<b>return</b> PTR_ERR(t);</td></tr>
<tr><th id="1157">1157</th><td>	raw_spin_lock_irqsave_rcu_node(rnp, flags);</td></tr>
<tr><th id="1158">1158</th><td>	rnp-&gt;boost_kthread_task = t;</td></tr>
<tr><th id="1159">1159</th><td>	raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1160">1160</th><td>	sp.sched_priority = kthread_prio;</td></tr>
<tr><th id="1161">1161</th><td>	sched_setscheduler_nocheck(t, SCHED_FIFO, &amp;sp);</td></tr>
<tr><th id="1162">1162</th><td>	wake_up_process(t); <i>/* get to TASK_INTERRUPTIBLE quickly. */</i></td></tr>
<tr><th id="1163">1163</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1164">1164</th><td>}</td></tr>
<tr><th id="1165">1165</th><td></td></tr>
<tr><th id="1166">1166</th><td><i>/*</i></td></tr>
<tr><th id="1167">1167</th><td><i> * Set the per-rcu_node kthread's affinity to cover all CPUs that are</i></td></tr>
<tr><th id="1168">1168</th><td><i> * served by the rcu_node in question.  The CPU hotplug lock is still</i></td></tr>
<tr><th id="1169">1169</th><td><i> * held, so the value of rnp-&gt;qsmaskinit will be stable.</i></td></tr>
<tr><th id="1170">1170</th><td><i> *</i></td></tr>
<tr><th id="1171">1171</th><td><i> * We don't include outgoingcpu in the affinity set, use -1 if there is</i></td></tr>
<tr><th id="1172">1172</th><td><i> * no outgoing CPU.  If there are no CPUs left in the affinity set,</i></td></tr>
<tr><th id="1173">1173</th><td><i> * this function allows the kthread to execute on any CPU.</i></td></tr>
<tr><th id="1174">1174</th><td><i> */</i></td></tr>
<tr><th id="1175">1175</th><td><em>static</em> <em>void</em> rcu_boost_kthread_setaffinity(<b>struct</b> rcu_node *rnp, <em>int</em> outgoingcpu)</td></tr>
<tr><th id="1176">1176</th><td>{</td></tr>
<tr><th id="1177">1177</th><td>	<b>struct</b> task_struct *t = rnp-&gt;boost_kthread_task;</td></tr>
<tr><th id="1178">1178</th><td>	<em>unsigned</em> <em>long</em> mask = rcu_rnp_online_cpus(rnp);</td></tr>
<tr><th id="1179">1179</th><td>	cpumask_var_t cm;</td></tr>
<tr><th id="1180">1180</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="1181">1181</th><td></td></tr>
<tr><th id="1182">1182</th><td>	<b>if</b> (!t)</td></tr>
<tr><th id="1183">1183</th><td>		<b>return</b>;</td></tr>
<tr><th id="1184">1184</th><td>	<b>if</b> (!zalloc_cpumask_var(&amp;cm, GFP_KERNEL))</td></tr>
<tr><th id="1185">1185</th><td>		<b>return</b>;</td></tr>
<tr><th id="1186">1186</th><td>	for_each_leaf_node_possible_cpu(rnp, cpu)</td></tr>
<tr><th id="1187">1187</th><td>		<b>if</b> ((mask &amp; leaf_node_cpu_bit(rnp, cpu)) &amp;&amp;</td></tr>
<tr><th id="1188">1188</th><td>		    cpu != outgoingcpu)</td></tr>
<tr><th id="1189">1189</th><td>			cpumask_set_cpu(cpu, cm);</td></tr>
<tr><th id="1190">1190</th><td>	<b>if</b> (cpumask_weight(cm) == <var>0</var>)</td></tr>
<tr><th id="1191">1191</th><td>		cpumask_setall(cm);</td></tr>
<tr><th id="1192">1192</th><td>	set_cpus_allowed_ptr(t, cm);</td></tr>
<tr><th id="1193">1193</th><td>	free_cpumask_var(cm);</td></tr>
<tr><th id="1194">1194</th><td>}</td></tr>
<tr><th id="1195">1195</th><td></td></tr>
<tr><th id="1196">1196</th><td><i>/*</i></td></tr>
<tr><th id="1197">1197</th><td><i> * Spawn boost kthreads -- called as soon as the scheduler is running.</i></td></tr>
<tr><th id="1198">1198</th><td><i> */</i></td></tr>
<tr><th id="1199">1199</th><td><em>static</em> <em>void</em> __init rcu_spawn_boost_kthreads(<em>void</em>)</td></tr>
<tr><th id="1200">1200</th><td>{</td></tr>
<tr><th id="1201">1201</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1202">1202</th><td></td></tr>
<tr><th id="1203">1203</th><td>	rcu_for_each_leaf_node(rnp)</td></tr>
<tr><th id="1204">1204</th><td>		(<em>void</em>)rcu_spawn_one_boost_kthread(rnp);</td></tr>
<tr><th id="1205">1205</th><td>}</td></tr>
<tr><th id="1206">1206</th><td></td></tr>
<tr><th id="1207">1207</th><td><em>static</em> <em>void</em> rcu_prepare_kthreads(<em>int</em> cpu)</td></tr>
<tr><th id="1208">1208</th><td>{</td></tr>
<tr><th id="1209">1209</th><td>	<b>struct</b> rcu_data *rdp = per_cpu_ptr(&amp;rcu_data, cpu);</td></tr>
<tr><th id="1210">1210</th><td>	<b>struct</b> rcu_node *rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1211">1211</th><td></td></tr>
<tr><th id="1212">1212</th><td>	<i>/* Fire up the incoming CPU's kthread and leaf rcu_node kthread. */</i></td></tr>
<tr><th id="1213">1213</th><td>	<b>if</b> (rcu_scheduler_fully_active)</td></tr>
<tr><th id="1214">1214</th><td>		(<em>void</em>)rcu_spawn_one_boost_kthread(rnp);</td></tr>
<tr><th id="1215">1215</th><td>}</td></tr>
<tr><th id="1216">1216</th><td></td></tr>
<tr><th id="1217">1217</th><td><u>#<span data-ppcond="977">else</span> /* #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="1218">1218</th><td></td></tr>
<tr><th id="1219">1219</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_initiate_boost" title='rcu_initiate_boost' data-ref="rcu_initiate_boost" data-ref-filename="rcu_initiate_boost">rcu_initiate_boost</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col1 decl" id="531rnp" title='rnp' data-type='struct rcu_node *' data-ref="531rnp" data-ref-filename="531rnp">rnp</dfn>, <em>unsigned</em> <em>long</em> <dfn class="local col2 decl" id="532flags" title='flags' data-type='unsigned long' data-ref="532flags" data-ref-filename="532flags">flags</dfn>)</td></tr>
<tr><th id="1220">1220</th><td>	<a class="macro" href="../../include/linux/compiler_types.h.html#42" title="" data-ref="_M/__releases">__releases</a>(rnp-&gt;lock)</td></tr>
<tr><th id="1221">1221</th><td>{</td></tr>
<tr><th id="1222">1222</th><td>	<a class="macro" href="rcu.h.html#403" title="do { ({ unsigned long __dummy; typeof(flags) __dummy2; (void)(&amp;__dummy == &amp;__dummy2); 1; }); _raw_spin_unlock_irqrestore(&amp;((rnp)-&gt;lock), flags); } while (0)" data-ref="_M/raw_spin_unlock_irqrestore_rcu_node">raw_spin_unlock_irqrestore_rcu_node</a>(<a class="local col1 ref" href="#531rnp" title='rnp' data-ref="531rnp" data-ref-filename="531rnp">rnp</a>, <a class="local col2 ref" href="#532flags" title='flags' data-ref="532flags" data-ref-filename="532flags">flags</a>);</td></tr>
<tr><th id="1223">1223</th><td>}</td></tr>
<tr><th id="1224">1224</th><td></td></tr>
<tr><th id="1225">1225</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="rcu_is_callbacks_kthread" title='rcu_is_callbacks_kthread' data-ref="rcu_is_callbacks_kthread" data-ref-filename="rcu_is_callbacks_kthread">rcu_is_callbacks_kthread</dfn>(<em>void</em>)</td></tr>
<tr><th id="1226">1226</th><td>{</td></tr>
<tr><th id="1227">1227</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="1228">1228</th><td>}</td></tr>
<tr><th id="1229">1229</th><td></td></tr>
<tr><th id="1230">1230</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_preempt_boost_start_gp" title='rcu_preempt_boost_start_gp' data-ref="rcu_preempt_boost_start_gp" data-ref-filename="rcu_preempt_boost_start_gp">rcu_preempt_boost_start_gp</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col3 decl" id="533rnp" title='rnp' data-type='struct rcu_node *' data-ref="533rnp" data-ref-filename="533rnp">rnp</dfn>)</td></tr>
<tr><th id="1231">1231</th><td>{</td></tr>
<tr><th id="1232">1232</th><td>}</td></tr>
<tr><th id="1233">1233</th><td></td></tr>
<tr><th id="1234">1234</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_boost_kthread_setaffinity" title='rcu_boost_kthread_setaffinity' data-ref="rcu_boost_kthread_setaffinity" data-ref-filename="rcu_boost_kthread_setaffinity">rcu_boost_kthread_setaffinity</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col4 decl" id="534rnp" title='rnp' data-type='struct rcu_node *' data-ref="534rnp" data-ref-filename="534rnp">rnp</dfn>, <em>int</em> <dfn class="local col5 decl" id="535outgoingcpu" title='outgoingcpu' data-type='int' data-ref="535outgoingcpu" data-ref-filename="535outgoingcpu">outgoingcpu</dfn>)</td></tr>
<tr><th id="1235">1235</th><td>{</td></tr>
<tr><th id="1236">1236</th><td>}</td></tr>
<tr><th id="1237">1237</th><td></td></tr>
<tr><th id="1238">1238</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__((__section__(&quot;.init.text&quot;))) __attribute__((__cold__))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_spawn_boost_kthreads" title='rcu_spawn_boost_kthreads' data-ref="rcu_spawn_boost_kthreads" data-ref-filename="rcu_spawn_boost_kthreads">rcu_spawn_boost_kthreads</dfn>(<em>void</em>)</td></tr>
<tr><th id="1239">1239</th><td>{</td></tr>
<tr><th id="1240">1240</th><td>}</td></tr>
<tr><th id="1241">1241</th><td></td></tr>
<tr><th id="1242">1242</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_prepare_kthreads" title='rcu_prepare_kthreads' data-ref="rcu_prepare_kthreads" data-ref-filename="rcu_prepare_kthreads">rcu_prepare_kthreads</dfn>(<em>int</em> <dfn class="local col6 decl" id="536cpu" title='cpu' data-type='int' data-ref="536cpu" data-ref-filename="536cpu">cpu</dfn>)</td></tr>
<tr><th id="1243">1243</th><td>{</td></tr>
<tr><th id="1244">1244</th><td>}</td></tr>
<tr><th id="1245">1245</th><td></td></tr>
<tr><th id="1246">1246</th><td><u>#<span data-ppcond="977">endif</span> /* #else #ifdef CONFIG_RCU_BOOST */</u></td></tr>
<tr><th id="1247">1247</th><td></td></tr>
<tr><th id="1248">1248</th><td><u>#<span data-ppcond="1248">if</span> !defined(<span class="macro" data-ref="_M/CONFIG_RCU_FAST_NO_HZ">CONFIG_RCU_FAST_NO_HZ</span>)</u></td></tr>
<tr><th id="1249">1249</th><td></td></tr>
<tr><th id="1250">1250</th><td><i>/*</i></td></tr>
<tr><th id="1251">1251</th><td><i> * Check to see if any future RCU-related work will need to be done</i></td></tr>
<tr><th id="1252">1252</th><td><i> * by the current CPU, even if none need be done immediately, returning</i></td></tr>
<tr><th id="1253">1253</th><td><i> * 1 if so.  This function is part of the RCU implementation; it is -not-</i></td></tr>
<tr><th id="1254">1254</th><td><i> * an exported member of the RCU API.</i></td></tr>
<tr><th id="1255">1255</th><td><i> *</i></td></tr>
<tr><th id="1256">1256</th><td><i> * Because we not have RCU_FAST_NO_HZ, just check whether or not this</i></td></tr>
<tr><th id="1257">1257</th><td><i> * CPU has RCU callbacks queued.</i></td></tr>
<tr><th id="1258">1258</th><td><i> */</i></td></tr>
<tr><th id="1259">1259</th><td><em>int</em> <dfn class="decl def fn" id="rcu_needs_cpu" title='rcu_needs_cpu' data-ref="rcu_needs_cpu" data-ref-filename="rcu_needs_cpu">rcu_needs_cpu</dfn>(<a class="typedef" href="../../include/asm-generic/int-ll64.h.html#u64" title='u64' data-type='__u64' data-ref="u64" data-ref-filename="u64">u64</a> <dfn class="local col7 decl" id="537basemono" title='basemono' data-type='u64' data-ref="537basemono" data-ref-filename="537basemono">basemono</dfn>, <a class="typedef" href="../../include/asm-generic/int-ll64.h.html#u64" title='u64' data-type='__u64' data-ref="u64" data-ref-filename="u64">u64</a> *<dfn class="local col8 decl" id="538nextevt" title='nextevt' data-type='u64 *' data-ref="538nextevt" data-ref-filename="538nextevt">nextevt</dfn>)</td></tr>
<tr><th id="1260">1260</th><td>{</td></tr>
<tr><th id="1261">1261</th><td>	*<a class="local col8 ref" href="#538nextevt" title='nextevt' data-ref="538nextevt" data-ref-filename="538nextevt">nextevt</a> = <a class="macro" href="../../include/linux/time64.h.html#33" title="((s64)~((u64)1 &lt;&lt; 63))" data-ref="_M/KTIME_MAX">KTIME_MAX</a>;</td></tr>
<tr><th id="1262">1262</th><td>	<b>return</b> !<a class="ref fn" href="rcu_segcblist.h.html#rcu_segcblist_empty" title='rcu_segcblist_empty' data-ref="rcu_segcblist_empty" data-ref-filename="rcu_segcblist_empty">rcu_segcblist_empty</a>(&amp;<a class="macro" href="../../include/linux/percpu-defs.h.html#253" title="({ do { const void *__vpp_verify = (typeof((&amp;rcu_data) + 0))((void *)0); (void)__vpp_verify; } while (0); ({ unsigned long tcp_ptr__; asm volatile(&quot;add &quot; &quot;%%&quot;&quot;gs&quot;&quot;:&quot; &quot;%&quot; &quot;1&quot; &quot;, %0&quot; : &quot;=r&quot; (tcp_ptr__) : &quot;m&quot; (this_cpu_off), &quot;0&quot; (&amp;rcu_data)); (typeof(*(&amp;rcu_data)) *)tcp_ptr__; }); })" data-ref="_M/this_cpu_ptr">this_cpu_ptr</a>(&amp;<a class="tu ref" href="tree.c.html#81" title='rcu_data' data-use='a' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a>)-&gt;<a class="ref field" href="tree.h.html#rcu_data::cblist" title='rcu_data::cblist' data-ref="rcu_data::cblist" data-ref-filename="rcu_data..cblist">cblist</a>);</td></tr>
<tr><th id="1263">1263</th><td>}</td></tr>
<tr><th id="1264">1264</th><td></td></tr>
<tr><th id="1265">1265</th><td><i>/*</i></td></tr>
<tr><th id="1266">1266</th><td><i> * Because we do not have RCU_FAST_NO_HZ, don't bother cleaning up</i></td></tr>
<tr><th id="1267">1267</th><td><i> * after it.</i></td></tr>
<tr><th id="1268">1268</th><td><i> */</i></td></tr>
<tr><th id="1269">1269</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_cleanup_after_idle" title='rcu_cleanup_after_idle' data-ref="rcu_cleanup_after_idle" data-ref-filename="rcu_cleanup_after_idle">rcu_cleanup_after_idle</dfn>(<em>void</em>)</td></tr>
<tr><th id="1270">1270</th><td>{</td></tr>
<tr><th id="1271">1271</th><td>}</td></tr>
<tr><th id="1272">1272</th><td></td></tr>
<tr><th id="1273">1273</th><td><i>/*</i></td></tr>
<tr><th id="1274">1274</th><td><i> * Do the idle-entry grace-period work, which, because CONFIG_RCU_FAST_NO_HZ=n,</i></td></tr>
<tr><th id="1275">1275</th><td><i> * is nothing.</i></td></tr>
<tr><th id="1276">1276</th><td><i> */</i></td></tr>
<tr><th id="1277">1277</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_prepare_for_idle" title='rcu_prepare_for_idle' data-ref="rcu_prepare_for_idle" data-ref-filename="rcu_prepare_for_idle">rcu_prepare_for_idle</dfn>(<em>void</em>)</td></tr>
<tr><th id="1278">1278</th><td>{</td></tr>
<tr><th id="1279">1279</th><td>}</td></tr>
<tr><th id="1280">1280</th><td></td></tr>
<tr><th id="1281">1281</th><td><u>#<span data-ppcond="1248">else</span> /* #if !defined(CONFIG_RCU_FAST_NO_HZ) */</u></td></tr>
<tr><th id="1282">1282</th><td></td></tr>
<tr><th id="1283">1283</th><td><i>/*</i></td></tr>
<tr><th id="1284">1284</th><td><i> * This code is invoked when a CPU goes idle, at which point we want</i></td></tr>
<tr><th id="1285">1285</th><td><i> * to have the CPU do everything required for RCU so that it can enter</i></td></tr>
<tr><th id="1286">1286</th><td><i> * the energy-efficient dyntick-idle mode.  This is handled by a</i></td></tr>
<tr><th id="1287">1287</th><td><i> * state machine implemented by rcu_prepare_for_idle() below.</i></td></tr>
<tr><th id="1288">1288</th><td><i> *</i></td></tr>
<tr><th id="1289">1289</th><td><i> * The following three proprocessor symbols control this state machine:</i></td></tr>
<tr><th id="1290">1290</th><td><i> *</i></td></tr>
<tr><th id="1291">1291</th><td><i> * RCU_IDLE_GP_DELAY gives the number of jiffies that a CPU is permitted</i></td></tr>
<tr><th id="1292">1292</th><td><i> *	to sleep in dyntick-idle mode with RCU callbacks pending.  This</i></td></tr>
<tr><th id="1293">1293</th><td><i> *	is sized to be roughly one RCU grace period.  Those energy-efficiency</i></td></tr>
<tr><th id="1294">1294</th><td><i> *	benchmarkers who might otherwise be tempted to set this to a large</i></td></tr>
<tr><th id="1295">1295</th><td><i> *	number, be warned: Setting RCU_IDLE_GP_DELAY too high can hang your</i></td></tr>
<tr><th id="1296">1296</th><td><i> *	system.  And if you are -that- concerned about energy efficiency,</i></td></tr>
<tr><th id="1297">1297</th><td><i> *	just power the system down and be done with it!</i></td></tr>
<tr><th id="1298">1298</th><td><i> * RCU_IDLE_LAZY_GP_DELAY gives the number of jiffies that a CPU is</i></td></tr>
<tr><th id="1299">1299</th><td><i> *	permitted to sleep in dyntick-idle mode with only lazy RCU</i></td></tr>
<tr><th id="1300">1300</th><td><i> *	callbacks pending.  Setting this too high can OOM your system.</i></td></tr>
<tr><th id="1301">1301</th><td><i> *</i></td></tr>
<tr><th id="1302">1302</th><td><i> * The values below work well in practice.  If future workloads require</i></td></tr>
<tr><th id="1303">1303</th><td><i> * adjustment, they can be converted into kernel config parameters, though</i></td></tr>
<tr><th id="1304">1304</th><td><i> * making the state machine smarter might be a better option.</i></td></tr>
<tr><th id="1305">1305</th><td><i> */</i></td></tr>
<tr><th id="1306">1306</th><td><u>#define RCU_IDLE_GP_DELAY 4		/* Roughly one grace period. */</u></td></tr>
<tr><th id="1307">1307</th><td><u>#define RCU_IDLE_LAZY_GP_DELAY (6 * HZ)	/* Roughly six seconds. */</u></td></tr>
<tr><th id="1308">1308</th><td></td></tr>
<tr><th id="1309">1309</th><td><em>static</em> <em>int</em> rcu_idle_gp_delay = RCU_IDLE_GP_DELAY;</td></tr>
<tr><th id="1310">1310</th><td>module_param(rcu_idle_gp_delay, <em>int</em>, <var>0644</var>);</td></tr>
<tr><th id="1311">1311</th><td><em>static</em> <em>int</em> rcu_idle_lazy_gp_delay = RCU_IDLE_LAZY_GP_DELAY;</td></tr>
<tr><th id="1312">1312</th><td>module_param(rcu_idle_lazy_gp_delay, <em>int</em>, <var>0644</var>);</td></tr>
<tr><th id="1313">1313</th><td></td></tr>
<tr><th id="1314">1314</th><td><i>/*</i></td></tr>
<tr><th id="1315">1315</th><td><i> * Try to advance callbacks on the current CPU, but only if it has been</i></td></tr>
<tr><th id="1316">1316</th><td><i> * awhile since the last time we did so.  Afterwards, if there are any</i></td></tr>
<tr><th id="1317">1317</th><td><i> * callbacks ready for immediate invocation, return true.</i></td></tr>
<tr><th id="1318">1318</th><td><i> */</i></td></tr>
<tr><th id="1319">1319</th><td><em>static</em> bool __maybe_unused rcu_try_advance_all_cbs(<em>void</em>)</td></tr>
<tr><th id="1320">1320</th><td>{</td></tr>
<tr><th id="1321">1321</th><td>	bool cbs_ready = false;</td></tr>
<tr><th id="1322">1322</th><td>	<b>struct</b> rcu_data *rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="1323">1323</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1324">1324</th><td></td></tr>
<tr><th id="1325">1325</th><td>	<i>/* Exit early if we advanced recently. */</i></td></tr>
<tr><th id="1326">1326</th><td>	<b>if</b> (jiffies == rdp-&gt;last_advance_all)</td></tr>
<tr><th id="1327">1327</th><td>		<b>return</b> false;</td></tr>
<tr><th id="1328">1328</th><td>	rdp-&gt;last_advance_all = jiffies;</td></tr>
<tr><th id="1329">1329</th><td></td></tr>
<tr><th id="1330">1330</th><td>	rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1331">1331</th><td></td></tr>
<tr><th id="1332">1332</th><td>	<i>/*</i></td></tr>
<tr><th id="1333">1333</th><td><i>	 * Don't bother checking unless a grace period has</i></td></tr>
<tr><th id="1334">1334</th><td><i>	 * completed since we last checked and there are</i></td></tr>
<tr><th id="1335">1335</th><td><i>	 * callbacks not yet ready to invoke.</i></td></tr>
<tr><th id="1336">1336</th><td><i>	 */</i></td></tr>
<tr><th id="1337">1337</th><td>	<b>if</b> ((rcu_seq_completed_gp(rdp-&gt;gp_seq,</td></tr>
<tr><th id="1338">1338</th><td>				  rcu_seq_current(&amp;rnp-&gt;gp_seq)) ||</td></tr>
<tr><th id="1339">1339</th><td>	     unlikely(READ_ONCE(rdp-&gt;gpwrap))) &amp;&amp;</td></tr>
<tr><th id="1340">1340</th><td>	    rcu_segcblist_pend_cbs(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1341">1341</th><td>		note_gp_changes(rdp);</td></tr>
<tr><th id="1342">1342</th><td></td></tr>
<tr><th id="1343">1343</th><td>	<b>if</b> (rcu_segcblist_ready_cbs(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1344">1344</th><td>		cbs_ready = true;</td></tr>
<tr><th id="1345">1345</th><td>	<b>return</b> cbs_ready;</td></tr>
<tr><th id="1346">1346</th><td>}</td></tr>
<tr><th id="1347">1347</th><td></td></tr>
<tr><th id="1348">1348</th><td><i>/*</i></td></tr>
<tr><th id="1349">1349</th><td><i> * Allow the CPU to enter dyntick-idle mode unless it has callbacks ready</i></td></tr>
<tr><th id="1350">1350</th><td><i> * to invoke.  If the CPU has callbacks, try to advance them.  Tell the</i></td></tr>
<tr><th id="1351">1351</th><td><i> * caller to set the timeout based on whether or not there are non-lazy</i></td></tr>
<tr><th id="1352">1352</th><td><i> * callbacks.</i></td></tr>
<tr><th id="1353">1353</th><td><i> *</i></td></tr>
<tr><th id="1354">1354</th><td><i> * The caller must have disabled interrupts.</i></td></tr>
<tr><th id="1355">1355</th><td><i> */</i></td></tr>
<tr><th id="1356">1356</th><td><em>int</em> rcu_needs_cpu(u64 basemono, u64 *nextevt)</td></tr>
<tr><th id="1357">1357</th><td>{</td></tr>
<tr><th id="1358">1358</th><td>	<b>struct</b> rcu_data *rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="1359">1359</th><td>	<em>unsigned</em> <em>long</em> dj;</td></tr>
<tr><th id="1360">1360</th><td></td></tr>
<tr><th id="1361">1361</th><td>	lockdep_assert_irqs_disabled();</td></tr>
<tr><th id="1362">1362</th><td></td></tr>
<tr><th id="1363">1363</th><td>	<i>/* If no callbacks, RCU doesn't need the CPU. */</i></td></tr>
<tr><th id="1364">1364</th><td>	<b>if</b> (rcu_segcblist_empty(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="1365">1365</th><td>		*nextevt = KTIME_MAX;</td></tr>
<tr><th id="1366">1366</th><td>		<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1367">1367</th><td>	}</td></tr>
<tr><th id="1368">1368</th><td></td></tr>
<tr><th id="1369">1369</th><td>	<i>/* Attempt to advance callbacks. */</i></td></tr>
<tr><th id="1370">1370</th><td>	<b>if</b> (rcu_try_advance_all_cbs()) {</td></tr>
<tr><th id="1371">1371</th><td>		<i>/* Some ready to invoke, so initiate later invocation. */</i></td></tr>
<tr><th id="1372">1372</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1373">1373</th><td>		<b>return</b> <var>1</var>;</td></tr>
<tr><th id="1374">1374</th><td>	}</td></tr>
<tr><th id="1375">1375</th><td>	rdp-&gt;last_accelerate = jiffies;</td></tr>
<tr><th id="1376">1376</th><td></td></tr>
<tr><th id="1377">1377</th><td>	<i>/* Request timer delay depending on laziness, and round. */</i></td></tr>
<tr><th id="1378">1378</th><td>	rdp-&gt;all_lazy = !rcu_segcblist_n_nonlazy_cbs(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="1379">1379</th><td>	<b>if</b> (rdp-&gt;all_lazy) {</td></tr>
<tr><th id="1380">1380</th><td>		dj = round_jiffies(rcu_idle_lazy_gp_delay + jiffies) - jiffies;</td></tr>
<tr><th id="1381">1381</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1382">1382</th><td>		dj = round_up(rcu_idle_gp_delay + jiffies,</td></tr>
<tr><th id="1383">1383</th><td>			       rcu_idle_gp_delay) - jiffies;</td></tr>
<tr><th id="1384">1384</th><td>	}</td></tr>
<tr><th id="1385">1385</th><td>	*nextevt = basemono + dj * TICK_NSEC;</td></tr>
<tr><th id="1386">1386</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1387">1387</th><td>}</td></tr>
<tr><th id="1388">1388</th><td></td></tr>
<tr><th id="1389">1389</th><td><i>/*</i></td></tr>
<tr><th id="1390">1390</th><td><i> * Prepare a CPU for idle from an RCU perspective.  The first major task</i></td></tr>
<tr><th id="1391">1391</th><td><i> * is to sense whether nohz mode has been enabled or disabled via sysfs.</i></td></tr>
<tr><th id="1392">1392</th><td><i> * The second major task is to check to see if a non-lazy callback has</i></td></tr>
<tr><th id="1393">1393</th><td><i> * arrived at a CPU that previously had only lazy callbacks.  The third</i></td></tr>
<tr><th id="1394">1394</th><td><i> * major task is to accelerate (that is, assign grace-period numbers to)</i></td></tr>
<tr><th id="1395">1395</th><td><i> * any recently arrived callbacks.</i></td></tr>
<tr><th id="1396">1396</th><td><i> *</i></td></tr>
<tr><th id="1397">1397</th><td><i> * The caller must have disabled interrupts.</i></td></tr>
<tr><th id="1398">1398</th><td><i> */</i></td></tr>
<tr><th id="1399">1399</th><td><em>static</em> <em>void</em> rcu_prepare_for_idle(<em>void</em>)</td></tr>
<tr><th id="1400">1400</th><td>{</td></tr>
<tr><th id="1401">1401</th><td>	bool needwake;</td></tr>
<tr><th id="1402">1402</th><td>	<b>struct</b> rcu_data *rdp = this_cpu_ptr(&amp;rcu_data);</td></tr>
<tr><th id="1403">1403</th><td>	<b>struct</b> rcu_node *rnp;</td></tr>
<tr><th id="1404">1404</th><td>	<em>int</em> tne;</td></tr>
<tr><th id="1405">1405</th><td></td></tr>
<tr><th id="1406">1406</th><td>	lockdep_assert_irqs_disabled();</td></tr>
<tr><th id="1407">1407</th><td>	<b>if</b> (rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="1408">1408</th><td>		<b>return</b>;</td></tr>
<tr><th id="1409">1409</th><td></td></tr>
<tr><th id="1410">1410</th><td>	<i>/* Handle nohz enablement switches conservatively. */</i></td></tr>
<tr><th id="1411">1411</th><td>	tne = READ_ONCE(tick_nohz_active);</td></tr>
<tr><th id="1412">1412</th><td>	<b>if</b> (tne != rdp-&gt;tick_nohz_enabled_snap) {</td></tr>
<tr><th id="1413">1413</th><td>		<b>if</b> (!rcu_segcblist_empty(&amp;rdp-&gt;cblist))</td></tr>
<tr><th id="1414">1414</th><td>			invoke_rcu_core(); <i>/* force nohz to see update. */</i></td></tr>
<tr><th id="1415">1415</th><td>		rdp-&gt;tick_nohz_enabled_snap = tne;</td></tr>
<tr><th id="1416">1416</th><td>		<b>return</b>;</td></tr>
<tr><th id="1417">1417</th><td>	}</td></tr>
<tr><th id="1418">1418</th><td>	<b>if</b> (!tne)</td></tr>
<tr><th id="1419">1419</th><td>		<b>return</b>;</td></tr>
<tr><th id="1420">1420</th><td></td></tr>
<tr><th id="1421">1421</th><td>	<i>/*</i></td></tr>
<tr><th id="1422">1422</th><td><i>	 * If a non-lazy callback arrived at a CPU having only lazy</i></td></tr>
<tr><th id="1423">1423</th><td><i>	 * callbacks, invoke RCU core for the side-effect of recalculating</i></td></tr>
<tr><th id="1424">1424</th><td><i>	 * idle duration on re-entry to idle.</i></td></tr>
<tr><th id="1425">1425</th><td><i>	 */</i></td></tr>
<tr><th id="1426">1426</th><td>	<b>if</b> (rdp-&gt;all_lazy &amp;&amp; rcu_segcblist_n_nonlazy_cbs(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="1427">1427</th><td>		rdp-&gt;all_lazy = false;</td></tr>
<tr><th id="1428">1428</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1429">1429</th><td>		<b>return</b>;</td></tr>
<tr><th id="1430">1430</th><td>	}</td></tr>
<tr><th id="1431">1431</th><td></td></tr>
<tr><th id="1432">1432</th><td>	<i>/*</i></td></tr>
<tr><th id="1433">1433</th><td><i>	 * If we have not yet accelerated this jiffy, accelerate all</i></td></tr>
<tr><th id="1434">1434</th><td><i>	 * callbacks on this CPU.</i></td></tr>
<tr><th id="1435">1435</th><td><i>	 */</i></td></tr>
<tr><th id="1436">1436</th><td>	<b>if</b> (rdp-&gt;last_accelerate == jiffies)</td></tr>
<tr><th id="1437">1437</th><td>		<b>return</b>;</td></tr>
<tr><th id="1438">1438</th><td>	rdp-&gt;last_accelerate = jiffies;</td></tr>
<tr><th id="1439">1439</th><td>	<b>if</b> (rcu_segcblist_pend_cbs(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="1440">1440</th><td>		rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1441">1441</th><td>		raw_spin_lock_rcu_node(rnp); <i>/* irqs already disabled. */</i></td></tr>
<tr><th id="1442">1442</th><td>		needwake = rcu_accelerate_cbs(rnp, rdp);</td></tr>
<tr><th id="1443">1443</th><td>		raw_spin_unlock_rcu_node(rnp); <i>/* irqs remain disabled. */</i></td></tr>
<tr><th id="1444">1444</th><td>		<b>if</b> (needwake)</td></tr>
<tr><th id="1445">1445</th><td>			rcu_gp_kthread_wake();</td></tr>
<tr><th id="1446">1446</th><td>	}</td></tr>
<tr><th id="1447">1447</th><td>}</td></tr>
<tr><th id="1448">1448</th><td></td></tr>
<tr><th id="1449">1449</th><td><i>/*</i></td></tr>
<tr><th id="1450">1450</th><td><i> * Clean up for exit from idle.  Attempt to advance callbacks based on</i></td></tr>
<tr><th id="1451">1451</th><td><i> * any grace periods that elapsed while the CPU was idle, and if any</i></td></tr>
<tr><th id="1452">1452</th><td><i> * callbacks are now ready to invoke, initiate invocation.</i></td></tr>
<tr><th id="1453">1453</th><td><i> */</i></td></tr>
<tr><th id="1454">1454</th><td><em>static</em> <em>void</em> rcu_cleanup_after_idle(<em>void</em>)</td></tr>
<tr><th id="1455">1455</th><td>{</td></tr>
<tr><th id="1456">1456</th><td>	lockdep_assert_irqs_disabled();</td></tr>
<tr><th id="1457">1457</th><td>	<b>if</b> (rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="1458">1458</th><td>		<b>return</b>;</td></tr>
<tr><th id="1459">1459</th><td>	<b>if</b> (rcu_try_advance_all_cbs())</td></tr>
<tr><th id="1460">1460</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1461">1461</th><td>}</td></tr>
<tr><th id="1462">1462</th><td></td></tr>
<tr><th id="1463">1463</th><td><u>#<span data-ppcond="1248">endif</span> /* #else #if !defined(CONFIG_RCU_FAST_NO_HZ) */</u></td></tr>
<tr><th id="1464">1464</th><td></td></tr>
<tr><th id="1465">1465</th><td><u>#<span data-ppcond="1465">ifdef</span> <span class="macro" data-ref="_M/CONFIG_RCU_NOCB_CPU">CONFIG_RCU_NOCB_CPU</span></u></td></tr>
<tr><th id="1466">1466</th><td></td></tr>
<tr><th id="1467">1467</th><td><i>/*</i></td></tr>
<tr><th id="1468">1468</th><td><i> * Offload callback processing from the boot-time-specified set of CPUs</i></td></tr>
<tr><th id="1469">1469</th><td><i> * specified by rcu_nocb_mask.  For the CPUs in the set, there are kthreads</i></td></tr>
<tr><th id="1470">1470</th><td><i> * created that pull the callbacks from the corresponding CPU, wait for</i></td></tr>
<tr><th id="1471">1471</th><td><i> * a grace period to elapse, and invoke the callbacks.  These kthreads</i></td></tr>
<tr><th id="1472">1472</th><td><i> * are organized into leaders, which manage incoming callbacks, wait for</i></td></tr>
<tr><th id="1473">1473</th><td><i> * grace periods, and awaken followers, and the followers, which only</i></td></tr>
<tr><th id="1474">1474</th><td><i> * invoke callbacks.  Each leader is its own follower.  The no-CBs CPUs</i></td></tr>
<tr><th id="1475">1475</th><td><i> * do a wake_up() on their kthread when they insert a callback into any</i></td></tr>
<tr><th id="1476">1476</th><td><i> * empty list, unless the rcu_nocb_poll boot parameter has been specified,</i></td></tr>
<tr><th id="1477">1477</th><td><i> * in which case each kthread actively polls its CPU.  (Which isn't so great</i></td></tr>
<tr><th id="1478">1478</th><td><i> * for energy efficiency, but which does reduce RCU's overhead on that CPU.)</i></td></tr>
<tr><th id="1479">1479</th><td><i> *</i></td></tr>
<tr><th id="1480">1480</th><td><i> * This is intended to be used in conjunction with Frederic Weisbecker's</i></td></tr>
<tr><th id="1481">1481</th><td><i> * adaptive-idle work, which would seriously reduce OS jitter on CPUs</i></td></tr>
<tr><th id="1482">1482</th><td><i> * running CPU-bound user-mode computations.</i></td></tr>
<tr><th id="1483">1483</th><td><i> *</i></td></tr>
<tr><th id="1484">1484</th><td><i> * Offloading of callbacks can also be used as an energy-efficiency</i></td></tr>
<tr><th id="1485">1485</th><td><i> * measure because CPUs with no RCU callbacks queued are more aggressive</i></td></tr>
<tr><th id="1486">1486</th><td><i> * about entering dyntick-idle mode.</i></td></tr>
<tr><th id="1487">1487</th><td><i> */</i></td></tr>
<tr><th id="1488">1488</th><td></td></tr>
<tr><th id="1489">1489</th><td></td></tr>
<tr><th id="1490">1490</th><td><i>/*</i></td></tr>
<tr><th id="1491">1491</th><td><i> * Parse the boot-time rcu_nocb_mask CPU list from the kernel parameters.</i></td></tr>
<tr><th id="1492">1492</th><td><i> * The string after the "rcu_nocbs=" is either "all" for all CPUs, or a</i></td></tr>
<tr><th id="1493">1493</th><td><i> * comma-separated list of CPUs and/or CPU ranges.  If an invalid list is</i></td></tr>
<tr><th id="1494">1494</th><td><i> * given, a warning is emitted and all CPUs are offloaded.</i></td></tr>
<tr><th id="1495">1495</th><td><i> */</i></td></tr>
<tr><th id="1496">1496</th><td><em>static</em> <em>int</em> __init rcu_nocb_setup(<em>char</em> *str)</td></tr>
<tr><th id="1497">1497</th><td>{</td></tr>
<tr><th id="1498">1498</th><td>	alloc_bootmem_cpumask_var(&amp;rcu_nocb_mask);</td></tr>
<tr><th id="1499">1499</th><td>	<b>if</b> (!strcasecmp(str, <q>"all"</q>))</td></tr>
<tr><th id="1500">1500</th><td>		cpumask_setall(rcu_nocb_mask);</td></tr>
<tr><th id="1501">1501</th><td>	<b>else</b></td></tr>
<tr><th id="1502">1502</th><td>		<b>if</b> (cpulist_parse(str, rcu_nocb_mask)) {</td></tr>
<tr><th id="1503">1503</th><td>			pr_warn(<q>"rcu_nocbs= bad CPU range, all CPUs set\n"</q>);</td></tr>
<tr><th id="1504">1504</th><td>			cpumask_setall(rcu_nocb_mask);</td></tr>
<tr><th id="1505">1505</th><td>		}</td></tr>
<tr><th id="1506">1506</th><td>	<b>return</b> <var>1</var>;</td></tr>
<tr><th id="1507">1507</th><td>}</td></tr>
<tr><th id="1508">1508</th><td>__setup(<q>"rcu_nocbs="</q>, rcu_nocb_setup);</td></tr>
<tr><th id="1509">1509</th><td></td></tr>
<tr><th id="1510">1510</th><td><em>static</em> <em>int</em> __init parse_rcu_nocb_poll(<em>char</em> *arg)</td></tr>
<tr><th id="1511">1511</th><td>{</td></tr>
<tr><th id="1512">1512</th><td>	rcu_nocb_poll = true;</td></tr>
<tr><th id="1513">1513</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1514">1514</th><td>}</td></tr>
<tr><th id="1515">1515</th><td>early_param(<q>"rcu_nocb_poll"</q>, parse_rcu_nocb_poll);</td></tr>
<tr><th id="1516">1516</th><td></td></tr>
<tr><th id="1517">1517</th><td><i>/*</i></td></tr>
<tr><th id="1518">1518</th><td><i> * Wake up any no-CBs CPUs' kthreads that were waiting on the just-ended</i></td></tr>
<tr><th id="1519">1519</th><td><i> * grace period.</i></td></tr>
<tr><th id="1520">1520</th><td><i> */</i></td></tr>
<tr><th id="1521">1521</th><td><em>static</em> <em>void</em> rcu_nocb_gp_cleanup(<b>struct</b> swait_queue_head *sq)</td></tr>
<tr><th id="1522">1522</th><td>{</td></tr>
<tr><th id="1523">1523</th><td>	swake_up_all(sq);</td></tr>
<tr><th id="1524">1524</th><td>}</td></tr>
<tr><th id="1525">1525</th><td></td></tr>
<tr><th id="1526">1526</th><td><em>static</em> <b>struct</b> swait_queue_head *rcu_nocb_gp_get(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1527">1527</th><td>{</td></tr>
<tr><th id="1528">1528</th><td>	<b>return</b> &amp;rnp-&gt;nocb_gp_wq[rcu_seq_ctr(rnp-&gt;gp_seq) &amp; <var>0x1</var>];</td></tr>
<tr><th id="1529">1529</th><td>}</td></tr>
<tr><th id="1530">1530</th><td></td></tr>
<tr><th id="1531">1531</th><td><em>static</em> <em>void</em> rcu_init_one_nocb(<b>struct</b> rcu_node *rnp)</td></tr>
<tr><th id="1532">1532</th><td>{</td></tr>
<tr><th id="1533">1533</th><td>	init_swait_queue_head(&amp;rnp-&gt;nocb_gp_wq[<var>0</var>]);</td></tr>
<tr><th id="1534">1534</th><td>	init_swait_queue_head(&amp;rnp-&gt;nocb_gp_wq[<var>1</var>]);</td></tr>
<tr><th id="1535">1535</th><td>}</td></tr>
<tr><th id="1536">1536</th><td></td></tr>
<tr><th id="1537">1537</th><td><i>/* Is the specified CPU a no-CBs CPU? */</i></td></tr>
<tr><th id="1538">1538</th><td>bool rcu_is_nocb_cpu(<em>int</em> cpu)</td></tr>
<tr><th id="1539">1539</th><td>{</td></tr>
<tr><th id="1540">1540</th><td>	<b>if</b> (cpumask_available(rcu_nocb_mask))</td></tr>
<tr><th id="1541">1541</th><td>		<b>return</b> cpumask_test_cpu(cpu, rcu_nocb_mask);</td></tr>
<tr><th id="1542">1542</th><td>	<b>return</b> false;</td></tr>
<tr><th id="1543">1543</th><td>}</td></tr>
<tr><th id="1544">1544</th><td></td></tr>
<tr><th id="1545">1545</th><td><i>/*</i></td></tr>
<tr><th id="1546">1546</th><td><i> * Kick the leader kthread for this NOCB group.  Caller holds -&gt;nocb_lock</i></td></tr>
<tr><th id="1547">1547</th><td><i> * and this function releases it.</i></td></tr>
<tr><th id="1548">1548</th><td><i> */</i></td></tr>
<tr><th id="1549">1549</th><td><em>static</em> <em>void</em> __wake_nocb_leader(<b>struct</b> rcu_data *rdp, bool force,</td></tr>
<tr><th id="1550">1550</th><td>			       <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1551">1551</th><td>	__releases(rdp-&gt;nocb_lock)</td></tr>
<tr><th id="1552">1552</th><td>{</td></tr>
<tr><th id="1553">1553</th><td>	<b>struct</b> rcu_data *rdp_leader = rdp-&gt;nocb_leader;</td></tr>
<tr><th id="1554">1554</th><td></td></tr>
<tr><th id="1555">1555</th><td>	lockdep_assert_held(&amp;rdp-&gt;nocb_lock);</td></tr>
<tr><th id="1556">1556</th><td>	<b>if</b> (!READ_ONCE(rdp_leader-&gt;nocb_kthread)) {</td></tr>
<tr><th id="1557">1557</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1558">1558</th><td>		<b>return</b>;</td></tr>
<tr><th id="1559">1559</th><td>	}</td></tr>
<tr><th id="1560">1560</th><td>	<b>if</b> (rdp_leader-&gt;nocb_leader_sleep || force) {</td></tr>
<tr><th id="1561">1561</th><td>		<i>/* Prior smp_mb__after_atomic() orders against prior enqueue. */</i></td></tr>
<tr><th id="1562">1562</th><td>		WRITE_ONCE(rdp_leader-&gt;nocb_leader_sleep, false);</td></tr>
<tr><th id="1563">1563</th><td>		del_timer(&amp;rdp-&gt;nocb_timer);</td></tr>
<tr><th id="1564">1564</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1565">1565</th><td>		smp_mb(); <i>/* -&gt;nocb_leader_sleep before swake_up_one(). */</i></td></tr>
<tr><th id="1566">1566</th><td>		swake_up_one(&amp;rdp_leader-&gt;nocb_wq);</td></tr>
<tr><th id="1567">1567</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1568">1568</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1569">1569</th><td>	}</td></tr>
<tr><th id="1570">1570</th><td>}</td></tr>
<tr><th id="1571">1571</th><td></td></tr>
<tr><th id="1572">1572</th><td><i>/*</i></td></tr>
<tr><th id="1573">1573</th><td><i> * Kick the leader kthread for this NOCB group, but caller has not</i></td></tr>
<tr><th id="1574">1574</th><td><i> * acquired locks.</i></td></tr>
<tr><th id="1575">1575</th><td><i> */</i></td></tr>
<tr><th id="1576">1576</th><td><em>static</em> <em>void</em> wake_nocb_leader(<b>struct</b> rcu_data *rdp, bool force)</td></tr>
<tr><th id="1577">1577</th><td>{</td></tr>
<tr><th id="1578">1578</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1579">1579</th><td></td></tr>
<tr><th id="1580">1580</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1581">1581</th><td>	__wake_nocb_leader(rdp, force, flags);</td></tr>
<tr><th id="1582">1582</th><td>}</td></tr>
<tr><th id="1583">1583</th><td></td></tr>
<tr><th id="1584">1584</th><td><i>/*</i></td></tr>
<tr><th id="1585">1585</th><td><i> * Arrange to wake the leader kthread for this NOCB group at some</i></td></tr>
<tr><th id="1586">1586</th><td><i> * future time when it is safe to do so.</i></td></tr>
<tr><th id="1587">1587</th><td><i> */</i></td></tr>
<tr><th id="1588">1588</th><td><em>static</em> <em>void</em> wake_nocb_leader_defer(<b>struct</b> rcu_data *rdp, <em>int</em> waketype,</td></tr>
<tr><th id="1589">1589</th><td>				   <em>const</em> <em>char</em> *reason)</td></tr>
<tr><th id="1590">1590</th><td>{</td></tr>
<tr><th id="1591">1591</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1592">1592</th><td></td></tr>
<tr><th id="1593">1593</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1594">1594</th><td>	<b>if</b> (rdp-&gt;nocb_defer_wakeup == RCU_NOCB_WAKE_NOT)</td></tr>
<tr><th id="1595">1595</th><td>		mod_timer(&amp;rdp-&gt;nocb_timer, jiffies + <var>1</var>);</td></tr>
<tr><th id="1596">1596</th><td>	WRITE_ONCE(rdp-&gt;nocb_defer_wakeup, waketype);</td></tr>
<tr><th id="1597">1597</th><td>	trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, reason);</td></tr>
<tr><th id="1598">1598</th><td>	raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1599">1599</th><td>}</td></tr>
<tr><th id="1600">1600</th><td></td></tr>
<tr><th id="1601">1601</th><td><i>/* Does rcu_barrier need to queue an RCU callback on the specified CPU?  */</i></td></tr>
<tr><th id="1602">1602</th><td><em>static</em> bool rcu_nocb_cpu_needs_barrier(<em>int</em> cpu)</td></tr>
<tr><th id="1603">1603</th><td>{</td></tr>
<tr><th id="1604">1604</th><td>	<b>struct</b> rcu_data *rdp = per_cpu_ptr(&amp;rcu_data, cpu);</td></tr>
<tr><th id="1605">1605</th><td>	<em>unsigned</em> <em>long</em> ret;</td></tr>
<tr><th id="1606">1606</th><td><u>#ifdef CONFIG_PROVE_RCU</u></td></tr>
<tr><th id="1607">1607</th><td>	<b>struct</b> rcu_head *rhp;</td></tr>
<tr><th id="1608">1608</th><td><u>#endif /* #ifdef CONFIG_PROVE_RCU */</u></td></tr>
<tr><th id="1609">1609</th><td></td></tr>
<tr><th id="1610">1610</th><td>	<i>/*</i></td></tr>
<tr><th id="1611">1611</th><td><i>	 * Check count of all no-CBs callbacks awaiting invocation.</i></td></tr>
<tr><th id="1612">1612</th><td><i>	 * There needs to be a barrier before this function is called,</i></td></tr>
<tr><th id="1613">1613</th><td><i>	 * but associated with a prior determination that no more</i></td></tr>
<tr><th id="1614">1614</th><td><i>	 * callbacks would be posted.  In the worst case, the first</i></td></tr>
<tr><th id="1615">1615</th><td><i>	 * barrier in rcu_barrier() suffices (but the caller cannot</i></td></tr>
<tr><th id="1616">1616</th><td><i>	 * necessarily rely on this, not a substitute for the caller</i></td></tr>
<tr><th id="1617">1617</th><td><i>	 * getting the concurrency design right!).  There must also be a</i></td></tr>
<tr><th id="1618">1618</th><td><i>	 * barrier between the following load and posting of a callback</i></td></tr>
<tr><th id="1619">1619</th><td><i>	 * (if a callback is in fact needed).  This is associated with an</i></td></tr>
<tr><th id="1620">1620</th><td><i>	 * atomic_inc() in the caller.</i></td></tr>
<tr><th id="1621">1621</th><td><i>	 */</i></td></tr>
<tr><th id="1622">1622</th><td>	ret = rcu_get_n_cbs_nocb_cpu(rdp);</td></tr>
<tr><th id="1623">1623</th><td></td></tr>
<tr><th id="1624">1624</th><td><u>#ifdef CONFIG_PROVE_RCU</u></td></tr>
<tr><th id="1625">1625</th><td>	rhp = READ_ONCE(rdp-&gt;nocb_head);</td></tr>
<tr><th id="1626">1626</th><td>	<b>if</b> (!rhp)</td></tr>
<tr><th id="1627">1627</th><td>		rhp = READ_ONCE(rdp-&gt;nocb_gp_head);</td></tr>
<tr><th id="1628">1628</th><td>	<b>if</b> (!rhp)</td></tr>
<tr><th id="1629">1629</th><td>		rhp = READ_ONCE(rdp-&gt;nocb_follower_head);</td></tr>
<tr><th id="1630">1630</th><td></td></tr>
<tr><th id="1631">1631</th><td>	<i>/* Having no rcuo kthread but CBs after scheduler starts is bad! */</i></td></tr>
<tr><th id="1632">1632</th><td>	<b>if</b> (!READ_ONCE(rdp-&gt;nocb_kthread) &amp;&amp; rhp &amp;&amp;</td></tr>
<tr><th id="1633">1633</th><td>	    rcu_scheduler_fully_active) {</td></tr>
<tr><th id="1634">1634</th><td>		<i>/* RCU callback enqueued before CPU first came online??? */</i></td></tr>
<tr><th id="1635">1635</th><td>		pr_err(<q>"RCU: Never-onlined no-CBs CPU %d has CB %p\n"</q>,</td></tr>
<tr><th id="1636">1636</th><td>		       cpu, rhp-&gt;func);</td></tr>
<tr><th id="1637">1637</th><td>		WARN_ON_ONCE(<var>1</var>);</td></tr>
<tr><th id="1638">1638</th><td>	}</td></tr>
<tr><th id="1639">1639</th><td><u>#endif /* #ifdef CONFIG_PROVE_RCU */</u></td></tr>
<tr><th id="1640">1640</th><td></td></tr>
<tr><th id="1641">1641</th><td>	<b>return</b> !!ret;</td></tr>
<tr><th id="1642">1642</th><td>}</td></tr>
<tr><th id="1643">1643</th><td></td></tr>
<tr><th id="1644">1644</th><td><i>/*</i></td></tr>
<tr><th id="1645">1645</th><td><i> * Enqueue the specified string of rcu_head structures onto the specified</i></td></tr>
<tr><th id="1646">1646</th><td><i> * CPU's no-CBs lists.  The CPU is specified by rdp, the head of the</i></td></tr>
<tr><th id="1647">1647</th><td><i> * string by rhp, and the tail of the string by rhtp.  The non-lazy/lazy</i></td></tr>
<tr><th id="1648">1648</th><td><i> * counts are supplied by rhcount and rhcount_lazy.</i></td></tr>
<tr><th id="1649">1649</th><td><i> *</i></td></tr>
<tr><th id="1650">1650</th><td><i> * If warranted, also wake up the kthread servicing this CPUs queues.</i></td></tr>
<tr><th id="1651">1651</th><td><i> */</i></td></tr>
<tr><th id="1652">1652</th><td><em>static</em> <em>void</em> __call_rcu_nocb_enqueue(<b>struct</b> rcu_data *rdp,</td></tr>
<tr><th id="1653">1653</th><td>				    <b>struct</b> rcu_head *rhp,</td></tr>
<tr><th id="1654">1654</th><td>				    <b>struct</b> rcu_head **rhtp,</td></tr>
<tr><th id="1655">1655</th><td>				    <em>int</em> rhcount, <em>int</em> rhcount_lazy,</td></tr>
<tr><th id="1656">1656</th><td>				    <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1657">1657</th><td>{</td></tr>
<tr><th id="1658">1658</th><td>	<em>int</em> len;</td></tr>
<tr><th id="1659">1659</th><td>	<b>struct</b> rcu_head **old_rhpp;</td></tr>
<tr><th id="1660">1660</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="1661">1661</th><td></td></tr>
<tr><th id="1662">1662</th><td>	<i>/* Enqueue the callback on the nocb list and update counts. */</i></td></tr>
<tr><th id="1663">1663</th><td>	atomic_long_add(rhcount, &amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="1664">1664</th><td>	<i>/* rcu_barrier() relies on -&gt;nocb_q_count add before xchg. */</i></td></tr>
<tr><th id="1665">1665</th><td>	old_rhpp = xchg(&amp;rdp-&gt;nocb_tail, rhtp);</td></tr>
<tr><th id="1666">1666</th><td>	WRITE_ONCE(*old_rhpp, rhp);</td></tr>
<tr><th id="1667">1667</th><td>	atomic_long_add(rhcount_lazy, &amp;rdp-&gt;nocb_q_count_lazy);</td></tr>
<tr><th id="1668">1668</th><td>	smp_mb__after_atomic(); <i>/* Store *old_rhpp before _wake test. */</i></td></tr>
<tr><th id="1669">1669</th><td></td></tr>
<tr><th id="1670">1670</th><td>	<i>/* If we are not being polled and there is a kthread, awaken it ... */</i></td></tr>
<tr><th id="1671">1671</th><td>	t = READ_ONCE(rdp-&gt;nocb_kthread);</td></tr>
<tr><th id="1672">1672</th><td>	<b>if</b> (rcu_nocb_poll || !t) {</td></tr>
<tr><th id="1673">1673</th><td>		trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu,</td></tr>
<tr><th id="1674">1674</th><td>				    TPS(<q>"WakeNotPoll"</q>));</td></tr>
<tr><th id="1675">1675</th><td>		<b>return</b>;</td></tr>
<tr><th id="1676">1676</th><td>	}</td></tr>
<tr><th id="1677">1677</th><td>	len = rcu_get_n_cbs_nocb_cpu(rdp);</td></tr>
<tr><th id="1678">1678</th><td>	<b>if</b> (old_rhpp == &amp;rdp-&gt;nocb_head) {</td></tr>
<tr><th id="1679">1679</th><td>		<b>if</b> (!irqs_disabled_flags(flags)) {</td></tr>
<tr><th id="1680">1680</th><td>			<i>/* ... if queue was empty ... */</i></td></tr>
<tr><th id="1681">1681</th><td>			wake_nocb_leader(rdp, false);</td></tr>
<tr><th id="1682">1682</th><td>			trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu,</td></tr>
<tr><th id="1683">1683</th><td>					    TPS(<q>"WakeEmpty"</q>));</td></tr>
<tr><th id="1684">1684</th><td>		} <b>else</b> {</td></tr>
<tr><th id="1685">1685</th><td>			wake_nocb_leader_defer(rdp, RCU_NOCB_WAKE,</td></tr>
<tr><th id="1686">1686</th><td>					       TPS(<q>"WakeEmptyIsDeferred"</q>));</td></tr>
<tr><th id="1687">1687</th><td>		}</td></tr>
<tr><th id="1688">1688</th><td>		rdp-&gt;qlen_last_fqs_check = <var>0</var>;</td></tr>
<tr><th id="1689">1689</th><td>	} <b>else</b> <b>if</b> (len &gt; rdp-&gt;qlen_last_fqs_check + qhimark) {</td></tr>
<tr><th id="1690">1690</th><td>		<i>/* ... or if many callbacks queued. */</i></td></tr>
<tr><th id="1691">1691</th><td>		<b>if</b> (!irqs_disabled_flags(flags)) {</td></tr>
<tr><th id="1692">1692</th><td>			wake_nocb_leader(rdp, true);</td></tr>
<tr><th id="1693">1693</th><td>			trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu,</td></tr>
<tr><th id="1694">1694</th><td>					    TPS(<q>"WakeOvf"</q>));</td></tr>
<tr><th id="1695">1695</th><td>		} <b>else</b> {</td></tr>
<tr><th id="1696">1696</th><td>			wake_nocb_leader_defer(rdp, RCU_NOCB_WAKE_FORCE,</td></tr>
<tr><th id="1697">1697</th><td>					       TPS(<q>"WakeOvfIsDeferred"</q>));</td></tr>
<tr><th id="1698">1698</th><td>		}</td></tr>
<tr><th id="1699">1699</th><td>		rdp-&gt;qlen_last_fqs_check = LONG_MAX / <var>2</var>;</td></tr>
<tr><th id="1700">1700</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1701">1701</th><td>		trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, TPS(<q>"WakeNot"</q>));</td></tr>
<tr><th id="1702">1702</th><td>	}</td></tr>
<tr><th id="1703">1703</th><td>	<b>return</b>;</td></tr>
<tr><th id="1704">1704</th><td>}</td></tr>
<tr><th id="1705">1705</th><td></td></tr>
<tr><th id="1706">1706</th><td><i>/*</i></td></tr>
<tr><th id="1707">1707</th><td><i> * This is a helper for __call_rcu(), which invokes this when the normal</i></td></tr>
<tr><th id="1708">1708</th><td><i> * callback queue is inoperable.  If this is not a no-CBs CPU, this</i></td></tr>
<tr><th id="1709">1709</th><td><i> * function returns failure back to __call_rcu(), which can complain</i></td></tr>
<tr><th id="1710">1710</th><td><i> * appropriately.</i></td></tr>
<tr><th id="1711">1711</th><td><i> *</i></td></tr>
<tr><th id="1712">1712</th><td><i> * Otherwise, this function queues the callback where the corresponding</i></td></tr>
<tr><th id="1713">1713</th><td><i> * "rcuo" kthread can find it.</i></td></tr>
<tr><th id="1714">1714</th><td><i> */</i></td></tr>
<tr><th id="1715">1715</th><td><em>static</em> bool __call_rcu_nocb(<b>struct</b> rcu_data *rdp, <b>struct</b> rcu_head *rhp,</td></tr>
<tr><th id="1716">1716</th><td>			    bool lazy, <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1717">1717</th><td>{</td></tr>
<tr><th id="1718">1718</th><td></td></tr>
<tr><th id="1719">1719</th><td>	<b>if</b> (!rcu_is_nocb_cpu(rdp-&gt;cpu))</td></tr>
<tr><th id="1720">1720</th><td>		<b>return</b> false;</td></tr>
<tr><th id="1721">1721</th><td>	__call_rcu_nocb_enqueue(rdp, rhp, &amp;rhp-&gt;next, <var>1</var>, lazy, flags);</td></tr>
<tr><th id="1722">1722</th><td>	<b>if</b> (__is_kfree_rcu_offset((<em>unsigned</em> <em>long</em>)rhp-&gt;func))</td></tr>
<tr><th id="1723">1723</th><td>		trace_rcu_kfree_callback(rcu_state.name, rhp,</td></tr>
<tr><th id="1724">1724</th><td>					 (<em>unsigned</em> <em>long</em>)rhp-&gt;func,</td></tr>
<tr><th id="1725">1725</th><td>					 -atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="1726">1726</th><td>					 -rcu_get_n_cbs_nocb_cpu(rdp));</td></tr>
<tr><th id="1727">1727</th><td>	<b>else</b></td></tr>
<tr><th id="1728">1728</th><td>		trace_rcu_callback(rcu_state.name, rhp,</td></tr>
<tr><th id="1729">1729</th><td>				   -atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="1730">1730</th><td>				   -rcu_get_n_cbs_nocb_cpu(rdp));</td></tr>
<tr><th id="1731">1731</th><td></td></tr>
<tr><th id="1732">1732</th><td>	<i>/*</i></td></tr>
<tr><th id="1733">1733</th><td><i>	 * If called from an extended quiescent state with interrupts</i></td></tr>
<tr><th id="1734">1734</th><td><i>	 * disabled, invoke the RCU core in order to allow the idle-entry</i></td></tr>
<tr><th id="1735">1735</th><td><i>	 * deferred-wakeup check to function.</i></td></tr>
<tr><th id="1736">1736</th><td><i>	 */</i></td></tr>
<tr><th id="1737">1737</th><td>	<b>if</b> (irqs_disabled_flags(flags) &amp;&amp;</td></tr>
<tr><th id="1738">1738</th><td>	    !rcu_is_watching() &amp;&amp;</td></tr>
<tr><th id="1739">1739</th><td>	    cpu_online(smp_processor_id()))</td></tr>
<tr><th id="1740">1740</th><td>		invoke_rcu_core();</td></tr>
<tr><th id="1741">1741</th><td></td></tr>
<tr><th id="1742">1742</th><td>	<b>return</b> true;</td></tr>
<tr><th id="1743">1743</th><td>}</td></tr>
<tr><th id="1744">1744</th><td></td></tr>
<tr><th id="1745">1745</th><td><i>/*</i></td></tr>
<tr><th id="1746">1746</th><td><i> * Adopt orphaned callbacks on a no-CBs CPU, or return 0 if this is</i></td></tr>
<tr><th id="1747">1747</th><td><i> * not a no-CBs CPU.</i></td></tr>
<tr><th id="1748">1748</th><td><i> */</i></td></tr>
<tr><th id="1749">1749</th><td><em>static</em> bool __maybe_unused rcu_nocb_adopt_orphan_cbs(<b>struct</b> rcu_data *my_rdp,</td></tr>
<tr><th id="1750">1750</th><td>						     <b>struct</b> rcu_data *rdp,</td></tr>
<tr><th id="1751">1751</th><td>						     <em>unsigned</em> <em>long</em> flags)</td></tr>
<tr><th id="1752">1752</th><td>{</td></tr>
<tr><th id="1753">1753</th><td>	lockdep_assert_irqs_disabled();</td></tr>
<tr><th id="1754">1754</th><td>	<b>if</b> (!rcu_is_nocb_cpu(smp_processor_id()))</td></tr>
<tr><th id="1755">1755</th><td>		<b>return</b> false; <i>/* Not NOCBs CPU, caller must migrate CBs. */</i></td></tr>
<tr><th id="1756">1756</th><td>	__call_rcu_nocb_enqueue(my_rdp, rcu_segcblist_head(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="1757">1757</th><td>				rcu_segcblist_tail(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="1758">1758</th><td>				rcu_segcblist_n_cbs(&amp;rdp-&gt;cblist),</td></tr>
<tr><th id="1759">1759</th><td>				rcu_segcblist_n_lazy_cbs(&amp;rdp-&gt;cblist), flags);</td></tr>
<tr><th id="1760">1760</th><td>	rcu_segcblist_init(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="1761">1761</th><td>	rcu_segcblist_disable(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="1762">1762</th><td>	<b>return</b> true;</td></tr>
<tr><th id="1763">1763</th><td>}</td></tr>
<tr><th id="1764">1764</th><td></td></tr>
<tr><th id="1765">1765</th><td><i>/*</i></td></tr>
<tr><th id="1766">1766</th><td><i> * If necessary, kick off a new grace period, and either way wait</i></td></tr>
<tr><th id="1767">1767</th><td><i> * for a subsequent grace period to complete.</i></td></tr>
<tr><th id="1768">1768</th><td><i> */</i></td></tr>
<tr><th id="1769">1769</th><td><em>static</em> <em>void</em> rcu_nocb_wait_gp(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="1770">1770</th><td>{</td></tr>
<tr><th id="1771">1771</th><td>	<em>unsigned</em> <em>long</em> c;</td></tr>
<tr><th id="1772">1772</th><td>	bool d;</td></tr>
<tr><th id="1773">1773</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1774">1774</th><td>	bool needwake;</td></tr>
<tr><th id="1775">1775</th><td>	<b>struct</b> rcu_node *rnp = rdp-&gt;mynode;</td></tr>
<tr><th id="1776">1776</th><td></td></tr>
<tr><th id="1777">1777</th><td>	local_irq_save(flags);</td></tr>
<tr><th id="1778">1778</th><td>	c = rcu_seq_snap(&amp;rcu_state.gp_seq);</td></tr>
<tr><th id="1779">1779</th><td>	<b>if</b> (!rdp-&gt;gpwrap &amp;&amp; ULONG_CMP_GE(rdp-&gt;gp_seq_needed, c)) {</td></tr>
<tr><th id="1780">1780</th><td>		local_irq_restore(flags);</td></tr>
<tr><th id="1781">1781</th><td>	} <b>else</b> {</td></tr>
<tr><th id="1782">1782</th><td>		raw_spin_lock_rcu_node(rnp); <i>/* irqs already disabled. */</i></td></tr>
<tr><th id="1783">1783</th><td>		needwake = rcu_start_this_gp(rnp, rdp, c);</td></tr>
<tr><th id="1784">1784</th><td>		raw_spin_unlock_irqrestore_rcu_node(rnp, flags);</td></tr>
<tr><th id="1785">1785</th><td>		<b>if</b> (needwake)</td></tr>
<tr><th id="1786">1786</th><td>			rcu_gp_kthread_wake();</td></tr>
<tr><th id="1787">1787</th><td>	}</td></tr>
<tr><th id="1788">1788</th><td></td></tr>
<tr><th id="1789">1789</th><td>	<i>/*</i></td></tr>
<tr><th id="1790">1790</th><td><i>	 * Wait for the grace period.  Do so interruptibly to avoid messing</i></td></tr>
<tr><th id="1791">1791</th><td><i>	 * up the load average.</i></td></tr>
<tr><th id="1792">1792</th><td><i>	 */</i></td></tr>
<tr><th id="1793">1793</th><td>	trace_rcu_this_gp(rnp, rdp, c, TPS(<q>"StartWait"</q>));</td></tr>
<tr><th id="1794">1794</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="1795">1795</th><td>		swait_event_interruptible_exclusive(</td></tr>
<tr><th id="1796">1796</th><td>			rnp-&gt;nocb_gp_wq[rcu_seq_ctr(c) &amp; <var>0x1</var>],</td></tr>
<tr><th id="1797">1797</th><td>			(d = rcu_seq_done(&amp;rnp-&gt;gp_seq, c)));</td></tr>
<tr><th id="1798">1798</th><td>		<b>if</b> (likely(d))</td></tr>
<tr><th id="1799">1799</th><td>			<b>break</b>;</td></tr>
<tr><th id="1800">1800</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="1801">1801</th><td>		trace_rcu_this_gp(rnp, rdp, c, TPS(<q>"ResumeWait"</q>));</td></tr>
<tr><th id="1802">1802</th><td>	}</td></tr>
<tr><th id="1803">1803</th><td>	trace_rcu_this_gp(rnp, rdp, c, TPS(<q>"EndWait"</q>));</td></tr>
<tr><th id="1804">1804</th><td>	smp_mb(); <i>/* Ensure that CB invocation happens after GP end. */</i></td></tr>
<tr><th id="1805">1805</th><td>}</td></tr>
<tr><th id="1806">1806</th><td></td></tr>
<tr><th id="1807">1807</th><td><i>/*</i></td></tr>
<tr><th id="1808">1808</th><td><i> * Leaders come here to wait for additional callbacks to show up.</i></td></tr>
<tr><th id="1809">1809</th><td><i> * This function does not return until callbacks appear.</i></td></tr>
<tr><th id="1810">1810</th><td><i> */</i></td></tr>
<tr><th id="1811">1811</th><td><em>static</em> <em>void</em> nocb_leader_wait(<b>struct</b> rcu_data *my_rdp)</td></tr>
<tr><th id="1812">1812</th><td>{</td></tr>
<tr><th id="1813">1813</th><td>	bool firsttime = true;</td></tr>
<tr><th id="1814">1814</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1815">1815</th><td>	bool gotcbs;</td></tr>
<tr><th id="1816">1816</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="1817">1817</th><td>	<b>struct</b> rcu_head **tail;</td></tr>
<tr><th id="1818">1818</th><td></td></tr>
<tr><th id="1819">1819</th><td>wait_again:</td></tr>
<tr><th id="1820">1820</th><td></td></tr>
<tr><th id="1821">1821</th><td>	<i>/* Wait for callbacks to appear. */</i></td></tr>
<tr><th id="1822">1822</th><td>	<b>if</b> (!rcu_nocb_poll) {</td></tr>
<tr><th id="1823">1823</th><td>		trace_rcu_nocb_wake(rcu_state.name, my_rdp-&gt;cpu, TPS(<q>"Sleep"</q>));</td></tr>
<tr><th id="1824">1824</th><td>		swait_event_interruptible_exclusive(my_rdp-&gt;nocb_wq,</td></tr>
<tr><th id="1825">1825</th><td>				!READ_ONCE(my_rdp-&gt;nocb_leader_sleep));</td></tr>
<tr><th id="1826">1826</th><td>		raw_spin_lock_irqsave(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1827">1827</th><td>		my_rdp-&gt;nocb_leader_sleep = true;</td></tr>
<tr><th id="1828">1828</th><td>		WRITE_ONCE(my_rdp-&gt;nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);</td></tr>
<tr><th id="1829">1829</th><td>		del_timer(&amp;my_rdp-&gt;nocb_timer);</td></tr>
<tr><th id="1830">1830</th><td>		raw_spin_unlock_irqrestore(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1831">1831</th><td>	} <b>else</b> <b>if</b> (firsttime) {</td></tr>
<tr><th id="1832">1832</th><td>		firsttime = false; <i>/* Don't drown trace log with "Poll"! */</i></td></tr>
<tr><th id="1833">1833</th><td>		trace_rcu_nocb_wake(rcu_state.name, my_rdp-&gt;cpu, TPS(<q>"Poll"</q>));</td></tr>
<tr><th id="1834">1834</th><td>	}</td></tr>
<tr><th id="1835">1835</th><td></td></tr>
<tr><th id="1836">1836</th><td>	<i>/*</i></td></tr>
<tr><th id="1837">1837</th><td><i>	 * Each pass through the following loop checks a follower for CBs.</i></td></tr>
<tr><th id="1838">1838</th><td><i>	 * We are our own first follower.  Any CBs found are moved to</i></td></tr>
<tr><th id="1839">1839</th><td><i>	 * nocb_gp_head, where they await a grace period.</i></td></tr>
<tr><th id="1840">1840</th><td><i>	 */</i></td></tr>
<tr><th id="1841">1841</th><td>	gotcbs = false;</td></tr>
<tr><th id="1842">1842</th><td>	smp_mb(); <i>/* wakeup and _sleep before -&gt;nocb_head reads. */</i></td></tr>
<tr><th id="1843">1843</th><td>	<b>for</b> (rdp = my_rdp; rdp; rdp = rdp-&gt;nocb_next_follower) {</td></tr>
<tr><th id="1844">1844</th><td>		rdp-&gt;nocb_gp_head = READ_ONCE(rdp-&gt;nocb_head);</td></tr>
<tr><th id="1845">1845</th><td>		<b>if</b> (!rdp-&gt;nocb_gp_head)</td></tr>
<tr><th id="1846">1846</th><td>			<b>continue</b>;  <i>/* No CBs here, try next follower. */</i></td></tr>
<tr><th id="1847">1847</th><td></td></tr>
<tr><th id="1848">1848</th><td>		<i>/* Move callbacks to wait-for-GP list, which is empty. */</i></td></tr>
<tr><th id="1849">1849</th><td>		WRITE_ONCE(rdp-&gt;nocb_head, NULL);</td></tr>
<tr><th id="1850">1850</th><td>		rdp-&gt;nocb_gp_tail = xchg(&amp;rdp-&gt;nocb_tail, &amp;rdp-&gt;nocb_head);</td></tr>
<tr><th id="1851">1851</th><td>		gotcbs = true;</td></tr>
<tr><th id="1852">1852</th><td>	}</td></tr>
<tr><th id="1853">1853</th><td></td></tr>
<tr><th id="1854">1854</th><td>	<i>/* No callbacks?  Sleep a bit if polling, and go retry.  */</i></td></tr>
<tr><th id="1855">1855</th><td>	<b>if</b> (unlikely(!gotcbs)) {</td></tr>
<tr><th id="1856">1856</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="1857">1857</th><td>		<b>if</b> (rcu_nocb_poll) {</td></tr>
<tr><th id="1858">1858</th><td>			schedule_timeout_interruptible(<var>1</var>);</td></tr>
<tr><th id="1859">1859</th><td>		} <b>else</b> {</td></tr>
<tr><th id="1860">1860</th><td>			trace_rcu_nocb_wake(rcu_state.name, my_rdp-&gt;cpu,</td></tr>
<tr><th id="1861">1861</th><td>					    TPS(<q>"WokeEmpty"</q>));</td></tr>
<tr><th id="1862">1862</th><td>		}</td></tr>
<tr><th id="1863">1863</th><td>		<b>goto</b> wait_again;</td></tr>
<tr><th id="1864">1864</th><td>	}</td></tr>
<tr><th id="1865">1865</th><td></td></tr>
<tr><th id="1866">1866</th><td>	<i>/* Wait for one grace period. */</i></td></tr>
<tr><th id="1867">1867</th><td>	rcu_nocb_wait_gp(my_rdp);</td></tr>
<tr><th id="1868">1868</th><td></td></tr>
<tr><th id="1869">1869</th><td>	<i>/* Each pass through the following loop wakes a follower, if needed. */</i></td></tr>
<tr><th id="1870">1870</th><td>	<b>for</b> (rdp = my_rdp; rdp; rdp = rdp-&gt;nocb_next_follower) {</td></tr>
<tr><th id="1871">1871</th><td>		<b>if</b> (!rcu_nocb_poll &amp;&amp;</td></tr>
<tr><th id="1872">1872</th><td>		    READ_ONCE(rdp-&gt;nocb_head) &amp;&amp;</td></tr>
<tr><th id="1873">1873</th><td>		    READ_ONCE(my_rdp-&gt;nocb_leader_sleep)) {</td></tr>
<tr><th id="1874">1874</th><td>			raw_spin_lock_irqsave(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1875">1875</th><td>			my_rdp-&gt;nocb_leader_sleep = false;<i>/* No need to sleep.*/</i></td></tr>
<tr><th id="1876">1876</th><td>			raw_spin_unlock_irqrestore(&amp;my_rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1877">1877</th><td>		}</td></tr>
<tr><th id="1878">1878</th><td>		<b>if</b> (!rdp-&gt;nocb_gp_head)</td></tr>
<tr><th id="1879">1879</th><td>			<b>continue</b>; <i>/* No CBs, so no need to wake follower. */</i></td></tr>
<tr><th id="1880">1880</th><td></td></tr>
<tr><th id="1881">1881</th><td>		<i>/* Append callbacks to follower's "done" list. */</i></td></tr>
<tr><th id="1882">1882</th><td>		raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1883">1883</th><td>		tail = rdp-&gt;nocb_follower_tail;</td></tr>
<tr><th id="1884">1884</th><td>		rdp-&gt;nocb_follower_tail = rdp-&gt;nocb_gp_tail;</td></tr>
<tr><th id="1885">1885</th><td>		*tail = rdp-&gt;nocb_gp_head;</td></tr>
<tr><th id="1886">1886</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1887">1887</th><td>		<b>if</b> (rdp != my_rdp &amp;&amp; tail == &amp;rdp-&gt;nocb_follower_head) {</td></tr>
<tr><th id="1888">1888</th><td>			<i>/* List was empty, so wake up the follower.  */</i></td></tr>
<tr><th id="1889">1889</th><td>			swake_up_one(&amp;rdp-&gt;nocb_wq);</td></tr>
<tr><th id="1890">1890</th><td>		}</td></tr>
<tr><th id="1891">1891</th><td>	}</td></tr>
<tr><th id="1892">1892</th><td></td></tr>
<tr><th id="1893">1893</th><td>	<i>/* If we (the leader) don't have CBs, go wait some more. */</i></td></tr>
<tr><th id="1894">1894</th><td>	<b>if</b> (!my_rdp-&gt;nocb_follower_head)</td></tr>
<tr><th id="1895">1895</th><td>		<b>goto</b> wait_again;</td></tr>
<tr><th id="1896">1896</th><td>}</td></tr>
<tr><th id="1897">1897</th><td></td></tr>
<tr><th id="1898">1898</th><td><i>/*</i></td></tr>
<tr><th id="1899">1899</th><td><i> * Followers come here to wait for additional callbacks to show up.</i></td></tr>
<tr><th id="1900">1900</th><td><i> * This function does not return until callbacks appear.</i></td></tr>
<tr><th id="1901">1901</th><td><i> */</i></td></tr>
<tr><th id="1902">1902</th><td><em>static</em> <em>void</em> nocb_follower_wait(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="1903">1903</th><td>{</td></tr>
<tr><th id="1904">1904</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="1905">1905</th><td>		trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, TPS(<q>"FollowerSleep"</q>));</td></tr>
<tr><th id="1906">1906</th><td>		swait_event_interruptible_exclusive(rdp-&gt;nocb_wq,</td></tr>
<tr><th id="1907">1907</th><td>					 READ_ONCE(rdp-&gt;nocb_follower_head));</td></tr>
<tr><th id="1908">1908</th><td>		<b>if</b> (smp_load_acquire(&amp;rdp-&gt;nocb_follower_head)) {</td></tr>
<tr><th id="1909">1909</th><td>			<i>/* ^^^ Ensure CB invocation follows _head test. */</i></td></tr>
<tr><th id="1910">1910</th><td>			<b>return</b>;</td></tr>
<tr><th id="1911">1911</th><td>		}</td></tr>
<tr><th id="1912">1912</th><td>		WARN_ON(signal_pending(current));</td></tr>
<tr><th id="1913">1913</th><td>		trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, TPS(<q>"WokeEmpty"</q>));</td></tr>
<tr><th id="1914">1914</th><td>	}</td></tr>
<tr><th id="1915">1915</th><td>}</td></tr>
<tr><th id="1916">1916</th><td></td></tr>
<tr><th id="1917">1917</th><td><i>/*</i></td></tr>
<tr><th id="1918">1918</th><td><i> * Per-rcu_data kthread, but only for no-CBs CPUs.  Each kthread invokes</i></td></tr>
<tr><th id="1919">1919</th><td><i> * callbacks queued by the corresponding no-CBs CPU, however, there is</i></td></tr>
<tr><th id="1920">1920</th><td><i> * an optional leader-follower relationship so that the grace-period</i></td></tr>
<tr><th id="1921">1921</th><td><i> * kthreads don't have to do quite so many wakeups.</i></td></tr>
<tr><th id="1922">1922</th><td><i> */</i></td></tr>
<tr><th id="1923">1923</th><td><em>static</em> <em>int</em> rcu_nocb_kthread(<em>void</em> *arg)</td></tr>
<tr><th id="1924">1924</th><td>{</td></tr>
<tr><th id="1925">1925</th><td>	<em>int</em> c, cl;</td></tr>
<tr><th id="1926">1926</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1927">1927</th><td>	<b>struct</b> rcu_head *list;</td></tr>
<tr><th id="1928">1928</th><td>	<b>struct</b> rcu_head *next;</td></tr>
<tr><th id="1929">1929</th><td>	<b>struct</b> rcu_head **tail;</td></tr>
<tr><th id="1930">1930</th><td>	<b>struct</b> rcu_data *rdp = arg;</td></tr>
<tr><th id="1931">1931</th><td></td></tr>
<tr><th id="1932">1932</th><td>	<i>/* Each pass through this loop invokes one batch of callbacks */</i></td></tr>
<tr><th id="1933">1933</th><td>	<b>for</b> (;;) {</td></tr>
<tr><th id="1934">1934</th><td>		<i>/* Wait for callbacks. */</i></td></tr>
<tr><th id="1935">1935</th><td>		<b>if</b> (rdp-&gt;nocb_leader == rdp)</td></tr>
<tr><th id="1936">1936</th><td>			nocb_leader_wait(rdp);</td></tr>
<tr><th id="1937">1937</th><td>		<b>else</b></td></tr>
<tr><th id="1938">1938</th><td>			nocb_follower_wait(rdp);</td></tr>
<tr><th id="1939">1939</th><td></td></tr>
<tr><th id="1940">1940</th><td>		<i>/* Pull the ready-to-invoke callbacks onto local list. */</i></td></tr>
<tr><th id="1941">1941</th><td>		raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1942">1942</th><td>		list = rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="1943">1943</th><td>		rdp-&gt;nocb_follower_head = NULL;</td></tr>
<tr><th id="1944">1944</th><td>		tail = rdp-&gt;nocb_follower_tail;</td></tr>
<tr><th id="1945">1945</th><td>		rdp-&gt;nocb_follower_tail = &amp;rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="1946">1946</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1947">1947</th><td>		<b>if</b> (WARN_ON_ONCE(!list))</td></tr>
<tr><th id="1948">1948</th><td>			<b>continue</b>;</td></tr>
<tr><th id="1949">1949</th><td>		trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, TPS(<q>"WokeNonEmpty"</q>));</td></tr>
<tr><th id="1950">1950</th><td></td></tr>
<tr><th id="1951">1951</th><td>		<i>/* Each pass through the following loop invokes a callback. */</i></td></tr>
<tr><th id="1952">1952</th><td>		trace_rcu_batch_start(rcu_state.name,</td></tr>
<tr><th id="1953">1953</th><td>				      atomic_long_read(&amp;rdp-&gt;nocb_q_count_lazy),</td></tr>
<tr><th id="1954">1954</th><td>				      rcu_get_n_cbs_nocb_cpu(rdp), -<var>1</var>);</td></tr>
<tr><th id="1955">1955</th><td>		c = cl = <var>0</var>;</td></tr>
<tr><th id="1956">1956</th><td>		<b>while</b> (list) {</td></tr>
<tr><th id="1957">1957</th><td>			next = list-&gt;next;</td></tr>
<tr><th id="1958">1958</th><td>			<i>/* Wait for enqueuing to complete, if needed. */</i></td></tr>
<tr><th id="1959">1959</th><td>			<b>while</b> (next == NULL &amp;&amp; &amp;list-&gt;next != tail) {</td></tr>
<tr><th id="1960">1960</th><td>				trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu,</td></tr>
<tr><th id="1961">1961</th><td>						    TPS(<q>"WaitQueue"</q>));</td></tr>
<tr><th id="1962">1962</th><td>				schedule_timeout_interruptible(<var>1</var>);</td></tr>
<tr><th id="1963">1963</th><td>				trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu,</td></tr>
<tr><th id="1964">1964</th><td>						    TPS(<q>"WokeQueue"</q>));</td></tr>
<tr><th id="1965">1965</th><td>				next = list-&gt;next;</td></tr>
<tr><th id="1966">1966</th><td>			}</td></tr>
<tr><th id="1967">1967</th><td>			debug_rcu_head_unqueue(list);</td></tr>
<tr><th id="1968">1968</th><td>			local_bh_disable();</td></tr>
<tr><th id="1969">1969</th><td>			<b>if</b> (__rcu_reclaim(rcu_state.name, list))</td></tr>
<tr><th id="1970">1970</th><td>				cl++;</td></tr>
<tr><th id="1971">1971</th><td>			c++;</td></tr>
<tr><th id="1972">1972</th><td>			local_bh_enable();</td></tr>
<tr><th id="1973">1973</th><td>			cond_resched_tasks_rcu_qs();</td></tr>
<tr><th id="1974">1974</th><td>			list = next;</td></tr>
<tr><th id="1975">1975</th><td>		}</td></tr>
<tr><th id="1976">1976</th><td>		trace_rcu_batch_end(rcu_state.name, c, !!list, <var>0</var>, <var>0</var>, <var>1</var>);</td></tr>
<tr><th id="1977">1977</th><td>		smp_mb__before_atomic();  <i>/* _add after CB invocation. */</i></td></tr>
<tr><th id="1978">1978</th><td>		atomic_long_add(-c, &amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="1979">1979</th><td>		atomic_long_add(-cl, &amp;rdp-&gt;nocb_q_count_lazy);</td></tr>
<tr><th id="1980">1980</th><td>	}</td></tr>
<tr><th id="1981">1981</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="1982">1982</th><td>}</td></tr>
<tr><th id="1983">1983</th><td></td></tr>
<tr><th id="1984">1984</th><td><i>/* Is a deferred wakeup of rcu_nocb_kthread() required? */</i></td></tr>
<tr><th id="1985">1985</th><td><em>static</em> <em>int</em> rcu_nocb_need_deferred_wakeup(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="1986">1986</th><td>{</td></tr>
<tr><th id="1987">1987</th><td>	<b>return</b> READ_ONCE(rdp-&gt;nocb_defer_wakeup);</td></tr>
<tr><th id="1988">1988</th><td>}</td></tr>
<tr><th id="1989">1989</th><td></td></tr>
<tr><th id="1990">1990</th><td><i>/* Do a deferred wakeup of rcu_nocb_kthread(). */</i></td></tr>
<tr><th id="1991">1991</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup_common(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="1992">1992</th><td>{</td></tr>
<tr><th id="1993">1993</th><td>	<em>unsigned</em> <em>long</em> flags;</td></tr>
<tr><th id="1994">1994</th><td>	<em>int</em> ndw;</td></tr>
<tr><th id="1995">1995</th><td></td></tr>
<tr><th id="1996">1996</th><td>	raw_spin_lock_irqsave(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1997">1997</th><td>	<b>if</b> (!rcu_nocb_need_deferred_wakeup(rdp)) {</td></tr>
<tr><th id="1998">1998</th><td>		raw_spin_unlock_irqrestore(&amp;rdp-&gt;nocb_lock, flags);</td></tr>
<tr><th id="1999">1999</th><td>		<b>return</b>;</td></tr>
<tr><th id="2000">2000</th><td>	}</td></tr>
<tr><th id="2001">2001</th><td>	ndw = READ_ONCE(rdp-&gt;nocb_defer_wakeup);</td></tr>
<tr><th id="2002">2002</th><td>	WRITE_ONCE(rdp-&gt;nocb_defer_wakeup, RCU_NOCB_WAKE_NOT);</td></tr>
<tr><th id="2003">2003</th><td>	__wake_nocb_leader(rdp, ndw == RCU_NOCB_WAKE_FORCE, flags);</td></tr>
<tr><th id="2004">2004</th><td>	trace_rcu_nocb_wake(rcu_state.name, rdp-&gt;cpu, TPS(<q>"DeferredWake"</q>));</td></tr>
<tr><th id="2005">2005</th><td>}</td></tr>
<tr><th id="2006">2006</th><td></td></tr>
<tr><th id="2007">2007</th><td><i>/* Do a deferred wakeup of rcu_nocb_kthread() from a timer handler. */</i></td></tr>
<tr><th id="2008">2008</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup_timer(<b>struct</b> timer_list *t)</td></tr>
<tr><th id="2009">2009</th><td>{</td></tr>
<tr><th id="2010">2010</th><td>	<b>struct</b> rcu_data *rdp = from_timer(rdp, t, nocb_timer);</td></tr>
<tr><th id="2011">2011</th><td></td></tr>
<tr><th id="2012">2012</th><td>	do_nocb_deferred_wakeup_common(rdp);</td></tr>
<tr><th id="2013">2013</th><td>}</td></tr>
<tr><th id="2014">2014</th><td></td></tr>
<tr><th id="2015">2015</th><td><i>/*</i></td></tr>
<tr><th id="2016">2016</th><td><i> * Do a deferred wakeup of rcu_nocb_kthread() from fastpath.</i></td></tr>
<tr><th id="2017">2017</th><td><i> * This means we do an inexact common-case check.  Note that if</i></td></tr>
<tr><th id="2018">2018</th><td><i> * we miss, -&gt;nocb_timer will eventually clean things up.</i></td></tr>
<tr><th id="2019">2019</th><td><i> */</i></td></tr>
<tr><th id="2020">2020</th><td><em>static</em> <em>void</em> do_nocb_deferred_wakeup(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2021">2021</th><td>{</td></tr>
<tr><th id="2022">2022</th><td>	<b>if</b> (rcu_nocb_need_deferred_wakeup(rdp))</td></tr>
<tr><th id="2023">2023</th><td>		do_nocb_deferred_wakeup_common(rdp);</td></tr>
<tr><th id="2024">2024</th><td>}</td></tr>
<tr><th id="2025">2025</th><td></td></tr>
<tr><th id="2026">2026</th><td><em>void</em> __init rcu_init_nohz(<em>void</em>)</td></tr>
<tr><th id="2027">2027</th><td>{</td></tr>
<tr><th id="2028">2028</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2029">2029</th><td>	bool need_rcu_nocb_mask = false;</td></tr>
<tr><th id="2030">2030</th><td></td></tr>
<tr><th id="2031">2031</th><td><u>#if defined(CONFIG_NO_HZ_FULL)</u></td></tr>
<tr><th id="2032">2032</th><td>	<b>if</b> (tick_nohz_full_running &amp;&amp; cpumask_weight(tick_nohz_full_mask))</td></tr>
<tr><th id="2033">2033</th><td>		need_rcu_nocb_mask = true;</td></tr>
<tr><th id="2034">2034</th><td><u>#endif /* #if defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2035">2035</th><td></td></tr>
<tr><th id="2036">2036</th><td>	<b>if</b> (!cpumask_available(rcu_nocb_mask) &amp;&amp; need_rcu_nocb_mask) {</td></tr>
<tr><th id="2037">2037</th><td>		<b>if</b> (!zalloc_cpumask_var(&amp;rcu_nocb_mask, GFP_KERNEL)) {</td></tr>
<tr><th id="2038">2038</th><td>			pr_info(<q>"rcu_nocb_mask allocation failed, callback offloading disabled.\n"</q>);</td></tr>
<tr><th id="2039">2039</th><td>			<b>return</b>;</td></tr>
<tr><th id="2040">2040</th><td>		}</td></tr>
<tr><th id="2041">2041</th><td>	}</td></tr>
<tr><th id="2042">2042</th><td>	<b>if</b> (!cpumask_available(rcu_nocb_mask))</td></tr>
<tr><th id="2043">2043</th><td>		<b>return</b>;</td></tr>
<tr><th id="2044">2044</th><td></td></tr>
<tr><th id="2045">2045</th><td><u>#if defined(CONFIG_NO_HZ_FULL)</u></td></tr>
<tr><th id="2046">2046</th><td>	<b>if</b> (tick_nohz_full_running)</td></tr>
<tr><th id="2047">2047</th><td>		cpumask_or(rcu_nocb_mask, rcu_nocb_mask, tick_nohz_full_mask);</td></tr>
<tr><th id="2048">2048</th><td><u>#endif /* #if defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2049">2049</th><td></td></tr>
<tr><th id="2050">2050</th><td>	<b>if</b> (!cpumask_subset(rcu_nocb_mask, cpu_possible_mask)) {</td></tr>
<tr><th id="2051">2051</th><td>		pr_info(<q>"\tNote: kernel parameter 'rcu_nocbs=', 'nohz_full', or 'isolcpus=' contains nonexistent CPUs.\n"</q>);</td></tr>
<tr><th id="2052">2052</th><td>		cpumask_and(rcu_nocb_mask, cpu_possible_mask,</td></tr>
<tr><th id="2053">2053</th><td>			    rcu_nocb_mask);</td></tr>
<tr><th id="2054">2054</th><td>	}</td></tr>
<tr><th id="2055">2055</th><td>	<b>if</b> (cpumask_empty(rcu_nocb_mask))</td></tr>
<tr><th id="2056">2056</th><td>		pr_info(<q>"\tOffload RCU callbacks from CPUs: (none).\n"</q>);</td></tr>
<tr><th id="2057">2057</th><td>	<b>else</b></td></tr>
<tr><th id="2058">2058</th><td>		pr_info(<q>"\tOffload RCU callbacks from CPUs: %*pbl.\n"</q>,</td></tr>
<tr><th id="2059">2059</th><td>			cpumask_pr_args(rcu_nocb_mask));</td></tr>
<tr><th id="2060">2060</th><td>	<b>if</b> (rcu_nocb_poll)</td></tr>
<tr><th id="2061">2061</th><td>		pr_info(<q>"\tPoll for callbacks from no-CBs CPUs.\n"</q>);</td></tr>
<tr><th id="2062">2062</th><td></td></tr>
<tr><th id="2063">2063</th><td>	for_each_cpu(cpu, rcu_nocb_mask)</td></tr>
<tr><th id="2064">2064</th><td>		init_nocb_callback_list(per_cpu_ptr(&amp;rcu_data, cpu));</td></tr>
<tr><th id="2065">2065</th><td>	rcu_organize_nocb_kthreads();</td></tr>
<tr><th id="2066">2066</th><td>}</td></tr>
<tr><th id="2067">2067</th><td></td></tr>
<tr><th id="2068">2068</th><td><i>/* Initialize per-rcu_data variables for no-CBs CPUs. */</i></td></tr>
<tr><th id="2069">2069</th><td><em>static</em> <em>void</em> __init rcu_boot_init_nocb_percpu_data(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2070">2070</th><td>{</td></tr>
<tr><th id="2071">2071</th><td>	rdp-&gt;nocb_tail = &amp;rdp-&gt;nocb_head;</td></tr>
<tr><th id="2072">2072</th><td>	init_swait_queue_head(&amp;rdp-&gt;nocb_wq);</td></tr>
<tr><th id="2073">2073</th><td>	rdp-&gt;nocb_follower_tail = &amp;rdp-&gt;nocb_follower_head;</td></tr>
<tr><th id="2074">2074</th><td>	raw_spin_lock_init(&amp;rdp-&gt;nocb_lock);</td></tr>
<tr><th id="2075">2075</th><td>	timer_setup(&amp;rdp-&gt;nocb_timer, do_nocb_deferred_wakeup_timer, <var>0</var>);</td></tr>
<tr><th id="2076">2076</th><td>}</td></tr>
<tr><th id="2077">2077</th><td></td></tr>
<tr><th id="2078">2078</th><td><i>/*</i></td></tr>
<tr><th id="2079">2079</th><td><i> * If the specified CPU is a no-CBs CPU that does not already have its</i></td></tr>
<tr><th id="2080">2080</th><td><i> * rcuo kthread, spawn it.  If the CPUs are brought online out of order,</i></td></tr>
<tr><th id="2081">2081</th><td><i> * this can require re-organizing the leader-follower relationships.</i></td></tr>
<tr><th id="2082">2082</th><td><i> */</i></td></tr>
<tr><th id="2083">2083</th><td><em>static</em> <em>void</em> rcu_spawn_one_nocb_kthread(<em>int</em> cpu)</td></tr>
<tr><th id="2084">2084</th><td>{</td></tr>
<tr><th id="2085">2085</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="2086">2086</th><td>	<b>struct</b> rcu_data *rdp_last;</td></tr>
<tr><th id="2087">2087</th><td>	<b>struct</b> rcu_data *rdp_old_leader;</td></tr>
<tr><th id="2088">2088</th><td>	<b>struct</b> rcu_data *rdp_spawn = per_cpu_ptr(&amp;rcu_data, cpu);</td></tr>
<tr><th id="2089">2089</th><td>	<b>struct</b> task_struct *t;</td></tr>
<tr><th id="2090">2090</th><td></td></tr>
<tr><th id="2091">2091</th><td>	<i>/*</i></td></tr>
<tr><th id="2092">2092</th><td><i>	 * If this isn't a no-CBs CPU or if it already has an rcuo kthread,</i></td></tr>
<tr><th id="2093">2093</th><td><i>	 * then nothing to do.</i></td></tr>
<tr><th id="2094">2094</th><td><i>	 */</i></td></tr>
<tr><th id="2095">2095</th><td>	<b>if</b> (!rcu_is_nocb_cpu(cpu) || rdp_spawn-&gt;nocb_kthread)</td></tr>
<tr><th id="2096">2096</th><td>		<b>return</b>;</td></tr>
<tr><th id="2097">2097</th><td></td></tr>
<tr><th id="2098">2098</th><td>	<i>/* If we didn't spawn the leader first, reorganize! */</i></td></tr>
<tr><th id="2099">2099</th><td>	rdp_old_leader = rdp_spawn-&gt;nocb_leader;</td></tr>
<tr><th id="2100">2100</th><td>	<b>if</b> (rdp_old_leader != rdp_spawn &amp;&amp; !rdp_old_leader-&gt;nocb_kthread) {</td></tr>
<tr><th id="2101">2101</th><td>		rdp_last = NULL;</td></tr>
<tr><th id="2102">2102</th><td>		rdp = rdp_old_leader;</td></tr>
<tr><th id="2103">2103</th><td>		<b>do</b> {</td></tr>
<tr><th id="2104">2104</th><td>			rdp-&gt;nocb_leader = rdp_spawn;</td></tr>
<tr><th id="2105">2105</th><td>			<b>if</b> (rdp_last &amp;&amp; rdp != rdp_spawn)</td></tr>
<tr><th id="2106">2106</th><td>				rdp_last-&gt;nocb_next_follower = rdp;</td></tr>
<tr><th id="2107">2107</th><td>			<b>if</b> (rdp == rdp_spawn) {</td></tr>
<tr><th id="2108">2108</th><td>				rdp = rdp-&gt;nocb_next_follower;</td></tr>
<tr><th id="2109">2109</th><td>			} <b>else</b> {</td></tr>
<tr><th id="2110">2110</th><td>				rdp_last = rdp;</td></tr>
<tr><th id="2111">2111</th><td>				rdp = rdp-&gt;nocb_next_follower;</td></tr>
<tr><th id="2112">2112</th><td>				rdp_last-&gt;nocb_next_follower = NULL;</td></tr>
<tr><th id="2113">2113</th><td>			}</td></tr>
<tr><th id="2114">2114</th><td>		} <b>while</b> (rdp);</td></tr>
<tr><th id="2115">2115</th><td>		rdp_spawn-&gt;nocb_next_follower = rdp_old_leader;</td></tr>
<tr><th id="2116">2116</th><td>	}</td></tr>
<tr><th id="2117">2117</th><td></td></tr>
<tr><th id="2118">2118</th><td>	<i>/* Spawn the kthread for this CPU. */</i></td></tr>
<tr><th id="2119">2119</th><td>	t = kthread_run(rcu_nocb_kthread, rdp_spawn,</td></tr>
<tr><th id="2120">2120</th><td>			<q>"rcuo%c/%d"</q>, rcu_state.abbr, cpu);</td></tr>
<tr><th id="2121">2121</th><td>	<b>if</b> (WARN_ONCE(IS_ERR(t), <q>"%s: Could not start rcuo kthread, OOM is now expected behavior\n"</q>, <b>__func__</b>))</td></tr>
<tr><th id="2122">2122</th><td>		<b>return</b>;</td></tr>
<tr><th id="2123">2123</th><td>	WRITE_ONCE(rdp_spawn-&gt;nocb_kthread, t);</td></tr>
<tr><th id="2124">2124</th><td>}</td></tr>
<tr><th id="2125">2125</th><td></td></tr>
<tr><th id="2126">2126</th><td><i>/*</i></td></tr>
<tr><th id="2127">2127</th><td><i> * If the specified CPU is a no-CBs CPU that does not already have its</i></td></tr>
<tr><th id="2128">2128</th><td><i> * rcuo kthread, spawn it.</i></td></tr>
<tr><th id="2129">2129</th><td><i> */</i></td></tr>
<tr><th id="2130">2130</th><td><em>static</em> <em>void</em> rcu_spawn_cpu_nocb_kthread(<em>int</em> cpu)</td></tr>
<tr><th id="2131">2131</th><td>{</td></tr>
<tr><th id="2132">2132</th><td>	<b>if</b> (rcu_scheduler_fully_active)</td></tr>
<tr><th id="2133">2133</th><td>		rcu_spawn_one_nocb_kthread(cpu);</td></tr>
<tr><th id="2134">2134</th><td>}</td></tr>
<tr><th id="2135">2135</th><td></td></tr>
<tr><th id="2136">2136</th><td><i>/*</i></td></tr>
<tr><th id="2137">2137</th><td><i> * Once the scheduler is running, spawn rcuo kthreads for all online</i></td></tr>
<tr><th id="2138">2138</th><td><i> * no-CBs CPUs.  This assumes that the early_initcall()s happen before</i></td></tr>
<tr><th id="2139">2139</th><td><i> * non-boot CPUs come online -- if this changes, we will need to add</i></td></tr>
<tr><th id="2140">2140</th><td><i> * some mutual exclusion.</i></td></tr>
<tr><th id="2141">2141</th><td><i> */</i></td></tr>
<tr><th id="2142">2142</th><td><em>static</em> <em>void</em> __init rcu_spawn_nocb_kthreads(<em>void</em>)</td></tr>
<tr><th id="2143">2143</th><td>{</td></tr>
<tr><th id="2144">2144</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2145">2145</th><td></td></tr>
<tr><th id="2146">2146</th><td>	for_each_online_cpu(cpu)</td></tr>
<tr><th id="2147">2147</th><td>		rcu_spawn_cpu_nocb_kthread(cpu);</td></tr>
<tr><th id="2148">2148</th><td>}</td></tr>
<tr><th id="2149">2149</th><td></td></tr>
<tr><th id="2150">2150</th><td><i>/* How many follower CPU IDs per leader?  Default of -1 for sqrt(nr_cpu_ids). */</i></td></tr>
<tr><th id="2151">2151</th><td><em>static</em> <em>int</em> rcu_nocb_leader_stride = -<var>1</var>;</td></tr>
<tr><th id="2152">2152</th><td>module_param(rcu_nocb_leader_stride, <em>int</em>, <var>0444</var>);</td></tr>
<tr><th id="2153">2153</th><td></td></tr>
<tr><th id="2154">2154</th><td><i>/*</i></td></tr>
<tr><th id="2155">2155</th><td><i> * Initialize leader-follower relationships for all no-CBs CPU.</i></td></tr>
<tr><th id="2156">2156</th><td><i> */</i></td></tr>
<tr><th id="2157">2157</th><td><em>static</em> <em>void</em> __init rcu_organize_nocb_kthreads(<em>void</em>)</td></tr>
<tr><th id="2158">2158</th><td>{</td></tr>
<tr><th id="2159">2159</th><td>	<em>int</em> cpu;</td></tr>
<tr><th id="2160">2160</th><td>	<em>int</em> ls = rcu_nocb_leader_stride;</td></tr>
<tr><th id="2161">2161</th><td>	<em>int</em> nl = <var>0</var>;  <i>/* Next leader. */</i></td></tr>
<tr><th id="2162">2162</th><td>	<b>struct</b> rcu_data *rdp;</td></tr>
<tr><th id="2163">2163</th><td>	<b>struct</b> rcu_data *rdp_leader = NULL;  <i>/* Suppress misguided gcc warn. */</i></td></tr>
<tr><th id="2164">2164</th><td>	<b>struct</b> rcu_data *rdp_prev = NULL;</td></tr>
<tr><th id="2165">2165</th><td></td></tr>
<tr><th id="2166">2166</th><td>	<b>if</b> (!cpumask_available(rcu_nocb_mask))</td></tr>
<tr><th id="2167">2167</th><td>		<b>return</b>;</td></tr>
<tr><th id="2168">2168</th><td>	<b>if</b> (ls == -<var>1</var>) {</td></tr>
<tr><th id="2169">2169</th><td>		ls = int_sqrt(nr_cpu_ids);</td></tr>
<tr><th id="2170">2170</th><td>		rcu_nocb_leader_stride = ls;</td></tr>
<tr><th id="2171">2171</th><td>	}</td></tr>
<tr><th id="2172">2172</th><td></td></tr>
<tr><th id="2173">2173</th><td>	<i>/*</i></td></tr>
<tr><th id="2174">2174</th><td><i>	 * Each pass through this loop sets up one rcu_data structure.</i></td></tr>
<tr><th id="2175">2175</th><td><i>	 * Should the corresponding CPU come online in the future, then</i></td></tr>
<tr><th id="2176">2176</th><td><i>	 * we will spawn the needed set of rcu_nocb_kthread() kthreads.</i></td></tr>
<tr><th id="2177">2177</th><td><i>	 */</i></td></tr>
<tr><th id="2178">2178</th><td>	for_each_cpu(cpu, rcu_nocb_mask) {</td></tr>
<tr><th id="2179">2179</th><td>		rdp = per_cpu_ptr(&amp;rcu_data, cpu);</td></tr>
<tr><th id="2180">2180</th><td>		<b>if</b> (rdp-&gt;cpu &gt;= nl) {</td></tr>
<tr><th id="2181">2181</th><td>			<i>/* New leader, set up for followers &amp; next leader. */</i></td></tr>
<tr><th id="2182">2182</th><td>			nl = DIV_ROUND_UP(rdp-&gt;cpu + <var>1</var>, ls) * ls;</td></tr>
<tr><th id="2183">2183</th><td>			rdp-&gt;nocb_leader = rdp;</td></tr>
<tr><th id="2184">2184</th><td>			rdp_leader = rdp;</td></tr>
<tr><th id="2185">2185</th><td>		} <b>else</b> {</td></tr>
<tr><th id="2186">2186</th><td>			<i>/* Another follower, link to previous leader. */</i></td></tr>
<tr><th id="2187">2187</th><td>			rdp-&gt;nocb_leader = rdp_leader;</td></tr>
<tr><th id="2188">2188</th><td>			rdp_prev-&gt;nocb_next_follower = rdp;</td></tr>
<tr><th id="2189">2189</th><td>		}</td></tr>
<tr><th id="2190">2190</th><td>		rdp_prev = rdp;</td></tr>
<tr><th id="2191">2191</th><td>	}</td></tr>
<tr><th id="2192">2192</th><td>}</td></tr>
<tr><th id="2193">2193</th><td></td></tr>
<tr><th id="2194">2194</th><td><i>/* Prevent __call_rcu() from enqueuing callbacks on no-CBs CPUs */</i></td></tr>
<tr><th id="2195">2195</th><td><em>static</em> bool init_nocb_callback_list(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2196">2196</th><td>{</td></tr>
<tr><th id="2197">2197</th><td>	<b>if</b> (!rcu_is_nocb_cpu(rdp-&gt;cpu))</td></tr>
<tr><th id="2198">2198</th><td>		<b>return</b> false;</td></tr>
<tr><th id="2199">2199</th><td></td></tr>
<tr><th id="2200">2200</th><td>	<i>/* If there are early-boot callbacks, move them to nocb lists. */</i></td></tr>
<tr><th id="2201">2201</th><td>	<b>if</b> (!rcu_segcblist_empty(&amp;rdp-&gt;cblist)) {</td></tr>
<tr><th id="2202">2202</th><td>		rdp-&gt;nocb_head = rcu_segcblist_head(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2203">2203</th><td>		rdp-&gt;nocb_tail = rcu_segcblist_tail(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2204">2204</th><td>		atomic_long_set(&amp;rdp-&gt;nocb_q_count,</td></tr>
<tr><th id="2205">2205</th><td>				rcu_segcblist_n_cbs(&amp;rdp-&gt;cblist));</td></tr>
<tr><th id="2206">2206</th><td>		atomic_long_set(&amp;rdp-&gt;nocb_q_count_lazy,</td></tr>
<tr><th id="2207">2207</th><td>				rcu_segcblist_n_lazy_cbs(&amp;rdp-&gt;cblist));</td></tr>
<tr><th id="2208">2208</th><td>		rcu_segcblist_init(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2209">2209</th><td>	}</td></tr>
<tr><th id="2210">2210</th><td>	rcu_segcblist_disable(&amp;rdp-&gt;cblist);</td></tr>
<tr><th id="2211">2211</th><td>	<b>return</b> true;</td></tr>
<tr><th id="2212">2212</th><td>}</td></tr>
<tr><th id="2213">2213</th><td></td></tr>
<tr><th id="2214">2214</th><td><i>/*</i></td></tr>
<tr><th id="2215">2215</th><td><i> * Bind the current task to the offloaded CPUs.  If there are no offloaded</i></td></tr>
<tr><th id="2216">2216</th><td><i> * CPUs, leave the task unbound.  Splat if the bind attempt fails.</i></td></tr>
<tr><th id="2217">2217</th><td><i> */</i></td></tr>
<tr><th id="2218">2218</th><td><em>void</em> rcu_bind_current_to_nocb(<em>void</em>)</td></tr>
<tr><th id="2219">2219</th><td>{</td></tr>
<tr><th id="2220">2220</th><td>	<b>if</b> (cpumask_available(rcu_nocb_mask) &amp;&amp; cpumask_weight(rcu_nocb_mask))</td></tr>
<tr><th id="2221">2221</th><td>		WARN_ON(sched_setaffinity(current-&gt;pid, rcu_nocb_mask));</td></tr>
<tr><th id="2222">2222</th><td>}</td></tr>
<tr><th id="2223">2223</th><td>EXPORT_SYMBOL_GPL(rcu_bind_current_to_nocb);</td></tr>
<tr><th id="2224">2224</th><td></td></tr>
<tr><th id="2225">2225</th><td><i>/*</i></td></tr>
<tr><th id="2226">2226</th><td><i> * Return the number of RCU callbacks still queued from the specified</i></td></tr>
<tr><th id="2227">2227</th><td><i> * CPU, which must be a nocbs CPU.</i></td></tr>
<tr><th id="2228">2228</th><td><i> */</i></td></tr>
<tr><th id="2229">2229</th><td><em>static</em> <em>unsigned</em> <em>long</em> rcu_get_n_cbs_nocb_cpu(<b>struct</b> rcu_data *rdp)</td></tr>
<tr><th id="2230">2230</th><td>{</td></tr>
<tr><th id="2231">2231</th><td>	<b>return</b> atomic_long_read(&amp;rdp-&gt;nocb_q_count);</td></tr>
<tr><th id="2232">2232</th><td>}</td></tr>
<tr><th id="2233">2233</th><td></td></tr>
<tr><th id="2234">2234</th><td><u>#<span data-ppcond="1465">else</span> /* #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="2235">2235</th><td></td></tr>
<tr><th id="2236">2236</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="rcu_nocb_cpu_needs_barrier" title='rcu_nocb_cpu_needs_barrier' data-ref="rcu_nocb_cpu_needs_barrier" data-ref-filename="rcu_nocb_cpu_needs_barrier">rcu_nocb_cpu_needs_barrier</dfn>(<em>int</em> <dfn class="local col9 decl" id="539cpu" title='cpu' data-type='int' data-ref="539cpu" data-ref-filename="539cpu">cpu</dfn>)</td></tr>
<tr><th id="2237">2237</th><td>{</td></tr>
<tr><th id="2238">2238</th><td>	<a class="macro" href="../../include/asm-generic/bug.h.html#68" title="({ int __ret_warn_on = !!(1); if (__builtin_expect(!!(__ret_warn_on), 0)) do { do { asm volatile(&quot;1:\t&quot; &quot;.byte 0x0f, 0x0b&quot; &quot;\n&quot; &quot;.pushsection __bug_table,\&quot;aw\&quot;\n&quot; &quot;2:\t&quot; &quot;.long &quot; &quot;1b&quot; &quot; - 2b&quot; &quot;\t# bug_entry::bug_addr\n&quot; &quot;\t&quot; &quot;.long &quot; &quot;%c0&quot; &quot; - 2b&quot; &quot;\t# bug_entry::file\n&quot; &quot;\t.word %c1&quot; &quot;\t# bug_entry::line\n&quot; &quot;\t.word %c2&quot; &quot;\t# bug_entry::flags\n&quot; &quot;\t.org 2b+%c3\n&quot; &quot;.popsection&quot; : : &quot;i&quot; (&quot;/home/woboq/linux-5.3.1/kernel/rcu/tree_plugin.h&quot;), &quot;i&quot; (2238), &quot;i&quot; ((1 &lt;&lt; 0)|((1 &lt;&lt; 1)|((9) &lt;&lt; 8))), &quot;i&quot; (sizeof(struct bug_entry))); } while (0); ({ asm volatile(&quot;%c0:\n\t&quot; &quot;.pushsection .discard.reachable\n\t&quot; &quot;.long %c0b - .\n\t&quot; &quot;.popsection\n\t&quot; : : &quot;i&quot; (282)); }); } while (0); __builtin_expect(!!(__ret_warn_on), 0); })" data-ref="_M/WARN_ON_ONCE">WARN_ON_ONCE</a>(<var>1</var>); <i>/* Should be dead code. */</i></td></tr>
<tr><th id="2239">2239</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2240">2240</th><td>}</td></tr>
<tr><th id="2241">2241</th><td></td></tr>
<tr><th id="2242">2242</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_nocb_gp_cleanup" title='rcu_nocb_gp_cleanup' data-ref="rcu_nocb_gp_cleanup" data-ref-filename="rcu_nocb_gp_cleanup">rcu_nocb_gp_cleanup</dfn>(<b>struct</b> <a class="type" href="../../include/linux/swait.h.html#swait_queue_head" title='swait_queue_head' data-ref="swait_queue_head" data-ref-filename="swait_queue_head">swait_queue_head</a> *<dfn class="local col0 decl" id="540sq" title='sq' data-type='struct swait_queue_head *' data-ref="540sq" data-ref-filename="540sq">sq</dfn>)</td></tr>
<tr><th id="2243">2243</th><td>{</td></tr>
<tr><th id="2244">2244</th><td>}</td></tr>
<tr><th id="2245">2245</th><td></td></tr>
<tr><th id="2246">2246</th><td><em>static</em> <b>struct</b> <a class="type" href="../../include/linux/swait.h.html#swait_queue_head" title='swait_queue_head' data-ref="swait_queue_head" data-ref-filename="swait_queue_head">swait_queue_head</a> *<dfn class="decl def fn" id="rcu_nocb_gp_get" title='rcu_nocb_gp_get' data-ref="rcu_nocb_gp_get" data-ref-filename="rcu_nocb_gp_get">rcu_nocb_gp_get</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col1 decl" id="541rnp" title='rnp' data-type='struct rcu_node *' data-ref="541rnp" data-ref-filename="541rnp">rnp</dfn>)</td></tr>
<tr><th id="2247">2247</th><td>{</td></tr>
<tr><th id="2248">2248</th><td>	<b>return</b> <a class="macro" href="../../include/linux/stddef.h.html#8" title="((void *)0)" data-ref="_M/NULL">NULL</a>;</td></tr>
<tr><th id="2249">2249</th><td>}</td></tr>
<tr><th id="2250">2250</th><td></td></tr>
<tr><th id="2251">2251</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_init_one_nocb" title='rcu_init_one_nocb' data-ref="rcu_init_one_nocb" data-ref-filename="rcu_init_one_nocb">rcu_init_one_nocb</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_node" title='rcu_node' data-ref="rcu_node" data-ref-filename="rcu_node">rcu_node</a> *<dfn class="local col2 decl" id="542rnp" title='rnp' data-type='struct rcu_node *' data-ref="542rnp" data-ref-filename="542rnp">rnp</dfn>)</td></tr>
<tr><th id="2252">2252</th><td>{</td></tr>
<tr><th id="2253">2253</th><td>}</td></tr>
<tr><th id="2254">2254</th><td></td></tr>
<tr><th id="2255">2255</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="__call_rcu_nocb" title='__call_rcu_nocb' data-ref="__call_rcu_nocb" data-ref-filename="__call_rcu_nocb">__call_rcu_nocb</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col3 decl" id="543rdp" title='rdp' data-type='struct rcu_data *' data-ref="543rdp" data-ref-filename="543rdp">rdp</dfn>, <b>struct</b> <a class="macro" href="../../include/linux/types.h.html#223" title="callback_head" data-ref="_M/rcu_head">rcu_head</a> *<dfn class="local col4 decl" id="544rhp" title='rhp' data-type='struct callback_head *' data-ref="544rhp" data-ref-filename="544rhp">rhp</dfn>,</td></tr>
<tr><th id="2256">2256</th><td>			    <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="local col5 decl" id="545lazy" title='lazy' data-type='bool' data-ref="545lazy" data-ref-filename="545lazy">lazy</dfn>, <em>unsigned</em> <em>long</em> <dfn class="local col6 decl" id="546flags" title='flags' data-type='unsigned long' data-ref="546flags" data-ref-filename="546flags">flags</dfn>)</td></tr>
<tr><th id="2257">2257</th><td>{</td></tr>
<tr><th id="2258">2258</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2259">2259</th><td>}</td></tr>
<tr><th id="2260">2260</th><td></td></tr>
<tr><th id="2261">2261</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <a class="macro" href="../../include/linux/compiler_attributes.h.html#242" title="__attribute__((__unused__))" data-ref="_M/__maybe_unused">__maybe_unused</a> <dfn class="decl def fn" id="rcu_nocb_adopt_orphan_cbs" title='rcu_nocb_adopt_orphan_cbs' data-ref="rcu_nocb_adopt_orphan_cbs" data-ref-filename="rcu_nocb_adopt_orphan_cbs">rcu_nocb_adopt_orphan_cbs</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col7 decl" id="547my_rdp" title='my_rdp' data-type='struct rcu_data *' data-ref="547my_rdp" data-ref-filename="547my_rdp">my_rdp</dfn>,</td></tr>
<tr><th id="2262">2262</th><td>						     <b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col8 decl" id="548rdp" title='rdp' data-type='struct rcu_data *' data-ref="548rdp" data-ref-filename="548rdp">rdp</dfn>,</td></tr>
<tr><th id="2263">2263</th><td>						     <em>unsigned</em> <em>long</em> <dfn class="local col9 decl" id="549flags" title='flags' data-type='unsigned long' data-ref="549flags" data-ref-filename="549flags">flags</dfn>)</td></tr>
<tr><th id="2264">2264</th><td>{</td></tr>
<tr><th id="2265">2265</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2266">2266</th><td>}</td></tr>
<tr><th id="2267">2267</th><td></td></tr>
<tr><th id="2268">2268</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__((__section__(&quot;.init.text&quot;))) __attribute__((__cold__))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_boot_init_nocb_percpu_data" title='rcu_boot_init_nocb_percpu_data' data-ref="rcu_boot_init_nocb_percpu_data" data-ref-filename="rcu_boot_init_nocb_percpu_data">rcu_boot_init_nocb_percpu_data</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col0 decl" id="550rdp" title='rdp' data-type='struct rcu_data *' data-ref="550rdp" data-ref-filename="550rdp">rdp</dfn>)</td></tr>
<tr><th id="2269">2269</th><td>{</td></tr>
<tr><th id="2270">2270</th><td>}</td></tr>
<tr><th id="2271">2271</th><td></td></tr>
<tr><th id="2272">2272</th><td><em>static</em> <em>int</em> <dfn class="decl def fn" id="rcu_nocb_need_deferred_wakeup" title='rcu_nocb_need_deferred_wakeup' data-ref="rcu_nocb_need_deferred_wakeup" data-ref-filename="rcu_nocb_need_deferred_wakeup">rcu_nocb_need_deferred_wakeup</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col1 decl" id="551rdp" title='rdp' data-type='struct rcu_data *' data-ref="551rdp" data-ref-filename="551rdp">rdp</dfn>)</td></tr>
<tr><th id="2273">2273</th><td>{</td></tr>
<tr><th id="2274">2274</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2275">2275</th><td>}</td></tr>
<tr><th id="2276">2276</th><td></td></tr>
<tr><th id="2277">2277</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="do_nocb_deferred_wakeup" title='do_nocb_deferred_wakeup' data-ref="do_nocb_deferred_wakeup" data-ref-filename="do_nocb_deferred_wakeup">do_nocb_deferred_wakeup</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col2 decl" id="552rdp" title='rdp' data-type='struct rcu_data *' data-ref="552rdp" data-ref-filename="552rdp">rdp</dfn>)</td></tr>
<tr><th id="2278">2278</th><td>{</td></tr>
<tr><th id="2279">2279</th><td>}</td></tr>
<tr><th id="2280">2280</th><td></td></tr>
<tr><th id="2281">2281</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_spawn_cpu_nocb_kthread" title='rcu_spawn_cpu_nocb_kthread' data-ref="rcu_spawn_cpu_nocb_kthread" data-ref-filename="rcu_spawn_cpu_nocb_kthread">rcu_spawn_cpu_nocb_kthread</dfn>(<em>int</em> <dfn class="local col3 decl" id="553cpu" title='cpu' data-type='int' data-ref="553cpu" data-ref-filename="553cpu">cpu</dfn>)</td></tr>
<tr><th id="2282">2282</th><td>{</td></tr>
<tr><th id="2283">2283</th><td>}</td></tr>
<tr><th id="2284">2284</th><td></td></tr>
<tr><th id="2285">2285</th><td><em>static</em> <em>void</em> <a class="macro" href="../../include/linux/init.h.html#50" title="__attribute__((__section__(&quot;.init.text&quot;))) __attribute__((__cold__))" data-ref="_M/__init">__init</a> <dfn class="decl def fn" id="rcu_spawn_nocb_kthreads" title='rcu_spawn_nocb_kthreads' data-ref="rcu_spawn_nocb_kthreads" data-ref-filename="rcu_spawn_nocb_kthreads">rcu_spawn_nocb_kthreads</dfn>(<em>void</em>)</td></tr>
<tr><th id="2286">2286</th><td>{</td></tr>
<tr><th id="2287">2287</th><td>}</td></tr>
<tr><th id="2288">2288</th><td></td></tr>
<tr><th id="2289">2289</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="init_nocb_callback_list" title='init_nocb_callback_list' data-ref="init_nocb_callback_list" data-ref-filename="init_nocb_callback_list">init_nocb_callback_list</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col4 decl" id="554rdp" title='rdp' data-type='struct rcu_data *' data-ref="554rdp" data-ref-filename="554rdp">rdp</dfn>)</td></tr>
<tr><th id="2290">2290</th><td>{</td></tr>
<tr><th id="2291">2291</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2292">2292</th><td>}</td></tr>
<tr><th id="2293">2293</th><td></td></tr>
<tr><th id="2294">2294</th><td><em>static</em> <em>unsigned</em> <em>long</em> <dfn class="decl def fn" id="rcu_get_n_cbs_nocb_cpu" title='rcu_get_n_cbs_nocb_cpu' data-ref="rcu_get_n_cbs_nocb_cpu" data-ref-filename="rcu_get_n_cbs_nocb_cpu">rcu_get_n_cbs_nocb_cpu</dfn>(<b>struct</b> <a class="type" href="tree.h.html#rcu_data" title='rcu_data' data-ref="rcu_data" data-ref-filename="rcu_data">rcu_data</a> *<dfn class="local col5 decl" id="555rdp" title='rdp' data-type='struct rcu_data *' data-ref="555rdp" data-ref-filename="555rdp">rdp</dfn>)</td></tr>
<tr><th id="2295">2295</th><td>{</td></tr>
<tr><th id="2296">2296</th><td>	<b>return</b> <var>0</var>;</td></tr>
<tr><th id="2297">2297</th><td>}</td></tr>
<tr><th id="2298">2298</th><td></td></tr>
<tr><th id="2299">2299</th><td><u>#<span data-ppcond="1465">endif</span> /* #else #ifdef CONFIG_RCU_NOCB_CPU */</u></td></tr>
<tr><th id="2300">2300</th><td></td></tr>
<tr><th id="2301">2301</th><td><i>/*</i></td></tr>
<tr><th id="2302">2302</th><td><i> * Is this CPU a NO_HZ_FULL CPU that should ignore RCU so that the</i></td></tr>
<tr><th id="2303">2303</th><td><i> * grace-period kthread will do force_quiescent_state() processing?</i></td></tr>
<tr><th id="2304">2304</th><td><i> * The idea is to avoid waking up RCU core processing on such a</i></td></tr>
<tr><th id="2305">2305</th><td><i> * CPU unless the grace period has extended for too long.</i></td></tr>
<tr><th id="2306">2306</th><td><i> *</i></td></tr>
<tr><th id="2307">2307</th><td><i> * This code relies on the fact that all NO_HZ_FULL CPUs are also</i></td></tr>
<tr><th id="2308">2308</th><td><i> * CONFIG_RCU_NOCB_CPU CPUs.</i></td></tr>
<tr><th id="2309">2309</th><td><i> */</i></td></tr>
<tr><th id="2310">2310</th><td><em>static</em> <a class="typedef" href="../../include/linux/types.h.html#bool" title='bool' data-type='_Bool' data-ref="bool" data-ref-filename="bool">bool</a> <dfn class="decl def fn" id="rcu_nohz_full_cpu" title='rcu_nohz_full_cpu' data-ref="rcu_nohz_full_cpu" data-ref-filename="rcu_nohz_full_cpu">rcu_nohz_full_cpu</dfn>(<em>void</em>)</td></tr>
<tr><th id="2311">2311</th><td>{</td></tr>
<tr><th id="2312">2312</th><td><u>#<span data-ppcond="2312">ifdef</span> <span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span></u></td></tr>
<tr><th id="2313">2313</th><td>	<b>if</b> (tick_nohz_full_cpu(smp_processor_id()) &amp;&amp;</td></tr>
<tr><th id="2314">2314</th><td>	    (!rcu_gp_in_progress() ||</td></tr>
<tr><th id="2315">2315</th><td>	     ULONG_CMP_LT(jiffies, READ_ONCE(rcu_state.gp_start) + HZ)))</td></tr>
<tr><th id="2316">2316</th><td>		<b>return</b> true;</td></tr>
<tr><th id="2317">2317</th><td><u>#<span data-ppcond="2312">endif</span> /* #ifdef CONFIG_NO_HZ_FULL */</u></td></tr>
<tr><th id="2318">2318</th><td>	<b>return</b> <a class="enum" href="../../include/linux/stddef.h.html#false" title='false' data-ref="false" data-ref-filename="false">false</a>;</td></tr>
<tr><th id="2319">2319</th><td>}</td></tr>
<tr><th id="2320">2320</th><td></td></tr>
<tr><th id="2321">2321</th><td><i>/*</i></td></tr>
<tr><th id="2322">2322</th><td><i> * Bind the RCU grace-period kthreads to the housekeeping CPU.</i></td></tr>
<tr><th id="2323">2323</th><td><i> */</i></td></tr>
<tr><th id="2324">2324</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_bind_gp_kthread" title='rcu_bind_gp_kthread' data-ref="rcu_bind_gp_kthread" data-ref-filename="rcu_bind_gp_kthread">rcu_bind_gp_kthread</dfn>(<em>void</em>)</td></tr>
<tr><th id="2325">2325</th><td>{</td></tr>
<tr><th id="2326">2326</th><td>	<b>if</b> (!<a class="ref fn" href="../../include/linux/tick.h.html#tick_nohz_full_enabled" title='tick_nohz_full_enabled' data-ref="tick_nohz_full_enabled" data-ref-filename="tick_nohz_full_enabled">tick_nohz_full_enabled</a>())</td></tr>
<tr><th id="2327">2327</th><td>		<b>return</b>;</td></tr>
<tr><th id="2328">2328</th><td>	<a class="ref fn" href="../../include/linux/sched/isolation.h.html#housekeeping_affine" title='housekeeping_affine' data-ref="housekeeping_affine" data-ref-filename="housekeeping_affine">housekeeping_affine</a>(<a class="macro" href="../../arch/x86/include/asm/current.h.html#18" title="get_current()" data-ref="_M/current">current</a>, <a class="enum" href="../../include/linux/sched/isolation.h.html#HK_FLAG_RCU" title='HK_FLAG_RCU' data-ref="HK_FLAG_RCU" data-ref-filename="HK_FLAG_RCU">HK_FLAG_RCU</a>);</td></tr>
<tr><th id="2329">2329</th><td>}</td></tr>
<tr><th id="2330">2330</th><td></td></tr>
<tr><th id="2331">2331</th><td><i>/* Record the current task on dyntick-idle entry. */</i></td></tr>
<tr><th id="2332">2332</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_dynticks_task_enter" title='rcu_dynticks_task_enter' data-ref="rcu_dynticks_task_enter" data-ref-filename="rcu_dynticks_task_enter">rcu_dynticks_task_enter</dfn>(<em>void</em>)</td></tr>
<tr><th id="2333">2333</th><td>{</td></tr>
<tr><th id="2334">2334</th><td><u>#<span data-ppcond="2334">if</span> defined(<span class="macro" data-ref="_M/CONFIG_TASKS_RCU">CONFIG_TASKS_RCU</span>) &amp;&amp; defined(<span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span>)</u></td></tr>
<tr><th id="2335">2335</th><td>	WRITE_ONCE(current-&gt;rcu_tasks_idle_cpu, smp_processor_id());</td></tr>
<tr><th id="2336">2336</th><td><u>#<span data-ppcond="2334">endif</span> /* #if defined(CONFIG_TASKS_RCU) &amp;&amp; defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2337">2337</th><td>}</td></tr>
<tr><th id="2338">2338</th><td></td></tr>
<tr><th id="2339">2339</th><td><i>/* Record no current task on dyntick-idle exit. */</i></td></tr>
<tr><th id="2340">2340</th><td><em>static</em> <em>void</em> <dfn class="decl def fn" id="rcu_dynticks_task_exit" title='rcu_dynticks_task_exit' data-ref="rcu_dynticks_task_exit" data-ref-filename="rcu_dynticks_task_exit">rcu_dynticks_task_exit</dfn>(<em>void</em>)</td></tr>
<tr><th id="2341">2341</th><td>{</td></tr>
<tr><th id="2342">2342</th><td><u>#<span data-ppcond="2342">if</span> defined(<span class="macro" data-ref="_M/CONFIG_TASKS_RCU">CONFIG_TASKS_RCU</span>) &amp;&amp; defined(<span class="macro" data-ref="_M/CONFIG_NO_HZ_FULL">CONFIG_NO_HZ_FULL</span>)</u></td></tr>
<tr><th id="2343">2343</th><td>	WRITE_ONCE(current-&gt;rcu_tasks_idle_cpu, -<var>1</var>);</td></tr>
<tr><th id="2344">2344</th><td><u>#<span data-ppcond="2342">endif</span> /* #if defined(CONFIG_TASKS_RCU) &amp;&amp; defined(CONFIG_NO_HZ_FULL) */</u></td></tr>
<tr><th id="2345">2345</th><td>}</td></tr>
<tr><th id="2346">2346</th><td></td></tr>
</table><hr/><p id='footer'>
Generated while processing <a href='tree.c.html'>linux-5.3.1/kernel/rcu/tree.c</a><br/>Generated on <em>2020-Jun-10</em> from project linux-5.3.1 revision <em>5.3.1</em><br />Powered by <a href='https://woboq.com'><img alt='Woboq' src='https://code.woboq.org/woboq-16.png' width='41' height='16' /></a> <a href='https://code.woboq.org'>Code Browser</a> 2.1
<br/>Generator usage only permitted with license.</p>
</div></body></html>
